,Section Names,Section Length,Chapter Links,Section Descriptions
0,About Introduction to Probability and Data,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,"This course introduces you to sampling and exploring data, as well as basic probability theory. You will examine various types of sampling methods and discuss how such methods can impact the utility of a data analysis. The concepts in this module will serve as building blocks for our later courses.Each lesson comes with a set of learning objectives that will be covered in a series of short videos. Supplementary readings and practice problems will also be suggested from OpenIntro Statistics, 3rd Edition, https://leanpub.com/openintro-statistics/, (a free online introductory statistics textbook, that I co-authored). There will be weekly quizzes designed to assess your learning and mastery of the material covered that week in the videos. In addition, each week will also feature a lab assignment, in which you will use R to apply what you are learning to real data. There will also be a data analysis project designed to enable you to answer research questions of your own choosing. Since this is a Coursera course, you are welcome to participate as much or as little as you’d like, though I hope that you will begin by participating fully. One of the most rewarding aspects of a Coursera course is participation in forum discussions about the course materials. Please take advantage of other students' feedback and insight and contribute your own perspective where you see fit to do so. You can also check out the resource page (https://www.coursera.org/learn/probability-intro/resources/crMc4) listing useful resources for this course. Thank you for joining the Introduction to Probability and Data community! Say hello in the Discussion Forums. We are looking forward to your participation in the course."
1,Introduction to Data,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,"Welcome to Introduction to Probability and Data! I hope you are just as excited about this course as I am! In the next five weeks, we will learn about designing studies, explore data via numerical summaries and visualizations, and learn about rules of probability and commonly used probability distributions. If you have any questions, feel free to post them on this module's forum (https://www.coursera.org/learn/probability-intro/module/rQ9Al/discussions?sort=lastActivityAtDesc&page=1) and discuss with your peers! To get started, view the learning objectives (https://www.coursera.org/learn/probability-intro/supplement/rooeY/lesson-learning-objectives) of Lesson 1 in this module."
2,Introduction to Data Project,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,To complete this assignment you will use R and RStudio installed on your local computer or through RStudio Cloud.
3,Exploratory Data Analysis and Introduction to Inference,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,"Welcome to Week 2 of Introduction to Probability and Data! Hope you enjoyed materials from Week 1. This week we will delve into numerical and categorical data in more depth, and introduce inference."
4,Exploratory Data Analysis and Introduction to Inference Project,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,To complete this assignment you will use R and RStudio installed on your local computer or through RStudio Cloud.
5,Introduction to Probability,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,"Welcome to Week 3 of Introduction to Probability and Data! Last week we explored numerical and categorical data. This week we will discuss probability, conditional probability, the Bayes’ theorem, and provide a light introduction to Bayesian inference. Thank you for your enthusiasm and participation, and have a great week! I’m looking forward to working with you on the rest of this course."
6,Introduction to Probability Project,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,To complete this assignment you will use R and RStudio installed on your local computer or through RStudio Cloud.
7,Probability Distributions,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,"Great work so far! Welcome to Week 4 -- the last content week of Introduction to Probability and Data! This week we will introduce two probability distributions: the normal and the binomial distributions in particular. As usual, you can evaluate your knowledge in this week's quiz. There will be no labs for this week. Please don't hesitate to post any questions, discussions and related topics on this week's forum (https://www.coursera.org/learn/probability-intro/module/VdVNg/discussions?sort=lastActivityAtDesc&page=1)."
8,Data Analysis Project,9,https://www.coursera.org/learn/probability-intro?specialization=statistics,"Well done! You have reached the last week of Introduction to Probability and Data! There will not be any new videos in this week, instead, you will be asked to complete an initial data analysis project with a real-world data set. The project is designed to help you discover and explore research questions of your own, using real data and statistical methods we learn in this class. The the project will be graded via peer assessments, meaning that you will need to evaluate three peers' projects after submitting your own. Get started with your data analysis in this week! It should be interesting and very exciting! As usual, feel free to post questions, concerns, and comments about the project on this week's forum (https://www.coursera.org/learn/probability-intro/module/BaTDb/discussions?sort=lastActivityAtDesc&page=1)."
9,About the Specialization and the Course,6,https://www.coursera.org/learn/inferential-statistics-intro?specialization=statistics,"This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Inferential Statistics. Please take several minutes to browse them through. Thanks for joining us in this course!"
10,Central Limit Theorem and Confidence Interval,6,https://www.coursera.org/learn/inferential-statistics-intro?specialization=statistics,"Welcome to Inferential Statistics! In this course we will discuss Foundations for Inference. Check out the learning objectives, start watching the videos, and finally work on the quiz and the labs of this week. In addition to videos that introduce new concepts, you will also see a few videos that walk you through application examples related to the week's topics. In the first week we will introduce Central Limit Theorem (CLT) and confidence interval."
11,Inference and Significance,6,https://www.coursera.org/learn/inferential-statistics-intro?specialization=statistics,"Welcome to Week Two! This week we will discuss formal hypothesis testing and relate testing procedures back to estimation via confidence intervals. These topics will be introduced within the context of working with a population mean, however we will also give you a brief peek at what's to come in the next two weeks by discussing how the methods we're learning can be extended to other estimators. We will also discuss crucial considerations like decision errors and statistical vs. practical significance. The labs for this week will illustrate concepts of sampling distributions and confidence levels."
12,Inference for Comparing Means,6,https://www.coursera.org/learn/inferential-statistics-intro?specialization=statistics,"Welcome to Week Three of the course! This week we will introduce the t-distribution and comparing means as well as a simulation based method for creating a confidence interval: bootstrapping. If you have questions or discussions, please use this week's forum to ask/discuss with peers."
13,Inference for Proportions,6,https://www.coursera.org/learn/inferential-statistics-intro?specialization=statistics,"Welcome to Week Four of our course! In this unit, we’ll discuss inference for categorical data. We use methods introduced this week to answer questions like “What proportion of the American public approves of the job of the Supreme Court is doing?”."
14, Data Analysis Project,6,https://www.coursera.org/learn/inferential-statistics-intro?specialization=statistics,"In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. "
15,About Linear Regression and Modeling,5,https://www.coursera.org/learn/linear-regression-model?specialization=statistics,"This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Linear Regression and Modeling. Please take several minutes to browse them through. Thanks for joining us in this course!"
16,Linear Regression,5,https://www.coursera.org/learn/linear-regression-model?specialization=statistics,"In this week we’ll introduce linear regression. Many of you may be familiar with regression from reading the news, where graphs with straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables. "
17,More about Linear Regression,5,https://www.coursera.org/learn/linear-regression-model?specialization=statistics,"Welcome to week 2! In this week, we will look at outliers, inference in linear regression and variability partitioning. Please use this week to strengthen your understanding on linear regression. Don't forget to post your questions, concerns and suggestions in the discussion forum!"
18,Multiple Regression,5,https://www.coursera.org/learn/linear-regression-model?specialization=statistics,"In this week, we’ll explore multiple regression, which allows us to model numerical response variables using multiple predictors (numerical and categorical). We will also cover inference for multiple linear regression, model selection, and model diagnostics. Hope you enjoy!"
19,Final Project,5,https://www.coursera.org/learn/linear-regression-model?specialization=statistics,"In this week you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. "
20,About the Specialization and the Course,7,https://www.coursera.org/learn/bayesian?specialization=statistics,"This short module introduces basics about Coursera specializations and courses in general, this specialization: Statistics with R, and this course: Bayesian Statistics. Please take several minutes read this information. Thanks for joining us in this course!"
21,The Basics of Bayesian Statistics,7,https://www.coursera.org/learn/bayesian?specialization=statistics,"<p>Welcome! Over the next several weeks, we will together explore Bayesian statistics. <p>In this module, we will work with conditional probabilities, which is the probability of event B given event A. Conditional probabilities are very important in medical decisions. By the end of the week, you will be able to solve problems using Bayes' rule, and update prior probabilities.</p><p>Please use the learning objectives and practice quiz to help you learn about Bayes' Rule, and apply what you have learned in the lab and on the quiz. "
22,Bayesian Inference,7,https://www.coursera.org/learn/bayesian?specialization=statistics,"In this week, we will discuss the continuous version of Bayes' rule and show you how to use it in a conjugate family, and discuss credible intervals. By the end of this week, you will be able to understand and define the concepts of prior, likelihood, and posterior probability and identify how they relate to one another."
23,Decision Making,7,https://www.coursera.org/learn/bayesian?specialization=statistics,"In this module, we will discuss Bayesian decision making, hypothesis testing, and Bayesian testing. By the end of this week, you will be able to make optimal decisions based on Bayesian statistics and compare multiple hypotheses using Bayes Factors. "
24,Bayesian Regression,7,https://www.coursera.org/learn/bayesian?specialization=statistics,"This week, we will look at Bayesian linear regressions and model averaging, which allows you to make inferences and predictions using several models. By the end of this week, you will be able to implement Bayesian model averaging, interpret Bayesian multiple linear regression and understand its relationship to the frequentist linear regression approach. "
25,Perspectives on Bayesian Applications,7,https://www.coursera.org/learn/bayesian?specialization=statistics,"This week consists of interviews with statisticians on how they use Bayesian statistics in their work, as well as the final project in the course."
26,Data Analysis Project,7,https://www.coursera.org/learn/bayesian?specialization=statistics,"In this module you will use the data set provided to complete and report on a data analysis question. Please read the background information, review the report template (downloaded from the link in Lesson Project Information), and then complete the peer review assignment. "
27,About the Capstone Project,8,https://www.coursera.org/learn/statistics-project,"Welcome to the capstone project! This week's content is an introduction to the project assignment and goals. The readings in this week will introduce the data set that you will be analyzing for your project and the specific questions you will answer using data analysis techniques we learned in the previous courses. It is important to understand what we will be doing in the course before jumping into the detailed analysis. So we encourage you to start with the first lecture to get the big picture, and then delve into the specifics of the analysis. Enjoy, and good luck! Remember, if you have questions, you can post them on the discussion forums."
28,Exploratory Data Analysis (EDA),8,https://www.coursera.org/learn/statistics-project,"This week you will work on conducting an exploratory analysis of the housing data. Exploratory analysis is an essential first step for familiarizing yourself with and understanding the data. In this week, you will complete a quiz which will guide you through certain important aspects of the data. The insights you gain through this assignment will help inform modeling in the future quizzes and peer assessments. 

Feel free to post questions about this assignment on the discussion forum."
29,EDA and Basic Model Selection - Submission,8,https://www.coursera.org/learn/statistics-project,"This week we will dig deeper into our exploratory data analysis of the data. We now have all the information and data necessary to perform a deep dive into the EDA and it is time start your initial analysis report! We encourage you to start your analysis report (presented in peer-review format next week) early so you will have enough time to complete it. You will conduct exploratory data analysis, model selection, and model evaluation, and then complete a written report which answers several questions which will guide you through the process. This report will be your first peer-review assignment in this course. "
30,EDA and Basic Model Selection - Evaluation,8,https://www.coursera.org/learn/statistics-project,Great work so far! We hope you will also learn as much from evaluating your peers' work as completing your own assignment. Happy learning!
31,Model Selection and Diagnostics,8,https://www.coursera.org/learn/statistics-project,"We are half way through the course! In this week, you will continue model selection and model diagnostics, which will serve a starting point for your final project. You will be assessed on your work through a quiz. If you have any questions so far, don't hesitate to post on the forum so that others can help and discuss the question together."
32,Out of Sample Prediction,8,https://www.coursera.org/learn/statistics-project,"In this week, you will gain experience using your model to perform out-of-sample prediction and validation.  The skills honed this week will guide you through your final analysis in the weeks to come.  Please feel free to go back to prior weeks and review the necessary background knowledge. "
33,Final Data Analysis - Submission,8,https://www.coursera.org/learn/statistics-project,"In the next two weeks, you will complete your final data analysis project. You will submit your answers using the Final Data Analysis peer review assignment link in Week 8."
34,Final Data Analysis - Evaluation,8,https://www.coursera.org/learn/statistics-project,"Congratulations on making through to the final week of the course! In this week, we will finish this data analysis project by completing the evaluation of three of your peers' assignments. "
35,WEEK 1 - INTRODUCTION TO DATA,4,https://www.coursera.org/learn/understanding-visualization-data,"In the first week of the course, we will review a course outline and discover the various concepts and objectives to be mastered in the weeks to come. You will get an introduction to the field of statistics and explore a variety of perspectives the field has to offer. We will identify numerous types of data that exist and observe where they can be found in everyday life. You will delve into basic Python functionality, along with an introduction to Jupyter Notebook. All of the course information on grading, prerequisites, and expectations are on the course syllabus and you can find more information on our Course Resources page."
36,WEEK 2 - UNIVARIATE DATA,4,https://www.coursera.org/learn/understanding-visualization-data,"In the second week of this course, we will be looking at graphical and numerical interpretations for one variable (univariate data). In particular, we will be creating and analyzing histograms, box plots, and numerical summaries of our data in order to give a basis of analysis for quantitative data and bar charts and pie charts for categorical data. A few key interpretations will be made about our numerical summaries such as mean, IQR, and standard deviation. An assessment is included at the end of the week concerning numerical summaries and interpretations of these summaries."
37,WEEK 3 - MULTIVARIATE DATA,4,https://www.coursera.org/learn/understanding-visualization-data,"In the third week of this course on looking at data, we’ll introduce key ideas for examining research questions that require looking at more than one variable.  In particular, we will consider both numerically and visually how different variables interact, how summaries can appear deceiving if you don’t properly account for interactions, and differences between quantitative and categorical variables.  This week’s assignment will consist of a writing assignment along with reviewing those of your peers."
38,WEEK 4 - POPULATIONS AND SAMPLES,4,https://www.coursera.org/learn/understanding-visualization-data,"In this week, you’ll spend more time thinking about where data come from. The highest-quality statistical analyses of data will always incorporate information about the process used to generate the data, or features of the data collection design. You’ll be exposed to important concepts related to sampling from larger populations, including probability and non-probability sampling, and how we can make inferences about larger populations based on well-designed samples. You’ll also learn about the concept of a sampling distribution, and how estimation of the variance of that distribution plays a critical role in making statements about populations. Finally, you’ll learn about the importance of reading the documentation for a given data set; a key step in looking at data is also looking at the available documentation for that data set, which describes how the data were generated."
39,WEEK 1 - OVERVIEW & INFERENCE PROCEDURES,4,https://www.coursera.org/learn/inferential-statistical-analysis-python,"In this first week, we’ll review the course syllabus and discover the various concepts and objectives to be mastered in weeks to come. You’ll be introduced to inference methods and some of the research questions we’ll discuss in the course, as well as an overall framework for making decisions using data, considerations for how you make those decisions, and evaluating errors that you may have made.On the Python side, we’ll review some high level concepts from the first course in this series, Python’s statistics landscape, and walk through intermediate level Python concepts. All of the course information on grading, prerequisites, and expectations are on the course syllabus and you can find more information on our Course Resources page."
40,WEEK 2 - CONFIDENCE INTERVALS,4,https://www.coursera.org/learn/inferential-statistical-analysis-python,"In this second week, we will learn about estimating population parameters via confidence intervals. You will be introduced to five different types of population parameters, assumptions needed to calculate a confidence interval for each of these five parameters, and how to calculate confidence intervals. Quizzes  will appear throughout the week to test your understanding. In addition, you’ll learn how to create confidence intervals in Python."
41,WEEK 3 - HYPOTHESIS TESTING,4,https://www.coursera.org/learn/inferential-statistical-analysis-python,"In week three, we’ll learn how to test various hypotheses - using the five different analysis methods covered in the previous week. We’ll discuss the importance of various factors and assumptions with hypothesis testing and learn to interpret our results. We will also review how to distinguish which procedure is appropriate for the research question at hand.  Quizzes and a peer assessment will appear throughout the week to test your understanding."
42,WEEK 4 - LEARNER APPLICATION,4,https://www.coursera.org/learn/inferential-statistical-analysis-python,"In the final week of this course, we will walk through several examples and case studies that illustrate applications of the inferential procedures discussed in prior weeks. Learners will see examples of well-formulated research questions related to the study designs and data sets that we have discussed thus far, and via both confidence interval estimation and formal hypothesis testing, we will formulate inferential responses to those questions."
43,WEEK 1 - OVERVIEW & CONSIDERATIONS FOR STATISTICAL MODELING,4,https://www.coursera.org/learn/fitting-statistical-models-data-python,"We begin this third course of the Statistics with Python specialization with an overview of what is meant by “fitting statistical models to data.” In this first week, we will introduce key model fitting concepts, including the distinction between dependent and independent variables, how to account for study designs when fitting models, assessing the quality of model fit, exploring how different types of variables are handled in statistical modeling, and clearly defining the objectives of fitting models."
44,WEEK 2 - FITTING MODELS TO INDEPENDENT DATA,4,https://www.coursera.org/learn/fitting-statistical-models-data-python,"In this second week, we’ll introduce you to the basics of two types of regression: linear regression and logistic regression.  You’ll get the chance to think about how to fit models, how to assess how well those models fit, and to consider how to interpret those models in the context of the data.  You’ll also learn how to implement those models within Python."
45,WEEK 3 - FITTING MODELS TO DEPENDENT DATA,4,https://www.coursera.org/learn/fitting-statistical-models-data-python,"In the third week of this course, we will be building upon the modeling concepts discussed in Week 2. Multilevel and marginal models will be our main topic of discussion, as these models enable researchers to account for dependencies in variables of interest introduced by study designs. We’ll be covering why and when we fit these alternative models, likelihood ratio tests, as well as fixed effects and their interpretations. "
46,WEEK 4: Special Topics,4,https://www.coursera.org/learn/fitting-statistical-models-data-python,"In this final week, we introduce special topics that extend the curriculum from previous weeks and courses further. We will cover a broad range of topics such as various types of dependent variables, exploring sampling methods and whether or not to use survey weights when fitting models, and in-depth case studies utilizing Bayesian techniques to derive insights from data. You’ll also have the opportunity to apply Bayesian techniques in Python."
47,Before we get started...,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"In this first module we'll consider the basic principles of the scientific method, its history and its philosophies. But before we start talking methods, I'll give you a broad sense of what the course is about and how it's organized. Are you new to Coursera or still deciding whether this is the course for you? Then make sure to check out the 'Introduction' and 'What to expect' section below, so you'll have the essential information you need to decide and to do well in this course! If you have any questions about the course format, deadlines or grading, you'll probably find the answers here. Are you a Coursera veteran and anxious to get started? Then you might want to skip ahead to the first course topic: the Origins of the Scientific Method. You can always check the general information later. Veterans and newbies alike: Don't forget to introduce yourself in the 'meet and greet' forum!"
48,Origins of the scientific method,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"Science is all about gaining knowledge, coming up with the best possible explanations of the world around us. So how do we decide which explanation is the best one? How do we make sure our explanations are accurate? How do we determine we actually know something? In science we try to resolve these questions by using a set of principles and procedures called the scientific method. You need to know its historical and philosophical  'origin story' to really understand the scientific method and to fully appreciate how hard it is to apply the scientific method in the social and behavioral sciences!"
49,The Scientific Method,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"In the first module we discussed how the scientific method developed, general philosophical approaches and the types of knowledge science aims to find. In this second module we'll make these abstract principles and concepts a little more concrete by discussing the empirical cycle and causality in more detail. We’ll see how, and in what order these concepts are implemented when we conduct a research study. We'll also consider the main criteria for evaluating the methodological quality of a research study: Validity and reliability. The focus will be on internal validity and how internal validity can be threatened."
50,Research Designs,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"In the previous module we discussed the empirical cycle, causality and the criteria for methodological quality, focusing on threats to internal validity. In this module we'll consider the most frequently used research designs and we'll see how they address threats to internal validity. We'll look at experimental, quasi-experimental and correlational designs, as well as some other designs you should be familiar with. To understand and appreciate these designs we will discuss some general concepts such as randomization and matching in a little more detail."
51,Measurement,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"Choosing a design is only the first step in the deduction phase (remember the empirical cycle?). The second step is deciding on specific ways to measure the variables of interest and disinterest. This step is extremely important, because even if we are able to perform a true double-blind experiment, if our measurement and manipulation method are of poor quality, then internal validity will still be compromised! 

In this module we'll look at what measurement is exactly and what the criteria for evaluating measurement are. We will also look more in-depth at self-report measures, including survey, questionnaires and tests. These methods are among the most frequently used measurement instruments in the social and behavioral sciences."
52,Sampling,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"In the previous two modules we discussed research designs and methods to measure and manipulate our variables of interest and disinterest. Before a researcher can move on to the testing phase and can actually collect data, there is just one more procedure that needs to be decided on: Sampling. Researchers need to determine who potential participants are and how they will be selected and recruited. "
53,"Practice, Ethics & Integrity",9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"In this last content module we will focus on the part of the research process that follows data collection. The specifics of storing data and using statistics form a course topic in their own right (see the specialization courses on Basic and Inferential Statistics). For now we will focus on more general issues to do with data, interpretation and dissemination of results that relate to ethics and integrity. Some of the concepts that we discuss here will be familiar if you watched the interviews of the past modules. It might be interesting to (re-)watch these if you have the time!"
54,Catch Up,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"In this module there's no new material to study. The only requirement in this module is that you finish up the final peer review assignment. We also advise you to take some extra time to review the material from the previous modules and to practice for the final exam. We've provided two practice exams that you can take as many times as you like. In the first one, feedback for each answer will be provided right after taking the test. We've also created some screencast videos that explain the right answers to the second practice exam in more detail. "
55,Exam Time!,9,https://www.coursera.org/learn/quantitative-methods?specialization=social-science,"This is the final module, where you can apply everything you've learned until now in the final exam. The final exam is structured exactly like the practice exam, so you know what to expect. Please note that you can only take the final exam once a month, so make sure you are fully prepared to take the test. Please follow the honor code and do not communicate or confer with others taking this exam. Good luck! Once you've taken the exam why not check out the bonus material - a series of presentations on research integrity in the social sciences, presented at a special symposium at the University of Amsterdam in 2014."
56,Philosophy of Qualitative Research,8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,"Welcome to the first week of the course. We start with an introduction, followed by two lessons on the  Philosophy of Qualitative Research."
57,Observation,8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,"In the first module we discussed the philosophy of qualitative research, explaining some basic notions and general philosophical approaches. In this second module we'll discuss observation as an important method within qualitative research. What types of observation are there? How do we observe? And how do we analyse and describe our data? "
58,Good Practices & Criteria,8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,"What makes qualitative research 'good' is a rather difficult question. Different criteria are suggested, but within the field of qualitative research there is not much agreement on these criteria. However, there is quite some agreement on what good practices of qualitative research are. In this module we will start in lesson 1 with a  discussion of  good practices of qualitative research."
59,Qualitative Interviewing,8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,In this module we'll look at what a qualitative interview entails by trying to define it and by discussing different forms of interviewing behaviour. 
60,Qualitative Analysis,8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,"In previous modules we discussed how you should observe a social situation or conduct a qualitative interview. Now we will focus on what to do with your data, by discussing qualitative analysis. In this module you will try to do a qualitative analysis by interpreting your observed data and try to code it. "
61,"Writing, mixing & ethics",8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,"In this module I will discuss ideas on writing in qualitative research, I will discuss mixing methods  and talk about the ethical issues you should consider. "
62,Catch up week,8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,In this module there's no new material. The only requirement in this module is that you finish up the final peer review assignment.
63,Exam week,8,https://www.coursera.org/learn/qualitative-methods?specialization=social-science,"This is the final module, where you can apply everything you've learned up until now in the final exam. Please note that you can only take the final exam once every day, so make sure you are fully prepared to take the test. Please follow the honor code and do not communicate or confer with others taking this exam. Good luck! Once you've taken the exam, please consider doing the other courses in our specialisation track. I hope it was an enjoyable experience. If it was, please consider joining in with the Massive Open Online Research by my colleague Christian Bröer. Thanks for all your hard work, feedback and interpretations, the course team and your fellow learners really appreciate it!"
64,Before we get started...,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"In this module we'll consider the basics of statistics. But before we start, we'll give you a broad sense of what the course is about and how it's organized. Are you new to Coursera or still deciding whether this is the course for you? Then make sure to check out the 'Course introduction' and 'What to expect from this course' sections below, so you'll have the essential information you need to decide and to do well in this course! If you have any questions about the course format, deadlines or grading, you'll probably find the answers here. Are you a Coursera veteran and ready to get started? Then you might want to skip ahead to the first course topic: 'Exploring data'. You can always check the general information later. Veterans and newbies alike: Don't forget to introduce yourself in the 'meet and greet' forum!"
65,Exploring Data,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"In this first module, we’ll introduce the basic concepts of descriptive statistics. We’ll talk about cases and variables, and we’ll explain how you can order them in a so-called data matrix. We’ll discuss various levels of measurement and we’ll show you how you can present your data by means of tables and graphs. We’ll also introduce measures of central tendency (like mode, median and mean) and dispersion (like range, interquartile range, variance and standard deviation). We’ll not only tell you how to interpret them; we’ll also explain how you can compute them. Finally, we’ll tell you more about z-scores. In this module we’ll only discuss situations in which we analyze one single variable. This is what we call univariate analysis. In the next module we will also introduce studies in which more variables are involved."
66,Correlation and Regression,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,In this second module we’ll look at bivariate analyses: studies with two variables. First we’ll introduce the concept of correlation. We’ll investigate contingency tables (when it comes to categorical variables) and scatterplots (regarding quantitative variables). We’ll also learn how to understand and compute one of the most frequently used measures of correlation: Pearson's r. In the next part of the module we’ll introduce the method of OLS regression analysis. We’ll explain how you (or the computer) can find the regression line and how you can describe this line by means of an equation. We’ll show you that you can assess how well the regression line fits your data by means of the so-called r-squared. We conclude the module with a discussion of why you should always be very careful when interpreting the results of a regression analysis.     
67,Probability,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"This module introduces concepts from probability theory and the rules for calculating with probabilities. This is not only useful for answering various kinds of applied statistical questions but also to understand the statistical analyses that will be introduced in subsequent modules. We start by describing randomness, and explain how random events surround us. Next, we provide an intuitive definition of probability through an example and relate this to the concepts of events, sample space and random trials. A graphical tool to understand these concepts is introduced here as well, the tree-diagram.Thereafter a number of concepts from set theory are explained and related to probability calculations. Here the relation is made to tree-diagrams again, as well as contingency tables. We end with a lesson where conditional probabilities, independence and Bayes rule are explained. All in all, this is quite a theoretical module on a topic that is not always easy to grasp. That's why we have included as many intuitive examples as possible."
68,Probability Distributions,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"Probability distributions form the core of many statistical calculations. They are used as mathematical models to represent some random phenomenon and subsequently answer statistical questions about that phenomenon. This module starts by explaining the basic properties of a probability distribution, highlighting how it quantifies a random variable and also pointing out how it differs between discrete and continuous random variables. Subsequently the cumulative probability distribution is introduced and its properties and usage are explained as well. In a next lecture it is shown how a random variable with its associated probability distribution can be characterized by statistics like a mean and variance, just like observational data. The effects of changing random variables by multiplication or addition on these statistics are explained as well.The lecture thereafter introduces the normal distribution, starting by explaining its functional form and some general properties. Next, the basic usage of the normal distribution to calculate probabilities is explained. And in a final lecture the binomial distribution, an important probability distribution for discrete data, is introduced and further explained. By the end of this module you have covered quite some ground and have a solid basis to answer the most frequently encountered statistical questions. Importantly, the fundamental knowledge about probability distributions that is presented here will also provide a solid basis to learn about inferential statistics in the next modules."
69,Sampling Distributions,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"Methods for summarizing sample data are called descriptive statistics. However, in most studies we’re not interested in samples, but in underlying populations. If we employ data obtained from a sample to draw conclusions about a wider population, we are using methods of inferential statistics. It is therefore of essential importance that you know how you should draw samples. In this module we’ll pay attention to good sampling methods as well as some poor practices. To draw conclusions about the population a sample is from, researchers make use of a probability distribution that is very important in the world of statistics: the sampling distribution. We’ll discuss sampling distributions in great detail and compare them to data distributions and population distributions. We’ll look at the sampling distribution of the sample mean and the sampling distribution of the sample proportion. "
70,Confidence Intervals,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"We can distinguish two types of statistical inference methods. We can: (1) estimate population parameters; and (2) test hypotheses about these parameters. In this module we’ll talk about the first type of inferential statistics: estimation by means of a confidence interval. A confidence interval is a range of numbers, which, most likely, contains the actual population value. The  probability that the interval actually contains the population value is what we call the confidence level. In this module we’ll show you how you can construct confidence intervals for means and proportions and how you should interpret them. We’ll also pay attention to how you can decide how large your sample size should be."
71,Significance Tests,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"In this module we’ll talk about statistical hypotheses. They form the main ingredients of the method of significance testing. An hypothesis is nothing more than an expectation about a population. When we conduct a significance test, we use (just like when we construct a confidence interval) sample data to draw inferences about population parameters. The significance test is, therefore, also a method of inferential statistics. We’ll show that each significance test is based on two hypotheses:  the null hypothesis and  the alternative hypothesis. When you do a significance test, you assume that the null hypothesis is true unless your data provide strong evidence against it. We’ll show you how you can conduct a significance test about a mean and how you can conduct a test about a proportion. We’ll also demonstrate that significance tests and confidence intervals are closely related. We conclude the module by arguing that you can make right and wrong decisions while doing a test. Wrong decisions are referred to as Type I and Type II errors."
72,Exam time!,9,https://www.coursera.org/learn/basic-statistics?specialization=social-science,"This is the final module, where you can apply everything you've learned until now in the final exam. Please note that you can only take the final exam once a month, so make sure you are fully prepared to take the test. Please follow the honor code and do not communicate or confer with others while taking this exam. Good luck! "
73,Before we get started...,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,[formatted text here]
74,Comparing two groups,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,"In this second module of week 1 we dive right in with a quick refresher on statistical hypothesis testing. Since we're assuming you just completed the course Basic Statistics, our treatment is a little more abstract and we go really fast! We provide the relevant Basic Statistics videos in case you need a gentler introduction. After the refresher we discuss methods to compare two groups on a categorical or quantitative dependent variable. We use different test for independent and dependent groups."
75,Categorical association,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,In this module we tackle categorical association. We'll mainly discuss the Chi-squared test that allows us to decide whether two categorical variables are related in the population. If two categorical variables are unrelated you would expect that categories of these variables don't 'go together'. You would expect the number of cases in each category of one variable to be proportionally similar at each level of the other variable. The Chi-squared test helps us to compare the actual number of cases for each combination of categories (the joint frequencies) to the expected number of cases if the variables are unrelated.
76,Simple regression,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,"In this module we’ll see how to describe the association between two quantitative variables using simple (linear) regression analysis. Regression analysis allows us to model the relation between two quantitative variables and - based on our sample -decide whether a 'real' relation exists in the population. Regression analysis is more useful than just calculating a correlation coefficient, since it allows us assess how well our regression line fits the data, it helps us to identify outliers and to predict scores on the dependent variable for new cases."
77,Multiple regression,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,"In this module we’ll see how we can use more than one predictor to describe or predict a quantitative outcome variable. In the social sciences relations between psychological and social variables are generally not very strong, since outcomes are generally influences by complex processes involving many variables. So it really helps to be able to describe an outcome variable with several predictors, not just to increase the fit of the model, but also to assess the individual contribution of each predictor, while controlling for the others. "
78,Analysis of variance,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,"In this module we'll discuss analysis of variance, a very popular technique that allows us to compare more than two groups on a quantitative dependent variable. The reason we call it analysis of variance is because we compare two estimates of the variance in the population. If the group means differ in the population then these variance estimates differ. Just like in multiple regression, factorial analysis of variance allows us to investigate the influence of several independent variables."
79,Non-parametric tests,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,"In this module we'll discuss the last topic of this course: Non-parametric tests. Until now we've mostly considered tests that require assumptions about the shape of the distribution (z-tests, t-tests and F-tests). Sometimes those assumptions don't hold. Non-parametric tests require fewer of those assumptions. There are several non-parametric tests that correspond to the parametric z-, t- and F-tests. These tests also come in handy when the response variable is an ordered categorical variable as opposed to a quantitative variable. There are also non-parametric equivalents to the correlation coefficient and some tests that have no parametric-counterparts."
80,Exam time!,8,https://www.coursera.org/learn/inferential-statistics?specialization=social-science,"In this final module there's no new material to study. We advise you to take some extra time to review the material from the previous modules and to practice for the final exam. We've provided a practice exam that you can take as many times as you like. The final exam is structured exactly like the practice exam, so you know what to expect. Please note that you can only take the final exam twice every seven days, so make sure you are fully prepared. Please follow the honor code and do not communicate or confer with others while taking this exam or after. In the open questions of the exam (i.e. those that are not multiple choice) you should report your answers to 3 decimal places, and use 5 decimal places in your calculations. Good luck!"
81,About the Final Research Project,10,https://www.coursera.org/learn/social-science-capstone,"This section provides all the general course information. You can find info on how this very special final research project is organized, what the important deadlines are, and how the grading works."
82,Preparing for Milestone 1 - Research topic,10,https://www.coursera.org/learn/social-science-capstone,"In this section you will learn about the research topic in preparation of reaching your first milestone - formulating a general research hypothesis and design. Make sure you read the material well, you will be working on this research topic for the next six weeks!"
83,Milestone 1 - General hypothesis and design,10,https://www.coursera.org/learn/social-science-capstone,In this section you will work on Milestone 1 - formulating a general hypothesis and design. 
84,"Milestone 2 - Design, operationalizations and expectations",10,https://www.coursera.org/learn/social-science-capstone,In this module - based on the collective general hypothesis and design - you will create a more detailed design.
85,Milestone 3 - Measurement and manipulation material,10,https://www.coursera.org/learn/social-science-capstone,In this module - based on the collective detailed design - you will create instruments to measure and manipulate the variables in our study.
86,"Milestone 4 - Data collection, methods documentation and  analysis plan",10,https://www.coursera.org/learn/social-science-capstone,In this module you will document the methods and material used to perform the study and you will create a statistical analysis plan.
87,Milestone 5 - Statistical analysis,10,https://www.coursera.org/learn/social-science-capstone,In this module you will perform and document the final statistical analyses
88,Milestone 6 - Report,10,https://www.coursera.org/learn/social-science-capstone,"In this module you will work on the final milestone, reporting the results of your research study."
89,Putting it all together (catch-up week),10,https://www.coursera.org/learn/social-science-capstone,"In this module we'll reflect on the outcome of our study and look back on the entire research process. If you're behind schedule submitting your last milestone assignments, there's still time to catch-up!"
90,Reflection (catch-up week),10,https://www.coursera.org/learn/social-science-capstone,In this final module we'll reflect on this final research project course and the entire specialization. Last chance to catch-up and give us feedback!
91,Week 1: Probability & Expected Values,4,https://www.coursera.org/learn/statistical-inference?specialization=data-science-statistics-machine-learning,"This week, we'll focus on the fundamentals including probability, random variables, expectations and more. "
92,"Week 2: Variability, Distribution, & Asymptotics",4,https://www.coursera.org/learn/statistical-inference?specialization=data-science-statistics-machine-learning,"We're going to tackle variability, distributions, limits, and confidence intervals."
93,"Week: Intervals, Testing, & Pvalues",4,https://www.coursera.org/learn/statistical-inference?specialization=data-science-statistics-machine-learning,"We will be taking a look at intervals, testing, and pvalues in this lesson."
94,"Week 4: Power, Bootstrapping, & Permutation Tests",4,https://www.coursera.org/learn/statistical-inference?specialization=data-science-statistics-machine-learning,"We will begin looking into power, bootstrapping, and permutation tests."
95,Week 1: Least Squares and Linear Regression,4,https://www.coursera.org/learn/regression-models?specialization=data-science-statistics-machine-learning,"This week, we focus on least squares and linear regression."
96,Week 2: Linear Regression & Multivariable Regression,4,https://www.coursera.org/learn/regression-models?specialization=data-science-statistics-machine-learning,"This week, we will work through the remainder of linear regression and then turn to the first part of  multivariable regression."
97,"Week 3: Multivariable Regression, Residuals, & Diagnostics",4,https://www.coursera.org/learn/regression-models?specialization=data-science-statistics-machine-learning,"This week, we'll build on last week's introduction to multivariable regression with some examples and then cover residuals, diagnostics, variance inflation, and model comparison. "
98,Week 4: Logistic Regression and Poisson Regression,4,https://www.coursera.org/learn/regression-models?specialization=data-science-statistics-machine-learning,"This week, we will work on generalized linear models, including binary outcomes and Poisson regression. "
99,"Week 1: Prediction, Errors, and Cross Validation",4,https://www.coursera.org/learn/practical-machine-learning?specialization=data-science-statistics-machine-learning,"This week will cover prediction, relative importance of steps, errors, and cross validation."
100,Week 2: The Caret Package,4,https://www.coursera.org/learn/practical-machine-learning?specialization=data-science-statistics-machine-learning,"This week will introduce the caret package, tools for creating features and preprocessing."
101,"Week 3: Predicting with trees, Random Forests, & Model Based Predictions",4,https://www.coursera.org/learn/practical-machine-learning?specialization=data-science-statistics-machine-learning,This week we introduce a number of machine learning algorithms you can use to complete your course project.
102,Week 4: Regularized Regression and Combining Predictors,4,https://www.coursera.org/learn/practical-machine-learning?specialization=data-science-statistics-machine-learning,"This week, we will cover regularized regression and combining predictors.  "
103,Course Overview,5,https://www.coursera.org/learn/data-products?specialization=data-science-statistics-machine-learning,"In this overview module, we'll go over some information and resources to help you get started and succeed in the course. "
104,"Shiny, GoogleVis, and Plotly",5,https://www.coursera.org/learn/data-products?specialization=data-science-statistics-machine-learning,"Now we can turn to the first substantive lessons. In this module, you'll learn how to develop basic applications and interactive graphics in shiny, compose interactive HTML graphics with GoogleVis, and prepare data visualizations with Plotly."
105,R Markdown and Leaflet,5,https://www.coursera.org/learn/data-products?specialization=data-science-statistics-machine-learning,"During this module, we'll learn how to create R Markdown files and embed R code in an Rmd. We'll also explore Leaflet and use it to create interactive annotated maps."
106,R Packages,5,https://www.coursera.org/learn/data-products?specialization=data-science-statistics-machine-learning,"In this module, we'll dive into the world of creating R packages and practice developing an R Markdown presentation that includes a data visualization built using Plotly."
107,Swirl and Course Project ,5,https://www.coursera.org/learn/data-products?specialization=data-science-statistics-machine-learning,"Week 4 is all about the Course Project, producing a Shiny Application and reproducible pitch."
108,"Overview, Understanding the Problem, and Getting the Data",7,https://www.coursera.org/learn/data-science-project,"This week, we introduce the project so you can get a clear grip on the problem at hand and begin working with the dataset."
109,Exploratory Data Analysis and Modeling,7,https://www.coursera.org/learn/data-science-project,"This week, we move on to the next tasks, exploratory data analysis and modeling. You'll also submit your milestone report and review submissions from your classmates."
110,Prediction Model,7,https://www.coursera.org/learn/data-science-project,"This week, you'll build and evaluate your prediction model. The goal is to make your model efficient and accurate. "
111,Creative Exploration,7,https://www.coursera.org/learn/data-science-project,This week's goal is to improve the predictive accuracy while reducing computational runtime and model complexity.
112,Data Product ,7,https://www.coursera.org/learn/data-science-project,"This week, you'll work on developing the first component of your final project, your data product. "
113,Slide Deck,7,https://www.coursera.org/learn/data-science-project,"This week, you'll work on developing the second component of your final project, a slide deck to accompany your data product. "
114,Final Project Submission and Evaluation ,7,https://www.coursera.org/learn/data-science-project,"This week, you'll submit your final project and review the work of your classmates."
115,Introduction to Spreadsheets,4,https://www.coursera.org/learn/excel-data-analysis?specialization=business-statistics-analysis,"Introduction to spreadsheets, reading data, manipulating data. Basic spreadsheet operations and functions."
116,Spreadsheet Functions to Organize Data,4,https://www.coursera.org/learn/excel-data-analysis?specialization=business-statistics-analysis,"Introduction to some more useful functions such as the IF, nested IF, VLOOKUP and HLOOKUP functions in Excel."
117,"Introduction to Filtering, Pivot Tables, and Charts",4,https://www.coursera.org/learn/excel-data-analysis?specialization=business-statistics-analysis,"Introduction to the Data filtering capabilities of Excel, the construction of Pivot Tables to organize data and introduction to charts in Excel."
118,Advanced Graphing and Charting,4,https://www.coursera.org/learn/excel-data-analysis?specialization=business-statistics-analysis,"Constructing various Line, Bar and Pie charts. Using the Pivot chart features of Excel. Understanding and constructing Histograms and Scatterplots."
119,Basic Data Descriptors,4,https://www.coursera.org/learn/descriptive-statistics-statistical-distributions-business-application?specialization=business-statistics-analysis,
120,"Descriptive Measures of Association, Probability, and Statistical Distributions",4,https://www.coursera.org/learn/descriptive-statistics-statistical-distributions-business-application?specialization=business-statistics-analysis,
121,The Normal Distribution,4,https://www.coursera.org/learn/descriptive-statistics-statistical-distributions-business-application?specialization=business-statistics-analysis,
122,"Working with Distributions (Normal, Binomial, Poisson), Population and Sample Data",4,https://www.coursera.org/learn/descriptive-statistics-statistical-distributions-business-application?specialization=business-statistics-analysis,
123,Confidence Interval - Introduction,4,https://www.coursera.org/learn/hypothesis-testing-confidence-intervals?specialization=business-statistics-analysis,
124,Confidence Interval - Applications,4,https://www.coursera.org/learn/hypothesis-testing-confidence-intervals?specialization=business-statistics-analysis,
125,Hypothesis Testing,4,https://www.coursera.org/learn/hypothesis-testing-confidence-intervals?specialization=business-statistics-analysis,
126,Hypothesis Test - Differences in Mean,4,https://www.coursera.org/learn/hypothesis-testing-confidence-intervals?specialization=business-statistics-analysis,
127,Regression Analysis: An Introduction,4,https://www.coursera.org/learn/linear-regression-business-statistics?specialization=business-statistics-analysis,
128,Regression Analysis: Hypothesis Testing and Goodness of Fit,4,https://www.coursera.org/learn/linear-regression-business-statistics?specialization=business-statistics-analysis,
129,"Regression Analysis: Dummy Variables, Multicollinearity",4,https://www.coursera.org/learn/linear-regression-business-statistics?specialization=business-statistics-analysis,
130,Regression Analysis: Various Extensions,4,https://www.coursera.org/learn/linear-regression-business-statistics?specialization=business-statistics-analysis,
131,Business Statistics and Analysis Capstone: An Introduction,4,https://www.coursera.org/learn/business-statistics-analysis-capstone,
132,Business Statistics and Analysis Capstone: Assessments 1 & 2,4,https://www.coursera.org/learn/business-statistics-analysis-capstone,
133,Business Statistics and Analysis Capstone: Assessment 3,4,https://www.coursera.org/learn/business-statistics-analysis-capstone,
134,Business Statistics and Analysis Capstone: Assessment 4,4,https://www.coursera.org/learn/business-statistics-analysis-capstone,
135,Defining Data Science and What Data Scientists Do,3,https://www.coursera.org/learn/what-is-datascience?specialization=ibm-data-science,"In this module, you will view the course syllabus to learn what will be taught in this course. You will hear from data science professionals to discover what data science is, what data scientists do, and what tools and algorithms data scientists use on a daily basis. Finally, you will complete a reading assignment to find out why data science is considered the sexiest job in the 21st century."
136,Data Science Topics,3,https://www.coursera.org/learn/what-is-datascience?specialization=ibm-data-science,"In this module, you will hear from Norman White, the Faculty Director of the Stern Centre for Research Computing at New York University, as he talks about data science and the skills required for anyone interested in pursuing a career in this field. He also advises those looking to start a career in data science. Finally, you will complete reading assignments to learn about the process of mining a given dataset and about regression analysis."
137,Data Science in Business,3,https://www.coursera.org/learn/what-is-datascience?specialization=ibm-data-science,"In this module, you will learn about the approaches companies can take to start working with data science. You will learn about some of the qualities that differentiate data scientists from other professionals. You will also learn about analytics, story-telling, and the pivotal role data scientists play in creating an effective final deliverable. Finally, you will apply what you learned about data science by answering open-ended questions."
138,Introducing Skills Network Labs,6,https://www.coursera.org/learn/open-source-tools-for-data-science?specialization=ibm-data-science,"This week, you will get an overview of the various data science tools available to you, hosted on Skills Network Labs. You will create an account and start exploring some of the features."
139,Jupyter Notebooks,6,https://www.coursera.org/learn/open-source-tools-for-data-science?specialization=ibm-data-science,"This week, you will learn about a popular data science tool, Jupyter Notebooks, its features, and why they are so popular among data scientists today."
140,Apache Zeppelin Notebooks,6,https://www.coursera.org/learn/open-source-tools-for-data-science?specialization=ibm-data-science,"This week, you will learn about Apache Zeppelin Notebooks, its feature, and how they are different from Jupyter Notebooks."
141,RStudio IDE,6,https://www.coursera.org/learn/open-source-tools-for-data-science?specialization=ibm-data-science,"This week, you will learn about a popular data science tool used by R programmers. You'll learn about the user interface and how to use its various features."
142,IBM Watson Studio,6,https://www.coursera.org/learn/open-source-tools-for-data-science?specialization=ibm-data-science,"This week, you will learn about an enterprise-ready data science platform by IBM, called Watson Studio (formerley known as Data Science Experience). You'll learn about some of the features and capabilities of what data scientists use in the industry."
143,Project: Create and share a Jupyter Notebook,6,https://www.coursera.org/learn/open-source-tools-for-data-science?specialization=ibm-data-science,
144,From Problem to Approach and From Requirements to Collection,3,https://www.coursera.org/learn/data-science-methodology?specialization=ibm-data-science,"In this module, you will learn about why we are interested in data science, what a methodology is, and why data scientists need a methodology. You will also learn about the data science methodology and its flowchart. You will learn about the first two stages of the data science methodology, namely Business Understanding and Analytic Approach. Finally, through a lab session, you will also obtain how to complete the Business Understanding and the Analytic Approach stages and the Data Requirements and Data Collection stages pertaining to any data science problem. "
145,From Understanding to Preparation and From Modeling to Evaluation,3,https://www.coursera.org/learn/data-science-methodology?specialization=ibm-data-science,"In this module, you will learn what it means to understand data, and prepare or clean data. You will also learn about the purpose of data modeling and some characteristics of the modeling process. Finally, through a lab session, you will learn how to complete the Data Understanding and the Data Preparation stages, as well as the Modeling and the Model Evaluation stages pertaining to any data science problem."
146,From Deployment to Feedback,3,https://www.coursera.org/learn/data-science-methodology?specialization=ibm-data-science,"In this module, you will learn about what happens when a model is deployed and why model feedback is important. Also, by completing a peer-reviewed assignment, you will demonstrate your understanding of the data science methodology by applying it to a problem that you define."
147,Python Basics ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai?specialization=ibm-data-science,
148,Python Data Structures ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai?specialization=ibm-data-science,
149,Python Programming Fundamentals ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai?specialization=ibm-data-science,
150,Working with Data in Python ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai?specialization=ibm-data-science,
151,Analyzing US Economic Data and Building a Dashboard ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai?specialization=ibm-data-science,
152,Data Science Fundamentals,4,https://www.coursera.org/learn/data-scientists-tools?specialization=jhu-data-science,"In this module, we'll introduce and define data science and data itself. We'll also go over some of the resources that data scientists use to get help when they're stuck."
153,R and RStudio,4,https://www.coursera.org/learn/data-scientists-tools?specialization=jhu-data-science,"In this module, we'll help you get up and running with both R and RStudio. Along the way, you'll learn some basics about both and why data scientists use them. "
154,Version Control and GitHub,4,https://www.coursera.org/learn/data-scientists-tools?specialization=jhu-data-science,"During this module, you'll learn about version control and why it's so important to data scientists. You'll also learn how to use Git and GitHub to manage version control in data science projects."
155,"R Markdown, Scientific Thinking, and Big Data ",4,https://www.coursera.org/learn/data-scientists-tools?specialization=jhu-data-science,"During this final module, you'll learn to use R Markdown and get an introduction to three concepts that are incredibly important to every successful data scientist: asking good questions, experimental design, and big data. "
156,"Week 1: Background, Getting Started, and Nuts & Bolts",4,https://www.coursera.org/learn/r-programming?specialization=jhu-data-science,"This week covers the basics to get you started up with R. The Background Materials lesson contains information about course mechanics and some videos on installing R. The Week 1 videos cover the history of R and S, go over the basic data types in R, and describe the functions for reading and writing data. I recommend that you watch the videos in the listed order, but watching the videos out of order isn't going to ruin the story. "
157,Week 2: Programming with R,4,https://www.coursera.org/learn/r-programming?specialization=jhu-data-science,"Welcome to Week 2 of R Programming. This week, we take the gloves off, and the lectures cover key topics like control structures and functions. We also introduce the first programming assignment for the course, which is due at the end of the week."
158,Week 3: Loop Functions and Debugging,4,https://www.coursera.org/learn/r-programming?specialization=jhu-data-science,"We have now entered the third week of R Programming, which also marks the halfway point. The lectures this week cover loop functions and the debugging tools in R. These aspects of R make R useful for both interactive work and writing longer code, and so they are commonly used in practice."
159,Week 4: Simulation & Profiling,4,https://www.coursera.org/learn/r-programming?specialization=jhu-data-science,"This week covers how to simulate data in R, which serves as the basis for doing simulation studies. We also cover the profiler in R which lets you collect detailed information on how your R functions are running and to identify bottlenecks that can be addressed. The profiler is a key tool in helping you optimize your programs. Finally, we cover the str function, which I personally believe is the most useful function in R."
160,Week 1,4,https://www.coursera.org/learn/data-cleaning?specialization=jhu-data-science,"In this first week of the course, we look at finding data and reading different file types."
161,Week 2,4,https://www.coursera.org/learn/data-cleaning?specialization=jhu-data-science,Welcome to Week 2 of Getting and Cleaning Data! The primary goal is to introduce you to the most common data storage systems and the appropriate tools to extract data from web or from databases like MySQL. 
162,Week 3,4,https://www.coursera.org/learn/data-cleaning?specialization=jhu-data-science,"Welcome to Week 3 of Getting and Cleaning Data! This week the lectures will focus on organizing, merging and managing the data you have collected using the lectures from Weeks 1 and 2. "
163,Week 4,4,https://www.coursera.org/learn/data-cleaning?specialization=jhu-data-science,Welcome to Week 4 of Getting and Cleaning Data! This week we finish up with lectures on text and date manipulation in R. In this final week we will also focus on peer grading of Course Projects. 
164,Week 1,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=jhu-data-science,This week covers the basics of analytic graphics and the base plotting system in R. We've also included some background material to help you install R if you haven't done so already. 
165,Week 2,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=jhu-data-science,"Welcome to Week 2 of Exploratory Data Analysis. This week covers some of the more advanced graphing systems available in R: the Lattice system and the ggplot2 system. While the base graphics system provides many important tools for visualizing data, it was part of the original R system and lacks many features that may be desirable in a plotting system, particularly when visualizing high dimensional data. The Lattice and ggplot2 systems also simplify the laying out of plots making it a much less tedious process."
166,Week 3,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=jhu-data-science,Welcome to Week 3 of Exploratory Data Analysis. This week covers some of the workhorse statistical methods for exploratory analysis. These methods include clustering and dimension reduction techniques that allow you to make graphical displays of very high dimensional data (many many variables). We also cover novel ways to specify colors in R so that you can use color as an important and useful dimension when making data graphics. All of this material is covered in chapters 9-12 of my book Exploratory Data Analysis with R.
167,Week 4,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=jhu-data-science,"This week, we'll look at two case studies in exploratory data analysis. The first involves the use of cluster analysis techniques, and the second is a more involved analysis of some air pollution data. How one goes about doing EDA is often personal, but I'm providing these videos to give you a sense of how you might proceed with a specific type of dataset. "
168,"Week 1: Concepts, Ideas, & Structure",4,https://www.coursera.org/learn/reproducible-research,"This week will cover the basic ideas of reproducible research since they may be unfamiliar to some of you. We also cover structuring and organizing a data analysis to help make it more reproducible. I recommend that you watch the videos in the order that they are listed on the web page, but watching the videos out of order isn't going to ruin the story. "
169,Week 2: Markdown & knitr,4,https://www.coursera.org/learn/reproducible-research,This week we cover some of the core tools for developing reproducible documents. We cover the literate programming tool knitr and show how to integrate it with Markdown to publish reproducible web documents. We also introduce the first peer assessment which will require you to write up a reproducible data analysis using knitr. 
170,Week 3: Reproducible Research Checklist & Evidence-based Data Analysis,4,https://www.coursera.org/learn/reproducible-research,"This week covers what one could call a basic check list for ensuring that a data analysis is reproducible. While it's not absolutely sufficient to follow the check list, it provides a necessary minimum standard that would be applicable to almost any area of analysis."
171,Week 4: Case Studies & Commentaries,4,https://www.coursera.org/learn/reproducible-research,This week there are two case studies involving the importance of reproducibility in science for you to watch.
172,Week 1: Probability & Expected Values,4,https://www.coursera.org/learn/statistical-inference,"This week, we'll focus on the fundamentals including probability, random variables, expectations and more. "
173,"Week 2: Variability, Distribution, & Asymptotics",4,https://www.coursera.org/learn/statistical-inference,"We're going to tackle variability, distributions, limits, and confidence intervals."
174,"Week: Intervals, Testing, & Pvalues",4,https://www.coursera.org/learn/statistical-inference,"We will be taking a look at intervals, testing, and pvalues in this lesson."
175,"Week 4: Power, Bootstrapping, & Permutation Tests",4,https://www.coursera.org/learn/statistical-inference,"We will begin looking into power, bootstrapping, and permutation tests."
176,Week 1: Least Squares and Linear Regression,4,https://www.coursera.org/learn/regression-models,"This week, we focus on least squares and linear regression."
177,Week 2: Linear Regression & Multivariable Regression,4,https://www.coursera.org/learn/regression-models,"This week, we will work through the remainder of linear regression and then turn to the first part of  multivariable regression."
178,"Week 3: Multivariable Regression, Residuals, & Diagnostics",4,https://www.coursera.org/learn/regression-models,"This week, we'll build on last week's introduction to multivariable regression with some examples and then cover residuals, diagnostics, variance inflation, and model comparison. "
179,Week 4: Logistic Regression and Poisson Regression,4,https://www.coursera.org/learn/regression-models,"This week, we will work on generalized linear models, including binary outcomes and Poisson regression. "
180,"Week 1: Prediction, Errors, and Cross Validation",4,https://www.coursera.org/learn/practical-machine-learning,"This week will cover prediction, relative importance of steps, errors, and cross validation."
181,Week 2: The Caret Package,4,https://www.coursera.org/learn/practical-machine-learning,"This week will introduce the caret package, tools for creating features and preprocessing."
182,"Week 3: Predicting with trees, Random Forests, & Model Based Predictions",4,https://www.coursera.org/learn/practical-machine-learning,This week we introduce a number of machine learning algorithms you can use to complete your course project.
183,Week 4: Regularized Regression and Combining Predictors,4,https://www.coursera.org/learn/practical-machine-learning,"This week, we will cover regularized regression and combining predictors.  "
184,Course Overview,5,https://www.coursera.org/learn/data-products,"In this overview module, we'll go over some information and resources to help you get started and succeed in the course. "
185,"Shiny, GoogleVis, and Plotly",5,https://www.coursera.org/learn/data-products,"Now we can turn to the first substantive lessons. In this module, you'll learn how to develop basic applications and interactive graphics in shiny, compose interactive HTML graphics with GoogleVis, and prepare data visualizations with Plotly."
186,R Markdown and Leaflet,5,https://www.coursera.org/learn/data-products,"During this module, we'll learn how to create R Markdown files and embed R code in an Rmd. We'll also explore Leaflet and use it to create interactive annotated maps."
187,R Packages,5,https://www.coursera.org/learn/data-products,"In this module, we'll dive into the world of creating R packages and practice developing an R Markdown presentation that includes a data visualization built using Plotly."
188,Swirl and Course Project ,5,https://www.coursera.org/learn/data-products,"Week 4 is all about the Course Project, producing a Shiny Application and reproducible pitch."
189,"Overview, Understanding the Problem, and Getting the Data",7,https://www.coursera.org/learn/data-science-project,"This week, we introduce the project so you can get a clear grip on the problem at hand and begin working with the dataset."
190,Exploratory Data Analysis and Modeling,7,https://www.coursera.org/learn/data-science-project,"This week, we move on to the next tasks, exploratory data analysis and modeling. You'll also submit your milestone report and review submissions from your classmates."
191,Prediction Model,7,https://www.coursera.org/learn/data-science-project,"This week, you'll build and evaluate your prediction model. The goal is to make your model efficient and accurate. "
192,Creative Exploration,7,https://www.coursera.org/learn/data-science-project,This week's goal is to improve the predictive accuracy while reducing computational runtime and model complexity.
193,Data Product ,7,https://www.coursera.org/learn/data-science-project,"This week, you'll work on developing the first component of your final project, your data product. "
194,Slide Deck,7,https://www.coursera.org/learn/data-science-project,"This week, you'll work on developing the second component of your final project, a slide deck to accompany your data product. "
195,Final Project Submission and Evaluation ,7,https://www.coursera.org/learn/data-science-project,"This week, you'll submit your final project and review the work of your classmates."
196,Introduction to optimization,6,https://www.coursera.org/learn/intro-to-deep-learning?specialization=aml,"Welcome to the ""Introduction to Deep Learning"" course! In the first week you'll learn about linear models and stochatic optimization methods. Linear models are basic building blocks for many deep architectures, and stochastic optimization is used to learn every model that we'll discuss in our course."
197,Introduction to neural networks,6,https://www.coursera.org/learn/intro-to-deep-learning?specialization=aml,This module is an introduction to the concept of a deep neural network. You'll begin with the linear model and finish with writing your very first deep network.
198,Deep Learning for images,6,https://www.coursera.org/learn/intro-to-deep-learning?specialization=aml,In this week you will learn about building blocks of deep learning for image input. You will learn how to build Convolutional Neural Network (CNN) architectures with these blocks and how to quickly solve a new task using so-called pre-trained models.
199,Unsupervised representation learning,6,https://www.coursera.org/learn/intro-to-deep-learning?specialization=aml,"This week we're gonna dive into unsupervised parts of deep learning. You'll learn how to generate, morph and search images with deep learning."
200,Deep learning for sequences,6,https://www.coursera.org/learn/intro-to-deep-learning?specialization=aml,"In this week you will learn how to use deep learning for sequences such as texts, video, audio, etc. You will learn about several Recurrent Neural Network (RNN) architectures and how to apply them for different tasks with sequential input/output."
201,Final Project,6,https://www.coursera.org/learn/intro-to-deep-learning?specialization=aml,In this week you will apply all your knowledge about neural networks for images and texts for the final project. You will solve the task of generating descriptions for real world images!
202,Introduction & Recap,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"This week we will introduce you to competitive data science. You will learn about competitions' mechanics, the difference between competitions and a real life data science,  hardware and software that people usually use in competitions. We will also briefly recap major ML models frequently used in competitions."
203,Feature Preprocessing and Generation with Respect to Models,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"In this module we will summarize approaches to work with features: preprocessing, generation and extraction. We will see, that the choice of the machine learning model impacts both preprocessing we apply to the features and our approach to generation of new ones. We will also discuss feature extraction from text with Bag Of Words and Word2vec, and feature extraction from images with Convolution Neural Networks."
204,Final Project Description,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"This is just a reminder, that the final project in this course is better to start soon! The final project is in fact a competition, in this module you can find an information about it."
205,Exploratory Data Analysis,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,We will start this week with Exploratory Data Analysis (EDA). It is a very broad and exciting topic and an essential component of solving process. Besides regular videos you will find a walk through EDA process for Springleaf competition data and an example of prolific EDA for NumerAI competition with extraordinary findings.
206,Validation,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,In this module we will discuss various validation strategies. We will see that the strategy we choose depends on the competition setup and that correct validation scheme is one of the bricks for any winning solution.   
207,Data Leakages,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"Finally, in this module we will cover something very unique to data science competitions. That is, we will see examples how it is sometimes possible to get a top position in a competition with a very little machine learning, just by exploiting a data leakage.   "
208,Metrics Optimization,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"This week we will first study another component of the competitions: the evaluation metrics. We will recap the most prominent ones and then see, how we can efficiently optimize a metric given in a competition."
209,Advanced Feature Engineering I,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"In this module we will study a very powerful technique for feature generation. It has a lot of names, but here we call it ""mean encodings"". We will see the intuition behind them, how to construct them, regularize and extend them.    "
210,Hyperparameter Optimization,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"In this module we will talk about hyperparameter optimization process. We will also have a special video with practical tips and tricks, recorded by four instructors."
211,Advanced feature engineering II,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,In this module we will learn about a few more advanced feature engineering techniques.
212,Ensembling,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"Nowadays it is hard to find a competition won by a single model! Every winning solution incorporates ensembles of models. In this module we will talk about the main ensembling techniques in general, and, of course, how it is better to ensemble the models in practice. "
213,Competitions go through,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,"For the 5th week we've prepared for you several ""walk-through"" videos. In these videos we discuss solutions to competitions we took prizes at. The video content is quite short this week to let you spend more time on the final project. Good luck!"
214,Final Project,13,https://www.coursera.org/learn/competitive-data-science?specialization=aml,Final project for the course.
215,Introduction to Bayesian methods & Conjugate priors,7,https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml,Welcome to first week of our course! Today we will discuss what bayesian methods are and what are probabilistic models. We will see how they can be used to model real-life situations and how to make conclusions from them. We will also learn about conjugate priors — a class of models where all math becomes really simple.
216,Expectation-Maximization algorithm,7,https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml,"This week we will about the central topic in probabilistic modeling: the Latent Variable Models and how to train them, namely the Expectation Maximization algorithm. We will see models for clustering and dimensionality reduction where Expectation Maximization algorithm can be applied as is. In the following weeks, we will spend weeks 3, 4, and 5 discussing numerous extensions to this algorithm to make it work for more complicated models and scale to large datasets."
217,Variational Inference & Latent Dirichlet Allocation,7,https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml,This week we will move on to approximate inference methods. We will see why we care about approximating distributions and see variational inference — one of the most powerful methods for this task. We will also see mean-field approximation in details. And apply it to text-mining algorithm called Latent Dirichlet Allocation
218,Markov chain Monte Carlo,7,https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml,This week we will learn how to approximate training and inference with sampling and how to sample from complicated distributions. This will allow us to build simple method to deal with LDA and with Bayesian Neural Networks — Neural Networks which weights are random variables themselves and instead of training (finding the best value for the weights) we will sample from the posterior distributions on weights.
219,Variational Autoencoder,7,https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml,"Welcome to the fifth week of the course! This week we will combine many ideas from the previous weeks and add some new to build Variational Autoencoder -- a model that can learn a distribution over structured data (like photographs or molecules) and then sample new data points from the learned distribution, hallucinating new photographs of non-existing people. We will also the same techniques to Bayesian Neural Networks and will see how this can greatly compress the weights of the network without reducing the accuracy."
220,Gaussian processes & Bayesian optimization,7,https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml,"Welcome to the final week of our course! This time we will see nonparametric Bayesian methods. Specifically, we will learn about Gaussian processes and their application to Bayesian optimization that allows one to perform optimization for scenarios in which each function evaluation is very expensive: oil probe, drug discovery and neural network architecture tuning."
221,Final project,7,https://www.coursera.org/learn/bayesian-methods-in-machine-learning?specialization=aml,In this module you will apply methods that you learned in this course to this final project
222,Intro: why should I care?,6,https://www.coursera.org/learn/practical-rl?specialization=aml,"In this module we gonna define and ""taste"" what reinforcement learning is about. We'll also learn one simple algorithm that can solve reinforcement learning problems with embarrassing efficiency."
223,At the heart of RL: Dynamic Programming,6,https://www.coursera.org/learn/practical-rl?specialization=aml,"This week we'll consider the reinforcement learning formalisms in a more rigorous, mathematical way.  You'll learn how to effectively compute the return your agent gets for a particular action - and how to pick best actions based on that return."
224,Model-free methods,6,https://www.coursera.org/learn/practical-rl?specialization=aml,This week we'll find out how to apply last week's ideas to the real world problems: ones where you don't have a perfect model of your environment.
225,Approximate Value Based Methods,6,https://www.coursera.org/learn/practical-rl?specialization=aml,This week we'll learn to scale things even farther up by training agents based on neural networks.
226,Policy-based methods,6,https://www.coursera.org/learn/practical-rl?specialization=aml,"We spent 3 previous modules working on the value-based methods: learning state values, action values and whatnot. Now's the time to see an alternative approach that doesn't require you to predict all future rewards to learn something."
227,Exploration,6,https://www.coursera.org/learn/practical-rl?specialization=aml,"In this final week you'll learn how to build better exploration strategies with a focus on contextual bandit setup. In honor track, you'll also learn how to apply reinforcement learning to train structured deep learning models."
228,Introduction to Linear Algebra and to Mathematics for Machine Learning,5,https://www.coursera.org/learn/linear-algebra-machine-learning,"In this first module we look at how linear algebra is relevant to machine learning and data science. Then we'll wind up the module with an initial introduction to vectors. Throughout, we're focussing on developing your mathematical intuition, not of crunching through algebra or doing long pen-and-paper examples. For many of these operations, there are callable functions in Python that can do the adding up - the point is to appreciate what they do and how they work so that, when things go wrong or there are special cases, you can understand why and what to do."
229,Vectors are objects that move around space,5,https://www.coursera.org/learn/linear-algebra-machine-learning,"In this module, we look at operations we can do with vectors - finding the modulus (size), angle between vectors (dot or inner product) and projections of one vector onto another. We can then examine how the entries describing a vector will depend on what vectors we use to define the axes - the basis. That will then let us determine whether a proposed set of basis vectors are what's called 'linearly independent.' This will complete our examination of vectors, allowing us to move on to matrices in module 3 and then start to solve linear algebra problems."
230,Matrices in Linear Algebra: Objects that operate on Vectors,5,https://www.coursera.org/learn/linear-algebra-machine-learning,"Now that we've looked at vectors, we can turn to matrices.  First we look at how to use matrices as tools to solve linear algebra problems, and as objects that transform vectors. Then we look at how to solve systems of linear equations using matrices, which will then take us on to look at inverse matrices and determinants, and to think about what the determinant really is, intuitively speaking. Finally, we'll look at cases of special matrices that mean that the determinant is zero or where the matrix isn't invertible - cases where algorithms that need to invert a matrix will fail."
231,Matrices make linear mappings,5,https://www.coursera.org/learn/linear-algebra-machine-learning,"In Module 4, we continue our discussion of matrices; first we think about how to code up matrix multiplication and matrix operations using the Einstein Summation Convention, which is a widely used notation in more advanced linear algebra courses. Then, we look at how matrices can transform a description of a vector from one basis (set of axes) to another. This will allow us to, for example, figure out how to apply a reflection to an image and manipulate images. We'll also look at how to construct a convenient basis vector set in order to do such transformations. Then, we'll write some code to do these transformations and apply this work computationally."
232,Eigenvalues and Eigenvectors: Application to Data Problems,5,https://www.coursera.org/learn/linear-algebra-machine-learning,"Eigenvectors are particular vectors that are unrotated by a transformation matrix, and eigenvalues are the amount by which the eigenvectors are stretched. These special 'eigen-things' are very useful in linear algebra and will let us examine Google's famous PageRank algorithm for presenting web search results. Then we'll apply this in code, which will wrap up the course."
233,What is calculus?,6,https://www.coursera.org/learn/multivariate-calculus-machine-learning,"Understanding calculus is central to understanding machine learning! You can think of calculus as simply a set of tools for analysing the relationship between functions and their inputs. Often, in machine learning, we are trying to find the inputs which enable a function to best match the data. We start this module from the basics, by recalling what a function is and where we might encounter one. Following this, we talk about the how, when sketching a function on a graph, the slope describes the rate of change of the output with respect to an input. Using this visual intuition we next derive a robust mathematical definition of a derivative, which we then use to differentiate some interesting functions. Finally, by studying a few examples, we develop four handy time saving rules that enable us to speed up differentiation for many common scenarios."
234,Multivariate calculus,6,https://www.coursera.org/learn/multivariate-calculus-machine-learning,"Building on the foundations of the previous module, we now generalise our calculus tools to handle multivariable systems. This means we can take a function with multiple inputs and determine the influence of each of them separately. It would not be unusual for a machine learning method to require the analysis of a function with thousands of inputs, so we will also introduce the linear algebra structures necessary for storing the results of our multivariate calculus analysis in an orderly fashion.  "
235,Multivariate chain rule and its applications,6,https://www.coursera.org/learn/multivariate-calculus-machine-learning,"Having seen that multivariate calculus is really no more complicated than the univariate case, we now focus on applications of the chain rule. Neural networks are one of the most popular and successful conceptual structures in machine learning. They are build up from a connected web of neurons and inspired by the structure of biological brains. The behaviour of each neuron is influenced by a set of control parameters, each of which needs to be optimised to best fit the data. The multivariate chain rule can be used to calculate the influence of each parameter of the networks, allow them to be updated during training. "
236,Taylor series and linearisation,6,https://www.coursera.org/learn/multivariate-calculus-machine-learning,"The Taylor series is a method for re-expressing functions as polynomial series. This approach is the rational behind the use of simple linear approximations to complicated functions. In this module, we will derive the formal expression for the univariate Taylor series and discuss some important consequences of this result relevant to machine learning. Finally, we will discuss the multivariate case and see how the Jacobian and the Hessian come in to play. "
237,Intro to optimisation,6,https://www.coursera.org/learn/multivariate-calculus-machine-learning,"If we want to find the minimum and maximum points of a function then we can use multivariate calculus to do this, say to optimise the parameters  (the space) of a function to fit some data.  First we’ll do this in one dimension and use the gradient to give us estimates of where the zero points of that function are, and then iterate in the Newton-Raphson method.  Then we’ll extend the idea to multiple dimensions by finding the gradient vector, Grad, which is the vector of the Jacobian.  This will then let us find our way to the minima and maxima in what is called the gradient descent method.  We’ll then take a moment to use Grad to find the minima and maxima along a constraint in the space, which is the Lagrange multipliers method."
238,Regression,6,https://www.coursera.org/learn/multivariate-calculus-machine-learning,"In order to optimise the fitting parameters of a fitting function to the best fit for some data, we need a way to define how good our fit is.  This goodness of fit is called chi-squared, which we’ll first apply to fitting a straight line - linear regression.  Then we’ll look at how to optimise our fitting function using chi-squared in the general case using the gradient descent method. Finally, we’ll look at how to do this easily in Python in just a few lines of code, which will wrap up the course."
239,Statistics of Datasets,4,https://www.coursera.org/learn/pca-machine-learning,"Principal Component Analysis (PCA) is one of the most important dimensionality reduction algorithms in machine learning. In this course, we lay the mathematical foundations to derive and understand PCA from a geometric point of view. In this module, we learn how to summarize datasets (e.g., images) using basic statistics, such as the mean and the variance. We also look at properties of the mean and the variance when we shift or scale the original data set. We will provide mathematical intuition as well as the skills to derive the results. We will also implement our results in code (jupyter notebooks), which will allow us to practice our mathematical understand to compute averages of image data sets."
240,Inner Products,4,https://www.coursera.org/learn/pca-machine-learning,"Data can be interpreted as vectors. Vectors allow us to talk about geometric concepts, such as lengths, distances and angles to characterise similarity between vectors. This will become important later in the course when we discuss PCA. In this module, we will introduce and practice the concept of an inner product. Inner products allow us to talk about geometric concepts in vector spaces. More specifically, we will start with the dot product (which we may still know from school) as a special case of an inner product, and then move toward a more general concept of an inner product, which play an integral part in some areas of machine learning, such as kernel machines (this includes support vector machines and Gaussian processes). We have a lot of exercises in this module to practice and understand the concept of inner products."
241,Orthogonal Projections,4,https://www.coursera.org/learn/pca-machine-learning,"In this module, we will look at orthogonal projections of vectors, which live in a high-dimensional vector space, onto lower-dimensional subspaces. This will play an important role in the next module when we derive PCA. We will start off with a geometric motivation of what an orthogonal projection is and work our way through the corresponding derivation. We will end up with a single equation that allows us to project any vector onto a lower-dimensional subspace. However, we will also understand how this equation came about. As in the other modules, we will have both pen-and-paper practice and a small programming example with a jupyter notebook."
242,Principal Component Analysis,4,https://www.coursera.org/learn/pca-machine-learning,"We can think of dimensionality reduction as a way of compressing data with some loss, similar to jpg or mp3. Principal Component Analysis (PCA) is one of the most fundamental dimensionality reduction techniques that are used in machine learning. In this module, we use the results from the first three modules of this course and derive PCA from a geometric point of view. Within this course, this module is the most challenging one, and we will go through an explicit derivation of PCA plus some coding exercises that will make us a proficient user of PCA. "
243,Basic Combinatorics,6,https://www.coursera.org/learn/discrete-math-and-analyzing-social-graphs,Suppose we need to count certain objects. Can we do anything better than just list all the objects? Do we need to create a list of all our data entries to check whether we have enough data to teach our ML model? Is there a way to tell whether our algorithm will run in a reasonable time before implementing and actually running it? All these questions are addressed by a mathematical field called Combinatorics. In this module we will give an introduction to this field that will help us to answer basic versions of the above questions.
244,Advanced Combinatorics,6,https://www.coursera.org/learn/discrete-math-and-analyzing-social-graphs,"In the first week we have already considered most of the standard settings in Combinatorics, that allow us to address many counting problems. However, successful application of this knowledge on practice requires considerable experience in this kind of problems. The goal of this module is twofold. First, we study extensively more advanced combinatorial settings. We discuss in more details binomial coefficients. Also, we address one more standard setting, combinations with repetitions. The second gaol of the course is to practice counting. We will gain some experience in this by discussing various problems in Combinatorics."
245,Discrete Probability,6,https://www.coursera.org/learn/discrete-math-and-analyzing-social-graphs,"Probability theory is a mathematical foundation of Statistics, the core of Data Science. During this week we study discrete probability, the first chapter of the probability theory, closely related to combinatorics. We discuss random experiments, their outcomes and events, introduce the notion of probability and some basic rules that follow immediately from the combinatorial results studied before. We also study simple probabilistic models like coin-tossing that will be used later."
246,Introduction to Graphs,6,https://www.coursera.org/learn/discrete-math-and-analyzing-social-graphs,"Graphs represent objects and relations between them in a compact geometric form. Objects are represented by vertices of a graph and relations correspond to edges. Applications of graphs include geoinformational systems (vertices are cities, edges are roads), social network analysis (people and friendship relations), chemistry (graphs of molecular structure), computer network topology, and many more. During this week, we introduce basic notions of graph theory and discuss basic algorithms on graphs."
247,Basic Graph Parameters,6,https://www.coursera.org/learn/discrete-math-and-analyzing-social-graphs,"Graph parameters, also called graph properties and graph invariants, are values (usually numerical), which are calculated for a given graph and depend only on its abstract structure (not, say, on a particular way of drawing the graph on a plane). Graph parameters are useful in data science, since they reduce a big amount of data (the graph) to a small one (the parameter), while conveying important information about the graph. We discuss some of the basic graph parameters in this module."
248,Graphs of Social Networks,6,https://www.coursera.org/learn/discrete-math-and-analyzing-social-graphs,"In this final part of the course we discuss a Python library for working with graphs, called NetworkX. In NetworkX, one can create and modify graphs, compute graph parameters, visualize graphs, etc. We shall show how NetworkX is used to operate on graphs coming from a real-world dataset."
249,"Introduction: Numerical Sets, Functions, Limits",6,https://www.coursera.org/learn/calculus-and-optimization-for-machine-learning,"Here we introduce basic concept the calculus course could not be imagine without: function. In order to properly do it, one should say that the function is a mapping from one set to another. Thus, we start with the ideas of numerical sets and mapping, then proceeding with functions itself. Since we are particularly interested in functions' graph, we spend a lot of time discussing simplest ways to produce a complex function graph from elementary case. In the second part of the week we start our calculus journey with a discrete limit, the limit of sequences, and master skills needed to calculate them."
250,Limits and Multivariate Functions,6,https://www.coursera.org/learn/calculus-and-optimization-for-machine-learning,"Now it is time to move from discrete limits to continuous ones: in other words, in the current module we are going to discuss limits of functions. We start with the basic question: does this case sufficiently differ from the sequences? Turns out, yes, it does thanks to significant structural differences between natural and real numbers. One of those differences - the continuousness - allows us to define and calculate limits at finite  moments. We spend some time specifically on the famous important limits, then we proceed with the idea of asymptotic comparison of functions, Big- and little-o notations. To top our module with, we introduce functions of several variables and spend some time getting used to conveniently plot and interpret them, finishing up with discussion of its limits."
251,Derivatives and Linear Approximations: Singlevariate Functions,6,https://www.coursera.org/learn/calculus-and-optimization-for-machine-learning,"Since we now know limits, let us use them in order to define some instantaneous characteristics of functions starting with its slope. Thus we define function's derivative and discuss all the machinery to calculate it. Since it is a purely technical issue, you are expected to be able to do it: in order to make sure that you can find a derivative we provide a drill. This skills could be used for finding approximate values via linear approximation or during the search for extremal values. To provide an understanding of the sufficient condition of the extremum, we introduce the concept of convexity."
252,Derivatives and Linear Approximations: Multivariate Functions,6,https://www.coursera.org/learn/calculus-and-optimization-for-machine-learning,"Whilst we have discussed all linear related concepts for single variate functions, it is essential to try and generalise it for the multivariate case. Since the derivative concept is hard to stretch directly, we start with the idea of linear approximation and tangent plane; thus we introduce partial derivatives and the differentiability. We separately spend sometime discussing neural network inspired composite multivariate functions and all-mighty chain rule. Our generalisation attempt finalised with the idea of convexity in terms of the second partial derivatives."
253,"Integrals: Anti-derivative, Area under Curve",6,https://www.coursera.org/learn/calculus-and-optimization-for-machine-learning,"As we introduced the operation of differentiation, it is essential to think about the inverse procedure - the integration. We start the module with basic definition of the integration and, as usual, all techniques required to calculate wide range of the indefinite integrals, stressing out that the result is not guaranteed now. Then we proceed with the idea and formal definition of area under curve and its relation to the indefinite case - the fundamental theorem of calculus. We finish our week with the discussion of the areas of infinite figures (improper integrals) and numerical methods to assess the value of the definite integral."
254,"Optimization: Directional derivative, Extrema and Gradient Descent",6,https://www.coursera.org/learn/calculus-and-optimization-for-machine-learning,"As we built up impressive base by introducing various estimations of change and overall function's behaviour, it is essential to speak about general idea of the optimisation procedure. Since we already tackled it in a single variate case, we try to generalise our principles of necessary and sufficient conditions to the case of multivariate functions. Whilst it provides theoretical understanding, one should seek for faster iterative way to find an extremal point. In order to do it, we start our week with the concept of the directional derivative in order to provide and understanding of the desired direction of iterative search. Thus we produce the idea and motivation of the gradient descent, the last  and final concept in our course you are asked to master."
255,Systems of linear equations and linear classifier ,4,https://www.coursera.org/learn/first-steps-in-linear-algebra-for-machine-learning,"In the first week we provide an introduction to multi-dimensional geometry and matrix algebra. After that, we study methods for finding linear system solutions based on Gaussian eliminations and LU-decompositions. We illustrate the methods with Python code examples of matrix calculations."
256,Full rank decomposition and systems of linear equations,4,https://www.coursera.org/learn/first-steps-in-linear-algebra-for-machine-learning,"The second week is devoted to getting to know some fundamental notions of  linear algebra, namely: vector spaces, linear independence, and basis. Next, we will discuss what a rank of a matrix is, and how it could help us decompose a matrix.  In addition, we will talk about the properties of a set of solutions for a system of linear equations. At the end of this week we will apply this theory to a scanned document processing."
257,Euclidean spaces ,4,https://www.coursera.org/learn/first-steps-in-linear-algebra-for-machine-learning,"In the third week, we firstly introduce coordinates in an abstract vector space. This allows us to apply the usual matrix arithmetic to abstract vectors. Next, we discuss the concept of Euclidean space which allows us to measure distances and angles in vector spaces. Then we use these measures in the least squares method to find approximate solutions of linear systems and in the linear regression model based on it. Finally, we describe the core of the most common linear classifier called Support Vector Machine. "
258,Final Project,4,https://www.coursera.org/learn/first-steps-in-linear-algebra-for-machine-learning,In this week we will apply the acquired knowledge about linear regression and SVM models in this final project.  
259,Conditional probability and Independence,6,https://www.coursera.org/learn/probability-theory-statistics,"During this week we discuss conditional probability and independence of events. Sometimes we can use this definition to find probabilities. Sometimes we check that this definition fulfills to assure whether events are independent. We discuss important law of total probability, which allows us to find probability of some event when we know its conditional probabilities provided some hypotheses and probabilities of the hypotheses. We also discuss Bayes's rule which allows us to find probability of hypothesis provided that some event occurred. We demonstrate how Python can be used for calculating conditional probabilities and checking independence of events."
260,Random variables,6,https://www.coursera.org/learn/probability-theory-statistics,"Random variable denotes a value that depends on the result of some random experiment. Some natural examples of random variables come from gambling and lotteries. There are two main classes of random variables that we will consider in this course. This week we'll learn discrete random variables that take finite or countable number of values. Discrete random variables can be described by their distribution. We'll consider various discrete distributions, introduce notions of expected value and variance and learn to generate and visualize discrete random variables with Python."
261,"Systems of random variables; properties of expectation and variance, covariance and correlation.",6,https://www.coursera.org/learn/probability-theory-statistics,"Several random variables associated with the same random experiment constitute a system of random variables. To describe system of discrete random variables one can use joint distribution, which takes into account all possible combinations of values that random variables may take. We'll find some joint distributions, research their properties and introduce independence of random variables. Then we'll discuss properties of expected value and variance with respect to arithmetic operations and introduce measures of independence between random variables."
262,Continuous random variables,6,https://www.coursera.org/learn/probability-theory-statistics,"This week we'll study continuous random variables that constitute important data type in statistics and data analysis. For continuous random variables we'll define probability density function (PDF) and cumulative distribution function (CDF), see how they are linked and how sampling from random variable may be used to approximate its PDF. We'll introduce expected value, variance, covariance and correlation for continuous random variables and discuss their properties. Finally, we'll use Python to generate independent and correlated continuous random variables."
263,From random variables to statistical data. Data summarization and descriptive statistics.,6,https://www.coursera.org/learn/probability-theory-statistics,"This week we'll introduce types of statistical data and discuss models that are used to pass from statistical data to random variables. We'll introduce descriptive statistics of sample data, such as various measures of central tendency and statistical dispersion, and find correspondences between properties of random variables (population) and the sample descriptive statistics, which are essential for statistical predictions. We’ll talk about visualization of statistical data and learn to work with them in Python."
264,Correlations and visualizations,6,https://www.coursera.org/learn/probability-theory-statistics,"This week we’ll consider correlation in statistical data and find out how its' related to the level of dependance  within the data and what it means for scatter plots. We’ll consider several types of correlation suitable for different types of data and discuss difference between correlation and causation. Finally, we’ll learn to visualize dependence between numeric variables and calculate correlation with Python."
265,Introduction the course and grading environment,4,https://www.coursera.org/learn/ds,
266,Tools that support BigData solutions,4,https://www.coursera.org/learn/ds,
267,Scaling Math for Statistics on Apache Spark,4,https://www.coursera.org/learn/ds,
268,Data Visualization of Big Data,4,https://www.coursera.org/learn/ds,
269,Setting the stage,4,https://www.coursera.org/learn/advanced-machine-learning-signal-processing,
270,Supervised Machine Learning,4,https://www.coursera.org/learn/advanced-machine-learning-signal-processing,
271,Unsupervised Machine Learning,4,https://www.coursera.org/learn/advanced-machine-learning-signal-processing,
272,Digital Signal Processing in Machine Learning,4,https://www.coursera.org/learn/advanced-machine-learning-signal-processing,
273,Introduction to deep learning,4,https://www.coursera.org/learn/ai,
274,DeepLearning Frameworks,4,https://www.coursera.org/learn/ai,
275,DeepLearning Applications,4,https://www.coursera.org/learn/ai,
276,Scaling and Deployment,4,https://www.coursera.org/learn/ai,
277,Week 1 - Identify DataSet and UseCase,4,https://www.coursera.org/learn/advanced-data-science-capstone,"In this module, the basic process model used for this capstone project is introduced. Furthermore, the learner is required to identify a practical use case and data set"
278,Week 2 - ETL and Feature Creation,4,https://www.coursera.org/learn/advanced-data-science-capstone,"This module emphasizes on the importance of ETL, data cleansing and feature creation as a preliminary step in ever data science project "
279,Week 3 - Model Definition and Training,4,https://www.coursera.org/learn/advanced-data-science-capstone,This module emphasizes on model selection based on use case and data set. It is important to understand how those two factors impact choice of a useful model algorithm. 
280,"Model Evaluation, Tuning, Deployment and Documentation",4,https://www.coursera.org/learn/advanced-data-science-capstone,"One a model is trained it is important to assess its performance using an appropriate metric. In addition, once the model is finished, it has to be made consumable by business stakeholders in an appropriate way  "
281,Introduction to Data Analytics,4,https://www.coursera.org/learn/decision-making?specialization=pwc-analytics,"In this module you'll learn the basics of data analytics and how businesses use to solve problems. You'll learn the value data analytics brings to business decision-making processes. We’ll introduce you to a framework for data analysis and tools used in data analytics. Finally, we’re going to talk about careers and roles in data analytics and data science."
282,Technology and types of data ,4,https://www.coursera.org/learn/decision-making?specialization=pwc-analytics,This module is an introductory look at big data and big data analytics where you will learn the about different types of data. We’ll also introduce you to PwC's perspective on big data and explain the impact of big data on businesses.  Finally we will name some of the different types of tools and technologies used to gather data.
283,Data analysis techniques and tools,4,https://www.coursera.org/learn/decision-making?specialization=pwc-analytics,In this module we will describe some of the tools for data analytics and some of the key technologies for data analysis. We will talk about how visualization is important to the practice of data analytics. Finally we will identify a variety of tools and languages used and consider when those tools are best used.
284,Data-driven decision making project ,4,https://www.coursera.org/learn/decision-making?specialization=pwc-analytics,"The course project will give you an opportunity to practice what you have learned.  You will participate in a simulated business situation in which you will select the best course of action.  You will then prepare a final deliverable which will be evaluated by your peers.  Additionally, you will have the opportunity to provide feedback on your peer's submissions.  "
285,Overview of Excel,4,https://www.coursera.org/learn/excel-analysis?specialization=pwc-analytics,"In this module, you will learn the basics of Excel navigation and Excel basic functionality. You will learn how to navigate the basic Excel screen including using formulas, subtotals and text formatting. We will provide you an opportunity to perform a problem solving exercise using basic Excel skills. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen."
286,vLookups and Data Cleansing,4,https://www.coursera.org/learn/excel-analysis?specialization=pwc-analytics,"In this module you will learn about VLookup, value cleansing and text functions. We will also introduce you to PwC's perspective on the value in cleansing data and using the appropriate functions.  Finally, we will provide you an opportunity to perform a problem solving exercise using VLookup, value cleansing and text function. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen."
287,Logical Functions & Pivot Tables,4,https://www.coursera.org/learn/excel-analysis?specialization=pwc-analytics,"In this module, you will learn about logical functions and pivot tables. We will show you how to create and use pivot tables to solve business problems. We will give you an opportunity to practice creating and using a pivot table to solve a business problem. Finally, we will share some insight on PwC’s perspectives on the impact of Excel on your career and work. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen."
288,More Advanced Formulas,4,https://www.coursera.org/learn/excel-analysis?specialization=pwc-analytics,"In this module you will learn more advanced Excel formulas. We will show you how to create statistical formulas, perform an index match, and lastly, build financial formulas. We will provide you with an opportunity to problem solve using statistical formulas. Finally, we will give you an opportunity to practice what you have learned through a final project. Note: We recommend viewing videos in full-screen mode by clicking the double arrow in the lower right hand corner of your screen."
289,Preparing a Professional Excel ,4,https://www.coursera.org/learn/advanced-excel?specialization=pwc-analytics,"During this first week, you are going to learn about the development of data models and databases.  We will cover the components of data sets and the relational database models, database keys, relationships, and joins.  We will also look at a tool called PowerPivot that is used to import and prepare data to build relational models, as well as visualize data.  By the end of the week, you will have a working knowledge of how to develop a data model. Be sure to complete lessons in the order in which they are sequenced in the course."
290,Advanced Scenario Analysis ,4,https://www.coursera.org/learn/advanced-excel?specialization=pwc-analytics,"This week, we are going to explore three different analytical methods used to help model different scenarios and deal with variable uncertainty. These methods are scenario analysis, sensitivity analysis and simulation.  We’ll look at what each method is and then go deeper into why and how you use each.  Following some guided demonstration, you’ll be given a chance to practice in an Excel workbook and demonstrate what you’ve learned. "
291,Data Visualization,4,https://www.coursera.org/learn/advanced-excel?specialization=pwc-analytics,"This week we are going to focus on data visualization. We will start off by discussing data visualization basics, outlining the theory and concepts behind data visualization. We will also discuss how to enable effective story telling through the correct selection, creation, and presentation of tables and charts. You’ll get a chance to learn how to create detailed graphs and charts to effectively tell a story about your data. "
292,Dashboarding ,4,https://www.coursera.org/learn/advanced-excel?specialization=pwc-analytics,"In the final week of this course, you are going to learn how to create a dynamic dashboard. We are going to discuss how to establish a good understanding of your audience and how to collect key requirements in order to determine what type of dashboard to build.  We will talk about some guiding design principles and things to consider when building a dashboard. You’ll have a chance to practice everything you learn this week by creating your own functional dashboard in Excel.  "
293,Preparing a Presentation,4,https://www.coursera.org/learn/powerpoint-presentations?specialization=pwc-analytics,"This course is about presenting the story of the data, using PowerPoint. You'll learn how to structure a presentation and how to include insights and supporting data. You'll also learn some design principles for creating effective PowerPoint slides with visuals displaying data. Though application based exercises, you'll gain foundational communication skills - including public speaking, professional presence and compelling storytelling. Finally, you'll be given a client profile, a business problem, and a set of basic Excel charts, that you will use to create  a presentation.  You’ll receive peer feedback that you can use to enhance future presentations. This course was created by PricewaterhouseCoopers LLP with an address at 300 Madison Avenue, New York, New York, 10017"
294,Communication styles,4,https://www.coursera.org/learn/powerpoint-presentations?specialization=pwc-analytics,"This week, we will be covering the different types of communications styles.  You’ll start off by gaining an understanding of your personal professional presence and learn how to maximize it.  You’ll learn about verbal and nonverbal communications, and strategies to enhance your questioning and listening skills. We will also discuss how differences in culture can impact how you communicate."
295,Creating effective slides using PowerPoint,4,https://www.coursera.org/learn/powerpoint-presentations?specialization=pwc-analytics,"This week, we're discussing how to create effective slides using PowerPoint. You’ll learn about the tools available within PowerPoint, how to structure your storyline, create storyboards, identify primary elements of slide design, display data and finalize your slide presentation. There is a peer review activity where you will apply the skills learned and create a storyboard. Finally, you will also get a chance to identify errors in a presentation to test your knowledge of standard industry practices."
296,Delivering a presentation,4,https://www.coursera.org/learn/powerpoint-presentations?specialization=pwc-analytics,"This week, you’re going to build and deliver a presentation to your peers, and receive feedback from them.  You will create a presentation of about 10 slides, employing the guidelines and industry best practices that have been discussed in this course. You can use the presentation storyboard that you created last week, which your peers have reviewed and given you feedback on.  Review what you’ve developed so far, and make changes or additions that you think will enhance the presentation. Once you’ve finalized your presentation, you will present it in a video using your smartphone or computer.Once you’re satisfied with the PowerPoint presentation and video, you will be submitting both for peer review.  You can use this feedback for current and future presentations that you will make during your career."
297,The Role of Statistics in Public Health Research,6,https://www.coursera.org/learn/summary-statistics,"Within this learning module, you will learn how statistics are used within public heath research. Topics covered include study design types, data types, and data summarization. You will complete a practice quiz before completing a graded quiz. "
298,Continuous Data Measures,6,https://www.coursera.org/learn/summary-statistics,"Module two involves several lectures, a practice quiz and a graded quiz. Topics include summary statistics, visual displays, role of sample size, and continuous data. "
299,The Normal Distribution,6,https://www.coursera.org/learn/summary-statistics,"The focus of this module is on normal distribution. Topics covered include defining the standard normal distribution, and application of principles of normal distribution to sample data. There is a practice quiz where you can test your knowledge before completing the graded quiz. "
300,Binary Data,6,https://www.coursera.org/learn/summary-statistics,"Module four covers binary data and its significance.  In addition to lectures, you will complete a practice quiz and a graded quiz. "
301,Dealing with the Element of Time,6,https://www.coursera.org/learn/summary-statistics,"In module five, you will explore how time is defined and studied in relation to data and learn about the Kaplan-Meir curve. In addition to a practice quiz, you will complete a graded quiz and project.  "
302,Course Project,6,https://www.coursera.org/learn/summary-statistics,"During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about asthma medication and the other about self-administration of injectable contraception. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies."
303,Sampling Distributions and Standard Errors,6,https://www.coursera.org/learn/hypothesis-testing-public-health,"Within module one, you will learn about sample statistics, sampling distribution, and the central limit theorem. You will have the opportunity to test your knowledge with a practice quiz and, then, apply what you learned to the graded quiz. "
304,Confidence Intervals for Single Population Parameters,6,https://www.coursera.org/learn/hypothesis-testing-public-health,"Module two builds upon previous materials to discuss confidence intervals, the need for ample sizes of data, and ways to get around the need for ample sizes of data. The practice quiz helps you prepare for the graded quiz.  "
305,Confidence Intervals for Population Comparison Measures,6,https://www.coursera.org/learn/hypothesis-testing-public-health,"Within module three, confidence intervals are discussed at length and ratios are discussed again. Aside from the lectures, you will also be completing a practice quiz and graded quiz.  "
306,Two-Group Hypothesis Testing: The General Concept and Comparing Means,6,https://www.coursera.org/learn/hypothesis-testing-public-health,"Within module four, you will look at statistical hypothesis tests, confidence intervals, and p-value. There is a practice quiz to prepare you for the graded quiz. "
307,Hypothesis Testing (Comparing Proportions and Incidence Rates Between Two Populations) & Extended Hypothesis Testing,6,https://www.coursera.org/learn/hypothesis-testing-public-health,
308,Project,6,https://www.coursera.org/learn/hypothesis-testing-public-health,"During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about asthma medication and the other about self-administration of injectable contraception. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies. If you've already taken the Summarization and Measurement course, then this scenario will be familiar. "
309,Simple Regression Methods,5,https://www.coursera.org/learn/simple-regression-analysis-public-health,"Module one covers simple regression, the four different types of regression, commonalities between them, and simple linear aggression. Before completing the graded quiz, you can test your knowledge with the practice quiz.  "
310,Simple Logistic Regression,5,https://www.coursera.org/learn/simple-regression-analysis-public-health,"Within module two, we will look at logistic regression, create confidence intervals, and estimate p-values. You will have the opportunity to test your knowledge in both a practice quiz and a graded quiz. "
311,Simple Cox Proportional Hazards Regression,5,https://www.coursera.org/learn/simple-regression-analysis-public-health,"Module three focuses on Cox regression with different predictors. You will have the opportunity to test your knowledge first with the practice quiz and, then, with the graded quiz.  "
312,"Confounding, Adjustment, and Effect Modification",5,https://www.coursera.org/learn/simple-regression-analysis-public-health,"Within module four, you will look at confounding and adjustment, and unadjusted and adjusted association estimates. Additionally, you will learn about effect modification. Similar to previous modules, you will first take a practice quiz before completing the graded quiz.  "
313,Course Project,5,https://www.coursera.org/learn/simple-regression-analysis-public-health,"During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about self-administration of injectable contraception and one about medical appointment scheduling in Brazil. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies. If you've already taken other courses in this specialization, then this scenario will be familiar. "
314,"An Overview of Multiple Regression for Estimation, Adjustment, and Basic Prediction, and Multiple Linear Regression",4,https://www.coursera.org/learn/multiple-regression-analysis-public-health,"Within this module, an overview of multiple regression will be provided. Additionally, examples and applications will be examined. A practice quiz is provided to test your knowledge before completing the graded quiz. "
315,Multiple Logistic Regression,4,https://www.coursera.org/learn/multiple-regression-analysis-public-health,"Module two covers examples of multiple logistic regression, basics of model estimates, and a discussion of effect modification. In addition to lectures, you will also be completing a practice quiz and graded quiz. "
316,Multiple Cox Regression,4,https://www.coursera.org/learn/multiple-regression-analysis-public-health,"The last module for this class focuses on multiple Cox regression, the “Linearity” assumption, examples, and applications. You will complete a practice quiz, graded quiz, and project. "
317,Course Project,4,https://www.coursera.org/learn/multiple-regression-analysis-public-health,"During this module, you get the chance to demonstrate what you've learned by putting yourself in the shoes of biostatistical consultant on two different studies, one about self-administration of injectable contraception and one about medical appointment scheduling in Brazil. The two research teams have asked you to help them interpret previously published results in order to inform the planning of their own studies. If you've already taken other courses in this specialization, then this scenario will be familiar. "
318,Introduction and Overview,7,https://www.coursera.org/learn/probabilistic-graphical-models,"This module provides an overall introduction to probabilistic graphical models, and defines a few of the key concepts that will be used later in the course."
319,Bayesian Network (Directed Models),7,https://www.coursera.org/learn/probabilistic-graphical-models,"In this module, we define the Bayesian network representation and its semantics. We also analyze the relationship between the graph structure and the independence properties of a distribution represented over that graph. Finally, we give some practical tips on how to model a real-world situation as a Bayesian network."
320,Template Models for Bayesian Networks,7,https://www.coursera.org/learn/probabilistic-graphical-models,"In many cases, we need to model distributions that have a recurring structure. In this module, we describe representations for two such situations. One is temporal scenarios, where we want to model a probabilistic structure that holds constant over time; here, we use Hidden Markov Models, or, more generally, Dynamic Bayesian Networks. The other is aimed at scenarios that involve multiple similar entities, each of whose properties is governed by a similar model; here, we use Plate Models."
321,Structured CPDs for Bayesian Networks,7,https://www.coursera.org/learn/probabilistic-graphical-models,A table-based representation of a CPD in a Bayesian network has a size that grows exponentially in the number of parents. There are a variety of other form of CPD that exploit some type of structure in the dependency model to allow for a much more compact representation. Here we describe a number of the ones most commonly used in practice.
322,Markov Networks (Undirected Models),7,https://www.coursera.org/learn/probabilistic-graphical-models,"In this module, we describe Markov networks (also called Markov random fields): probabilistic graphical models based on an undirected graph representation. We discuss the representation of these models and their semantics. We also analyze the independence properties of distributions encoded by these graphs, and their relationship to the graph structure. We compare these independencies to those encoded by a Bayesian network, giving us some insight on which type of model is more suitable for which scenarios."
323,Decision Making,7,https://www.coursera.org/learn/probabilistic-graphical-models,"In this module, we discuss the task of decision making under uncertainty. We describe the framework of decision theory, including some aspects of utility functions. We then talk about how decision making scenarios can be encoded as a graphical model called an Influence Diagram, and how such models provide insight both into decision making and the value of information gathering."
324,Knowledge Engineering & Summary,7,https://www.coursera.org/learn/probabilistic-graphical-models,This module provides an overview of graphical model representations and some of the real-world considerations when modeling a scenario as a graphical model. It also includes the course final exam.
325,Inference Overview ,7,https://www.coursera.org/learn/probabilistic-graphical-models-2-inference,"This module provides a high-level overview of the main types of inference tasks typically encountered in graphical models: conditional probability queries, and finding the most likely assignment (MAP inference)."
326,Variable Elimination,7,https://www.coursera.org/learn/probabilistic-graphical-models-2-inference,"This module presents the simplest algorithm for exact inference in graphical models: variable elimination. We describe the algorithm, and analyze its complexity in terms of properties of the graph structure."
327,Belief Propagation Algorithms ,7,https://www.coursera.org/learn/probabilistic-graphical-models-2-inference,This module describes an alternative view of exact inference in graphical models: that of message passing between clusters each of which encodes a factor over a subset of variables. This framework provides a basis for a variety of exact and approximate inference algorithms. We focus here on the basic framework and on its instantiation in the exact case of clique tree propagation. An optional lesson describes the loopy belief propagation (LBP) algorithm and its properties.
328,MAP Algorithms,7,https://www.coursera.org/learn/probabilistic-graphical-models-2-inference,"This module describes algorithms for finding the most likely assignment for a distribution encoded as a PGM (a task known as MAP inference). We describe message passing algorithms, which are very similar to the algorithms for computing conditional probabilities, except that we need to also consider how to decode the results to construct a single assignment. In an optional module, we describe a few other algorithms that are able to use very different techniques by exploiting the combinatorial optimization nature of the MAP task."
329,Sampling Methods,7,https://www.coursera.org/learn/probabilistic-graphical-models-2-inference,"In this module, we discuss a class of algorithms that uses random sampling to provide approximate answers to conditional probability queries. Most commonly used among these is the class of Markov Chain Monte Carlo (MCMC) algorithms, which includes the simple Gibbs sampling algorithm, as well as a family of methods known as Metropolis-Hastings."
330,Inference in Temporal Models,7,https://www.coursera.org/learn/probabilistic-graphical-models-2-inference,"In this brief lesson, we discuss some of the complexities of applying some of the exact or approximate inference algorithms that we learned earlier in this course to dynamic Bayesian networks."
331,Inference Summary,7,https://www.coursera.org/learn/probabilistic-graphical-models-2-inference,This module summarizes some of the topics that we covered in this course and discusses tradeoffs between different algorithms. It also includes the course final exam.
332,Learning: Overview,8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,This module presents some of the learning tasks for probabilistic graphical models that we will tackle in this course.
333,Review of Machine Learning Concepts from Prof. Andrew Ng's Machine Learning Class (Optional),8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,"This module contains some basic concepts from the general framework of machine learning, taken from Professor Andrew Ng's Stanford class offered on Coursera. Many of these concepts are highly relevant to the problems we'll tackle in this course."
334,Parameter Estimation in Bayesian Networks,8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,"This module discusses the simples and most basic of the learning problems in probabilistic graphical models: that of parameter estimation in a Bayesian network. We discuss maximum likelihood estimation, and the issues with it. We then discuss Bayesian estimation and how it can ameliorate these problems."
335,Learning Undirected Models,8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,"In this module, we discuss the parameter estimation problem for Markov networks - undirected graphical models. This task is considerably more complex, both conceptually and computationally, than parameter estimation for Bayesian networks, due to the issues presented by the global partition function."
336,Learning BN Structure,8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,"This module discusses the problem of learning the structure of Bayesian networks. We first discuss how this problem can be formulated as an optimization problem over a space of graph structures, and what are good ways to score different structures so as to trade off fit to data and model complexity. We then talk about how the optimization problem can be solved: exactly in a few cases, approximately in most others."
337,Learning BNs with Incomplete Data,8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,"In this module, we discuss the problem of learning models in cases where some of the variables in some of the data cases are not fully observed. We discuss why this situation is considerably more complex than the fully observable case. We then present the Expectation Maximization (EM) algorithm, which is used in a wide variety of problems."
338,Learning Summary and Final,8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,This module summarizes some of the issues that arise when learning probabilistic graphical models from data. It also contains the course final.
339,PGM Wrapup,8,https://www.coursera.org/learn/probabilistic-graphical-models-3-learning,"This module contains an overview of PGM methods as a whole, discussing some of the real-world tradeoffs when using this framework in practice. It refers to topics from all three of the PGM courses."
340,Module 1: Introduction to Models ,4,https://www.coursera.org/learn/wharton-quantitative-modeling?specialization=wharton-business-financial-modeling,"In this module, you will learn how to define a model, and how models are commonly used. You’ll examine the central steps in the modeling process, the four key mathematical functions used in models, and the essential vocabulary used to describe models. By the end of this module, you’ll be able to identify the four most common types of models, and how and when they should be used. You’ll also be able to define and correctly use the key terms of modeling, giving you not only a foundation for further study, but also the ability to ask questions and participate in conversations about quantitative models."
341,Module 2: Linear Models and Optimization,4,https://www.coursera.org/learn/wharton-quantitative-modeling?specialization=wharton-business-financial-modeling,"This module introduces linear models, the building block for almost all modeling. Through close examination of the common uses together with examples of linear models, you’ll learn how to apply linear models, including cost functions and production functions to your business. The module also includes a presentation of growth and decay processes in discrete time, growth and decay in continuous time, together with their associated present and future value calculations. Classical optimization techniques are discussed. By the end of this module, you’ll be able to identify and understand the key structure of linear models, and suggest when and how to use them to improve outcomes for your business. You’ll also be able to perform present value calculations that are foundational to valuation metrics. In addition, you will understand how you can leverage models for your business, through the use of optimization to really fine tune and optimize your business functions."
342,Module 3: Probabilistic Models,4,https://www.coursera.org/learn/wharton-quantitative-modeling?specialization=wharton-business-financial-modeling,"This module explains probabilistic models, which are ways of capturing risk in process. You’ll need to use probabilistic models when you don’t know all of your inputs. You’ll examine how probabilistic models incorporate uncertainty, and how that uncertainty continues through to the outputs of the model. You’ll also discover how propagating uncertainty allows you to determine a range of values for forecasting. You’ll learn the most-widely used models for risk, including regression models, tree-based models, Monte Carlo simulations, and Markov chains, as well as the building blocks of these probabilistic models, such as random variables, probability distributions, Bernoulli random variables, binomial random variables, the empirical rule, and perhaps the most important of all of the statistical distributions, the normal distribution, characterized by mean and standard deviation. By the end of this module, you’ll be able to define a probabilistic model, identify and understand the most commonly used probabilistic models, know the components of those models, and determine the most useful probabilistic models for capturing and exploring risk in your own business."
343,Module 4: Regression Models,4,https://www.coursera.org/learn/wharton-quantitative-modeling?specialization=wharton-business-financial-modeling,"This module explores regression models, which allow you to start with data and discover an underlying process. Regression models are the key tools in predictive analytics, and are also used when you have to incorporate uncertainty explicitly in the underlying data.  You’ll learn more about what regression models are, what they can and cannot do, and the questions regression models can answer. You’ll examine correlation and linear association, methodology to fit the best line to the data, interpretation of regression coefficients, multiple regression, and logistic regression. You’ll also see how logistic regression will allow you to estimate probabilities of success. By the end of this module, you’ll be able to identify regression models and their key components, understand when they are used, and be able to interpret them so that you can discuss your model and convince others that your model makes sense, with the ultimate goal of implementation."
344,Spreadsheets: A Tool for Thinking with Numbers,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models?specialization=wharton-business-financial-modeling,"This module was designed to introduce you to the history of spreadsheets, their basic capabilities, and how they can be used to create models. You'll learn the different types of data used in spreadsheets, spreadsheet notations for mathematical operations, common built-in formulas and functions, conditional expressions, relative and absolute references, and how to identify and correct circular references. By the end of this module, you'll understand the context of spreadsheets, be able to navigate a spreadsheet, use built-in formulas and functions in spreadsheets, create your own simple formulas, and identify and correct common errors so you can put spreadsheets to work for you."
345,From Spreadsheet to Model,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models?specialization=wharton-business-financial-modeling,"In this module, you'll move from spreadsheet to model, so you can begin to create your own models that reflect real-world events. You'll learn how to organize and lay out model elements, as well as the types of objective functions and their use. You'll also learn what-if analysis and scenarios, sensitivity analysis, and other classic models. By the end of this module, you'll be able to design a spreadsheet reflecting assumptions, decision variables, and outcomes, create a basic cashflow model, evaluate a small business opportunity, conduct what-if analysis, identify key variables using sensitivity analysis, and linear programming models and deterministic models."
346,Addressing Uncertainty and Probability in Models,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models?specialization=wharton-business-financial-modeling,"This module was designed to introduce you to how you can use spreadsheets to address uncertainty and probability. You'll learn about random variables, probability distributions, power, exponential, and log functions in model formulas, models for calculating probability trees and decision trees, how to use regression tools to make predictions, as well as multiple regression. By the end of this module, you'll be able to measure correlations between variables using spreadsheet statistical functions, understand the results of functions that calculate correlations, use regression tools to make predictions, and improve forecasts with multiple regression."
347, Simulation and Optimization,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models?specialization=wharton-business-financial-modeling,"In this module, you'll learn to use spreadsheets to implement Monte Carlo simulations as well as linear programs for optimization. You'll examine the purpose of Monte Carlo simulations, how to implement Monte Carlo simulations in spreadsheets, the types of problems you can address with linear programs and how to implement those linear programs in spreadsheets. By the end of this module, you'll be able to model uncertainty and risk in spreadsheets, and use Excel's solver to optimize resources to reach a desired outcome.  You'll also be able to identify the similarities and differences between Excel and Sheets, and be prepared for the next course in the Business and Financial Modeling Specialization."
348,Week 1: Modeling Decisions in Low Uncertainty Settings,4,https://www.coursera.org/learn/wharton-risk-models?specialization=wharton-business-financial-modeling,"This module is designed to teach you how to analyze settings with low levels of uncertainty, and how to identify the best decisions in these settings. You'll explore the optimization toolkit, learn how to build an algebraic model using an advertising example, convert the algebraic model to a spreadsheet model, work with Solver to discover the best possible decision, and examine an example that introduces a simple representation of risk to the model. By the end of this module, you'll be able to build an optimization model, use Solver to uncover the optimal decision based on your data, and begin to adjust your model to account for simple elements of risk. These skills will give you the power to deal with large models as long as the actual uncertainty in the input values is not too high."
349,Week 2: Risk and Reward: Modeling High Uncertainty Settings,4,https://www.coursera.org/learn/wharton-risk-models?specialization=wharton-business-financial-modeling,"What if uncertainty is the key feature of the setting you are trying to model? In this module, you'll learn how to create models for situations with a large number of variables. You'll examine high uncertainty settings, probability distributions, and risk, common scenarios for multiple random variables, how to incorporate risk reduction, how to calculate and interpret correlation values, and how to use scenarios for optimization, including sensitivity analysis and the efficient frontier. By the end of this module, you'll be able to identify and use common models of future uncertainty to build scenarios that help you optimize your business decisions when you have multiple variables and a higher degree of risk. "
350,Week 3: Choosing Distributions that Fit Your Data,4,https://www.coursera.org/learn/wharton-risk-models?specialization=wharton-business-financial-modeling,"When making business decisions, we often look to the past to make predictions for the future. In this module, you'll examine commonly used distributions of random variables to model the future and make predictions. You'll learn how to create meaningful data visualizations in Excel, how to choose the the right distribution for your data, explore the differences between discrete distributions and continuous distributions, and test your choice of model and your hypothesis for goodness of fit. By the end of this module, you'll be able to represent your data using graphs, choose the best distribution model for your data, and test your model and your hypothesis to see if they are the best fit for your data."
351,Week 4: Balancing Risk and Reward Using Simulation,4,https://www.coursera.org/learn/wharton-risk-models?specialization=wharton-business-financial-modeling,"This module is designed to help you use simulations to enabling compare different alternatives when continuous distributions are used to describe uncertainty. Through an in-depth examination of the simulation toolkit, you'll learn how to make decisions in high uncertainty settings where random inputs are described by continuous probability distributions. You'll also learn how to run a simulation model, analyze simulation output, and compare alternative decisions to decide on the most optimal solution.  By the end of this module, you'll be able to make decisions and manage risk using simulation, and more broadly, to make successful business decisions in an increasing complex and rapidly evolving business world."
352,Evaluation Criteria: Net Present Value,4,https://www.coursera.org/learn/wharton-decision-making-scenarios?specialization=wharton-business-financial-modeling,"This module was designed to introduce you to the many potential criteria for selecting investment projects, and to explore the most effective of these criteria: Net Present Value (NPV). Through the use of concrete examples, you'll learn the key components of Net Present Value, including the time value of money and the cost of capital, the main utility of NPV, and why it is ultimately more accurate and useful for evaluating projects than other commonly used criteria. By the end of this module, you'll be able to explain why net present value analysis is the appropriate criteria for choosing whether to accept or reject a project, and why other criteria, such as IRR, payback, ROI, etc. may not lead to decisions which maximize value."
353,Evaluating Projects,4,https://www.coursera.org/learn/wharton-decision-making-scenarios?specialization=wharton-business-financial-modeling,"In this module, you'll learn how to evaluate a project with emphasis on analyzing the incremental after-tax cash flows associated with the project. You'll work through a concrete example using alternative scenarios to test the effectiveness of this method. You'll also learn why only future cash flows are relevant, why to ignore financial costs, include all incidental effects, remember working capital requirements, consider the effect of taxes, forget sunk costs, remember opportunity costs, use expected cash flows, and perform sensitivity analysis.  By the end of this module, you'll be able to evaluate projects more thoroughly and effectively, with emphasis on how to model the change in the company’s after-tax cash flows, so that you can make more profitable decisions."
354,Expressing Business Strategies in Financial Terms,4,https://www.coursera.org/learn/wharton-decision-making-scenarios?specialization=wharton-business-financial-modeling,"This module was designed to give you the opportunity to learn how business activities, transactions and events are translated into financial statements, including balance sheets, income statements, and cash flow statements. You'll also learn how these three statements are linked to each other, and how balance sheets and income statements can help forecast the future cash flow statements. By the end of this module, you'll be able to explain how accounting systems translate business activities into financial terms, and how to use this to better forecast future cash flows, so that you can express your business strategies in these financial terms, and show ""the bottom line"" for your proposed plan of action."
355,New Product Value,4,https://www.coursera.org/learn/wharton-decision-making-scenarios?specialization=wharton-business-financial-modeling,"In this module, you'll apply what you’ve been learning to an analysis of a new product venture. You’ll learn how to map out a plan of the business activities, transactions and events that need to happen to implement the new venture, including their timing. You'll also learn how to set up a spreadsheet to help with forecasts, and to re-calculate things automatically as we re-think our plans. You'll see how to forecast out the implied financial statements, and calculate the Net Present Value (NPV). By the end of this module, you'll be able to use spreadsheets to explore different risks a venture may face, and analyze the implications of these scenarios for NPV, so that you can make the most profitable, data-driven decision possible."
356,Data and Analysis in the Real World,4,https://www.coursera.org/learn/data-analytics-business?specialization=data-analytics-business,"Welcome to week 1! In this module we’ll learn how to think about analytical problems and examine the process by which data enables analysis & decision making. We’ll introduce a framework called the Information-Action Value chain which describes the path from events in the world to business action, and we’ll look at some of the source systems that are used to capture data. At the end of this course you will be able to: Explain the information lifecycle from events in the real world to business actions, and how to think about analytical problems in that context , Recognize the types of events and characteristics that are often used in business analytics, and explain how the data is captured by source systems and stored using both traditional and emergent technologies, Gain a high-level familiarity with relational databases and learn how to use a simple but powerful language called SQL to extract analytical data sets of interest, Appreciate the spectrum of roles involved in the data lifecycle, and gain exposure to the various ways that organizations structure analytical functions,  Summarize some of the key ideas around data quality, data governance, and data privacy"
357,Analytical Tools,4,https://www.coursera.org/learn/data-analytics-business?specialization=data-analytics-business,"In this module we’ll learn about the technologies that enable analytical work.  We’ll examine data storage and databases, including the relational database.  We’ll talk about Big Data and Cloud technologies and ideas like federation, virtualization, and in-memory computing.  We’ll also walk through a landscape of some of the more common tool classes and learn how these tools support common analytical tasks."
358,Data Extraction Using SQL,4,https://www.coursera.org/learn/data-analytics-business?specialization=data-analytics-business,"In this module we’ll learn how to extract data from a relational database using Structured Query Language, or SQL.  We’ll cover all the basic SQL commands and learn how to combine and stack data from different tables.  We’ll also learn how to expand the power of our queries using operators and handle additional complexity using subqueries."
359,Real World Analytical Organizations,4,https://www.coursera.org/learn/data-analytics-business?specialization=data-analytics-business,"In this module we focus on the people and organizations that work with data and actually execute analytics.  We’ll discuss who does what and see how organizational structures can influence efficiency and effectiveness.  We’ll also look at the supporting rules & processes that help an analytical organization run smoothly, like Data Governance, Data Privacy, and Data Quality."
360, Exploratory Data Analysis and Visualizations,4,https://www.coursera.org/learn/predictive-modeling-analytics?specialization=data-analytics-business,At the end of this module students will be able to: 1. Carry out exploratory data analysis to gain insights and prepare data for predictive modeling 2. Summarize and visualize datasets using appropriate tools 3. Identify modeling techniques for prediction of continuous and discrete outcomes. 4. Explore datasets using Excel 5. Explain and perform several common data preprocessing steps         6. Choose appropriate graphs to explore and display datasets  
361,Predicting a Continuous Variable,4,https://www.coursera.org/learn/predictive-modeling-analytics?specialization=data-analytics-business,"This module introduces regression techniques to predict the value of continuous variables. Some fundamental concepts of predictive modeling are covered, including cross-validation, model selection, and overfitting. You will also learn how to build predictive models using the software tool XLMiner."
362,Predicting a Binary Outcome,4,https://www.coursera.org/learn/predictive-modeling-analytics?specialization=data-analytics-business,"This module introduces logistic regression models to predict the value of binary variables. Unlike continuous variables, a binary variable can only take two different values and predicting its value is commonly called classification. Several important concepts regarding classification are discussed, including cross validation and confusion matrix, cost sensitive classification, and ROC curves. You will also learn how to build classification models using the software tool XLMiner."
363,Trees and Other Predictive Models,4,https://www.coursera.org/learn/predictive-modeling-analytics?specialization=data-analytics-business,"This module introduces more advanced predictive models, including trees and neural networks. Both trees and neural networks can be used to predict continuous or binary variables. You will also learn how to build trees and neural networks using the software tool XLMiner."
364,Data Exploration and Reduction — Cluster Analysis,4,https://www.coursera.org/learn/business-analytics-decision-making?specialization=data-analytics-business,
365,Dealing with Uncertainty and Analyzing Risk,4,https://www.coursera.org/learn/business-analytics-decision-making?specialization=data-analytics-business,
366,Identifying the Best Options — Optimization,4,https://www.coursera.org/learn/business-analytics-decision-making?specialization=data-analytics-business,At the end of this module students should be able to: 1. Develop a spreadsheet model for an optimization problem 2. Use Excel to solve optimization models 3. Interpret solutions and conduct what-if analysis
367,Decision Analytics,4,https://www.coursera.org/learn/business-analytics-decision-making?specialization=data-analytics-business,"At the end of this module students should be able to: 1. Given a business situation, apply an appropriate technique to identify the best solution alternatives 2. Formulate and solve models for business problems that requires yes/no decisions and logical constraints 3. Create models that mix techniques and tools such as simulation and optimizationAnalyze and interpret results to make informed decisions"
368,Introduction to the Course,4,https://www.coursera.org/learn/communicating-business-analytics-results?specialization=data-analytics-business,"In this module we’ll briefly review the Information-Action Value Chain we introduced in Course 1.  Then we’ll see how analytical techniques are applied in business problems, first by looking at some “classic” business problems that have been around for a long time, then by looking at some “emergent” business problems that have resulted from more recent advances in technology."
369,Best  Practices in Data Visualization,4,https://www.coursera.org/learn/communicating-business-analytics-results?specialization=data-analytics-business,"In this module we’ll learn about a variety of visualizations used to illustrate and communicate data. We will start with the different vehicles used  to present quantitative information. We will then look at a set of examples of data visualizations and discuss what makes them effective or ineffective.  Finally, we discuss Excel charts and why most of them should be avoided.  After completing this module, you will be able to better understand the characteristics of good data visualization and avoid common mistakes when creating your own graphs."
370,"Interpreting, Telling, and Selling",4,https://www.coursera.org/learn/communicating-business-analytics-results?specialization=data-analytics-business,"In this module we’ll cover a number of topics around interpreting data, gathering additional data, and pitching our recommendations based on our analysis.  First, we’ll discuss ways in which we misinterpret or misrepresent data and how to avoid them, such as mistaking correlation with causation, allowing cognitive biases to influence how we see data, and visualizing data in misleading ways.  We’ll also learn how experimentation can help us obtain more data, including compromises we may need to make in measurement.  Finally, we’ll discuss how we communicate our results and recommendations, with a focus on knowing our audience, telling compelling stories, and creating clear and effective communication materials."
371,Acting on Data,4,https://www.coursera.org/learn/communicating-business-analytics-results?specialization=data-analytics-business,"In our final module we’ll walk through two case studies and illustrate the ideas we’ve covered in the course and in the specialization as a whole.  The first case shows how experimentation can be used to create data, sometimes with surprising results.  The second case presents a comprehensive analysis that illustrates the entire analytic lifecycle, and shows how different methods and both quantitative and qualitative analysis can be brought together to solve one strategically important analytical problem."
372,Data Science Fundamentals,4,https://www.coursera.org/learn/data-scientists-tools?specialization=data-science-foundations-r,"In this module, we'll introduce and define data science and data itself. We'll also go over some of the resources that data scientists use to get help when they're stuck."
373,R and RStudio,4,https://www.coursera.org/learn/data-scientists-tools?specialization=data-science-foundations-r,"In this module, we'll help you get up and running with both R and RStudio. Along the way, you'll learn some basics about both and why data scientists use them. "
374,Version Control and GitHub,4,https://www.coursera.org/learn/data-scientists-tools?specialization=data-science-foundations-r,"During this module, you'll learn about version control and why it's so important to data scientists. You'll also learn how to use Git and GitHub to manage version control in data science projects."
375,"R Markdown, Scientific Thinking, and Big Data ",4,https://www.coursera.org/learn/data-scientists-tools?specialization=data-science-foundations-r,"During this final module, you'll learn to use R Markdown and get an introduction to three concepts that are incredibly important to every successful data scientist: asking good questions, experimental design, and big data. "
376,"Week 1: Background, Getting Started, and Nuts & Bolts",4,https://www.coursera.org/learn/r-programming?specialization=data-science-foundations-r,"This week covers the basics to get you started up with R. The Background Materials lesson contains information about course mechanics and some videos on installing R. The Week 1 videos cover the history of R and S, go over the basic data types in R, and describe the functions for reading and writing data. I recommend that you watch the videos in the listed order, but watching the videos out of order isn't going to ruin the story. "
377,Week 2: Programming with R,4,https://www.coursera.org/learn/r-programming?specialization=data-science-foundations-r,"Welcome to Week 2 of R Programming. This week, we take the gloves off, and the lectures cover key topics like control structures and functions. We also introduce the first programming assignment for the course, which is due at the end of the week."
378,Week 3: Loop Functions and Debugging,4,https://www.coursera.org/learn/r-programming?specialization=data-science-foundations-r,"We have now entered the third week of R Programming, which also marks the halfway point. The lectures this week cover loop functions and the debugging tools in R. These aspects of R make R useful for both interactive work and writing longer code, and so they are commonly used in practice."
379,Week 4: Simulation & Profiling,4,https://www.coursera.org/learn/r-programming?specialization=data-science-foundations-r,"This week covers how to simulate data in R, which serves as the basis for doing simulation studies. We also cover the profiler in R which lets you collect detailed information on how your R functions are running and to identify bottlenecks that can be addressed. The profiler is a key tool in helping you optimize your programs. Finally, we cover the str function, which I personally believe is the most useful function in R."
380,Week 1,4,https://www.coursera.org/learn/data-cleaning?specialization=data-science-foundations-r,"In this first week of the course, we look at finding data and reading different file types."
381,Week 2,4,https://www.coursera.org/learn/data-cleaning?specialization=data-science-foundations-r,Welcome to Week 2 of Getting and Cleaning Data! The primary goal is to introduce you to the most common data storage systems and the appropriate tools to extract data from web or from databases like MySQL. 
382,Week 3,4,https://www.coursera.org/learn/data-cleaning?specialization=data-science-foundations-r,"Welcome to Week 3 of Getting and Cleaning Data! This week the lectures will focus on organizing, merging and managing the data you have collected using the lectures from Weeks 1 and 2. "
383,Week 4,4,https://www.coursera.org/learn/data-cleaning?specialization=data-science-foundations-r,Welcome to Week 4 of Getting and Cleaning Data! This week we finish up with lectures on text and date manipulation in R. In this final week we will also focus on peer grading of Course Projects. 
384,Week 1,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=data-science-foundations-r,This week covers the basics of analytic graphics and the base plotting system in R. We've also included some background material to help you install R if you haven't done so already. 
385,Week 2,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=data-science-foundations-r,"Welcome to Week 2 of Exploratory Data Analysis. This week covers some of the more advanced graphing systems available in R: the Lattice system and the ggplot2 system. While the base graphics system provides many important tools for visualizing data, it was part of the original R system and lacks many features that may be desirable in a plotting system, particularly when visualizing high dimensional data. The Lattice and ggplot2 systems also simplify the laying out of plots making it a much less tedious process."
386,Week 3,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=data-science-foundations-r,Welcome to Week 3 of Exploratory Data Analysis. This week covers some of the workhorse statistical methods for exploratory analysis. These methods include clustering and dimension reduction techniques that allow you to make graphical displays of very high dimensional data (many many variables). We also cover novel ways to specify colors in R so that you can use color as an important and useful dimension when making data graphics. All of this material is covered in chapters 9-12 of my book Exploratory Data Analysis with R.
387,Week 4,4,https://www.coursera.org/learn/exploratory-data-analysis?specialization=data-science-foundations-r,"This week, we'll look at two case studies in exploratory data analysis. The first involves the use of cluster analysis techniques, and the second is a more involved analysis of some air pollution data. How one goes about doing EDA is often personal, but I'm providing these videos to give you a sense of how you might proceed with a specific type of dataset. "
388,"Six Sigma Foundations, Principles, Roles and Responsibilities",4,https://www.coursera.org/learn/six-sigma-principles,"Welcome to the Six Sigma Yellow Belt Specialization! Six Sigma skills are widely sought by employers both nationally and internationally. These skills help to improve business processes and performance. Your team of instructors, Dr. Bill Bailey, Dr. David Cook, Dr. Christine Scherrer, and Dr. Gregory Wiles, currently work in the College of Engineering and Engineering Technology at Kennesaw State University. They have collaborated to create a specialization that is all encompassing of the Six Sigma methodologies for both Yellow and Green belt. Completion of this specialization will provide you with the knowledge to either continue to full Six Sigma certification or simply advance your knowledge professionally. In this module you will be introduced to the foundations of Six Sigma, the purpose of lean, and the value to the organization as a whole. "
389,Quality Tools and Six Sigma Metrics,4,https://www.coursera.org/learn/six-sigma-principles,In this module Dr. Bill Bailey will introduce you to the different types of quality tools as well as important Six Sigma metrics that can be used throughout the DMAIC process. Quality tools and metrics are a critical tool for process improvement. This module will introduce you to some of the most common quality tools as well as the most important Six Sigma metrics. 
390,Team Basics,4,https://www.coursera.org/learn/six-sigma-principles,"Teamwork is an essential component of successful quality improvement work. Many successful organizations have leaders who work in teams. In this module you will learn about why teams are so important to the Six Sigma process, the different types of teams and their different objectives, the different stages of team development, decision making methods for use in teams, and team communication methods. "
391,Lean Foundations and Principles,4,https://www.coursera.org/learn/six-sigma-principles,In this module you will be introduced to the purpose of lean and its methodologies. You will learn about the value of lean to an organization. This module will build off of what you have learned in the previous modules and help you to better understand how to better serve customers.
392,Project Identification,4,https://www.coursera.org/learn/six-sigma-tools-define-measure,"Welcome to Six Sigma Tools for Define and Measure! This is the second course in the Six Sigma Yellow Belt Specialization. Your team of instructors, Dr. Bill Bailey, Dr. David Cook, Dr. Christine Scherrer, and Dr. Gregory Wiles, currently work in the College of Engineering and Engineering Technology at Kennesaw State University. This module will introduce you to the process of project development and selection. The first phase of the DMAIC process is the Define phase which begins with project identification and proceeds onto project management. When identifying Six Sigma projects it is important to ensure that resources and time are used in the most effective and productive way. In this module you will learn about the importance of customer and stakeholder needs in relation to project development. "
393,Project Management Basics,4,https://www.coursera.org/learn/six-sigma-tools-define-measure,"Now that we have identified the Six Sigma project how to we begin to develop it? This module will cover the basics of project development and management. You will be introduced to many common aspects of a project plan as well as tools for managing your project. These tools include activity network diagrams, affinity diagrams, tree diagrams, relations charts, and matrix charts. Project Management is the second aspect of the Define phase in the DMAIC process of Six Sigma. "
394,Basic Statistics,4,https://www.coursera.org/learn/six-sigma-tools-define-measure,"The next phase of the DIMAC process is the Measure phase. We will begin by exploring basic statistics and its significance to Six Sigma. This module will introduce you to the measures of variance, range, standard deviation, center, mean, median, and mode. You will also learn how to interpret these measures using given data sets."
395,Data Collection,4,https://www.coursera.org/learn/six-sigma-tools-define-measure,"The second aspect of the Measure phase is Data Collection. In the previous module we learned about statistics and how to use data for different needs. In this module we will learn different techniques on how to collect data, the reasons for collecting data, and ensuring data accuracy and integrity."
396,Measurement System Analysis,4,https://www.coursera.org/learn/six-sigma-analyze,"Welcome to Six Sigma Tools for Analyze! This is the third course in the Six Sigma Yellow Belt Specialization. Your team of instructors, Dr. Bill Bailey, Dr. David Cook, Dr. Christine Scherrer, and Dr. Gregory Wiles, currently work in the College of Engineering and Engineering Technology at Kennesaw State University. This module will introduce you to Measurement System Analysis (MSA) which is a key component of the Measure phase of the DMAIC process. You will also learn about Gauge Repeatability & Reproducibility (GR&R) and why it is used in the measurement phase. "
397,Process Analysis Tools,4,https://www.coursera.org/learn/six-sigma-analyze,This module will introduce you to the Analysis phase of the DMAIC process. Process analysis helps you to better understand current processes and how they can be improved. You will be introduced to many of the different process analysis tools that are commonly used by Six Sigma experts. Failure Mode and Effects Analysis (FMEA) will also be introduced to help you better understand how to identify process failures. 
398,Root Cause Analysis,4,https://www.coursera.org/learn/six-sigma-analyze,"Root cause analysis is a common problem solving step. Determining the root cause of something is an important aspect of uncovering the causes of a problem. In this module you will review the different tools used in determining root cause including 5-whys, process mapping, force-field analysis, and matrix charts. "
399,Data Analysis,4,https://www.coursera.org/learn/six-sigma-analyze,In this module you will be diving into the statistical side of Six Sigma. You will begin with learning about the basic distribution types which include normal and binomial. You will then proceed to variation and will learn the difference between common and special cause variation. 
400,Correlation and Regression,5,https://www.coursera.org/learn/six-sigma-improve-control,"Welcome to Six Sigma Tools for Improve and Control! This is the fourth course in the Six Sigma Yellow Belt Specialization. Your team of instructors, Dr. Bill Bailey, Dr. David Cook, Dr. Christine Scherrer, and Dr. Gregory Wiles, currently work in the College of Engineering and Engineering Technology at Kennesaw State University. The next two modules will conclude the Analyze phase of the DMAIC process and in the final two modules you will be introduced to the Improve and Control phase. This module will continue to discussion analysis by explaining correlation and regression. You will learn the meaning of correlation and regression, the different types of analysis, and how it can be used in Six Sigma. "
401,Hypothesis Testing,5,https://www.coursera.org/learn/six-sigma-improve-control,"This the final module that covers the Analysis phase of the DMAIC process. Now that you have collected the data and calculated it you will need to determine how to make a statistical conclusion about your findings. In this module you will learn more about the importance of hypothesis testing, how to correctly do a hypothesis test reading as well as how to avoid errors, and statistical significance."
402,Improvement Techniques,5,https://www.coursera.org/learn/six-sigma-improve-control,"This module will introduce you to the Improve and Control phase of the DMAIC process. The first step in this phase is determining improvement techniques to help make improvements to an organization. This module will cover the best and most popular improvement techniques including kaizen and kaizen blitz, PDCA (Plan-Do-Check-Act) cycle, and cost benefit analysis (including cost of quality)."
403,Control Tools and Documentation,5,https://www.coursera.org/learn/six-sigma-improve-control,The final phase of the DMAIC process is the control phase. In this module you will learn about control tools that are useful in maintaining improvements. Communication is a key component of the control phase therefore it is important to develop standardized documentation. You will learn about how to standardize and manage documentation to help maintain processes that were implemented during the improve phase of DMAIC.
404, CAPSTONE PROJECT FOR COMPLETION OF YELLOW BELT SPECIALIZATION,5,https://www.coursera.org/learn/six-sigma-improve-control,"This module is the ""capstone project."" You should only complete this project if you have taken all three previous yellow belt specialization courses (Six Sigma Fundamentals, Six Sigma Tools for Define and Measure, and Six Sigma Tools for Analyze), AND you want to complete this specialization. It should be noted that completing the Yellow Belt Specialization does not give the learner ""professional accreditation"" in Six Sigma. However, successful completion will assist in better preparation for such professional accreditation testing."
405,Cryptography Overview,4,https://www.coursera.org/learn/crypto-info-theory,"This module defines the technical terms in cryptography and introduce Alice, Bob, and Eve. To study the attacker Eve's perspective, we will describe Kerckhoff's Principle (""the attacker knows the system""), which defines the scope of the attacker knowledge (what is secret vs. what the attacker can know). We will also describe Security by Obscurity, which contrasts Kerckhoff's Principle. "
406,Information Entropy,4,https://www.coursera.org/learn/crypto-info-theory,This module studies information entropy to quantify randomness. Information entropy can be used to measure the strength of the key or the secret information that determines the security of a cryptosystem against an attacker who does not know the random key. 
407,Brute-Force Attack and Cryptanalysis,4,https://www.coursera.org/learn/crypto-info-theory,"This module studies the attacker view whose objective is to learn the key and break the cryptographic protection using the key. First, we will define brute force attack and describe how to quantify the attacker effort for brute force attack. Next, we will contrast cryptanalysis and brute force attack. Lastly, we will discuss about perfect secrecy, which is immune to cryptanalysis and is a strong notion of security derived from information theory. "
408,Computational Security and Side-Channel Attack,4,https://www.coursera.org/learn/crypto-info-theory,"This module studies cryptography from the practical point of view. We will first define computational security, which relies on the fact that real world attackers are computationally limited unlike information theoretic security, e.g., one-time pad. We will then survey side channel attacks and unintentional information leakage from the physical implementations of cryptosystems. "
409,Classical Cipher: Substitution,5,https://www.coursera.org/learn/symmetric-crypto,"This module defines substitution cipher technique and describes multiple examples for substitution-based classical algorithms: Caesar Cipher, Monoalphabetic Cipher, and Vigenere Cipher (which is a type of Polyalphabetic Cipher). We will also discuss the mathematical concepts in Modulo Operations to use them to describe the cipher algorithms. "
410,Classical Cipher: Transposition,5,https://www.coursera.org/learn/symmetric-crypto,"This module studies transposition cipher which, along with substitution cipher, provides a base technique for symmetric ciphers. We define transposition cipher and product cipher and discuss transposition examples in Rail Fence and Permutation Cipher."
411,Block Cipher and DES,5,https://www.coursera.org/learn/symmetric-crypto,"This module is about modern ciphers based on product ciphers. We will first define block cipher and contrast it with stream cipher. We will then describe the ideal block cipher, which maximizes the number of transformations, and Feistel Cipher, which is a practical structure framework approximating the ideal block cipher. As a widely used cipher example based on the Feistel Cipher structure; we will study Data Encryption Standard (DES)."
412,3-DES and AES,5,https://www.coursera.org/learn/symmetric-crypto,"To provide stronger security than DES, modern symmetric ciphers can either use multiple ciphers or use an entirely different algorithm. This module reviews examples of each in Triple-DES and AES. "
413,Block Cipher Operation Modes,5,https://www.coursera.org/learn/symmetric-crypto,"Given a cipher and a key, this module reviews how to use block cipher operation modes when the data spans across multiple blocks. The module describes five popular operation modes: ECB, CBC, CFB, OFB, and CTR mode. "
414,Asymmetric Cryptography Overview,4,https://www.coursera.org/learn/asymmetric-crypto,"This module reviews the principles and requirements of asymmetric cryptography, which uses a pair of keys - with one party using a public key and the other using the corresponding private key or vice versa - in contrast to symmetric cryptography using a shared secret key. "
415,RSA Algorithm,4,https://www.coursera.org/learn/asymmetric-crypto,This module describes the RSA cipher algorithm from the key setup and the encryption/decryption operations to the Prime Factorization problem and the RSA security. 
416,Diffie-Hellman Key Exchange,4,https://www.coursera.org/learn/asymmetric-crypto,"Diffie-Hellman Key Exchange is an asymmetric cryptographic protocol for key exchange and its security is based on the computational hardness of solving a discrete logarithm problem. This module explains the discrete logarithm problem and describes the Diffie-Hellman Key Exchange protocol and its security issues, for example, against a man-in-the-middle attack.  "
417,Key Distribution and Management,4,https://www.coursera.org/learn/asymmetric-crypto,"Cryptographic schemes build on cryptographic key, which provides asymmetry between the legitimate parties and the attacker. This module describes key distribution approaches and focuses on the approach based on public-key certificates/authority and public-key infrastructure that can support large number of parties, e.g., Internet. "
418,Cryptographic Hash Function,4,https://www.coursera.org/learn/cryptographic-hash-integrity-protection,"Cryptographic hash function is a fundamental building block in modern cryptography and is used for digital signature, message authentication, anomaly detection, pseudo-random number generator, password security, and so on. This module define cryptographic hash functions and contrast it with ordinary hash functions. It also describes the iterative structure for hash implementation to support the hash requirements. "
419,Cryptographic Hash Function Applications,4,https://www.coursera.org/learn/cryptographic-hash-integrity-protection,"Building on the previous module defining cryptographic hash functions, this module review its uses and applications. We will first describe hash chain, which chains multiple hash functions in sequence, and apply hash chain for generating one-time passwords using a scheme called S/Key. Then, we will use hash functions to construct a binary tree and describe hash tree, also known as Merkle tree. Lastly, we will review the applications of hash function and hash tree for decentralized digital currency in the forms of cryptocurrency or bitcoins. "
420,Message Authentication Code (MAC),4,https://www.coursera.org/learn/cryptographic-hash-integrity-protection,"Message authentication is to protect the message integrity and to perform sender authentication.  This module describes message authentication code (MAC) which is based on symmetric keys. It contrasts MAC with hash functions or general encryption/decryption techniques and quantify the brute force attack difficulty for MAC and discuss the security requirements for MAC. The module also reviews two MAC implementations in Data Authentication Algorithm (DAA) and Cipher-Based MAC (CMAC), which are based on the use of block ciphers."
421,Digital Signature,4,https://www.coursera.org/learn/cryptographic-hash-integrity-protection,"Like physical signatures in paper transactions, digital signature provides sender authentication and non-repudiation. This module describes how to use public-key pair to ensure the source of the packet. Then, it describes the purpose of digital signatures and the corresponding requirements. Lastly, we review a digital signature construction, which is the basis for many digital signature implementations such as RSA signature and Digital Signature Standard (DSS)."
422,Python Basics ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai,
423,Python Data Structures ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai,
424,Python Programming Fundamentals ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai,
425,Working with Data in Python ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai,
426,Analyzing US Economic Data and Building a Dashboard ,5,https://www.coursera.org/learn/python-for-applied-data-science-ai,
427,Importing Datasets,7,https://www.coursera.org/learn/data-analysis-with-python,
428,Data Wrangling,7,https://www.coursera.org/learn/data-analysis-with-python,
429, Exploratory Data Analysis,7,https://www.coursera.org/learn/data-analysis-with-python,
430,Model Development,7,https://www.coursera.org/learn/data-analysis-with-python,
431, Model Evaluation,7,https://www.coursera.org/learn/data-analysis-with-python,
432,Final Assignment,7,https://www.coursera.org/learn/data-analysis-with-python,
433,IBM Digital Badge,7,https://www.coursera.org/learn/data-analysis-with-python,
434,Introduction to Data Visualization Tools,3,https://www.coursera.org/learn/python-for-data-visualization,"In this module, you will learn about data visualization and some of the best practices to keep in mind when creating plots and visuals. You will also learn about the history and the architecture of Matplotlib and learn about basic plotting with Matplotlib. In addition, you will learn about the dataset on immigration to Canada, which will be used extensively throughout the course. Finally, you will briefly learn how to read csv files into a pandas dataframe and process and manipulate the data in the dataframe, and how to generate line plots using Matplotlib."
435,Basic and Specialized Visualization Tools,3,https://www.coursera.org/learn/python-for-data-visualization,"In this module, you learn about area plots and how to create them with Matplotlib, histograms and how to create them with Matplotlib, bar charts, and how to create them with Matplotlib, pie charts, and how to create them with Matplotlib, box plots and how to create them with Matplotlib, and scatter plots and bubble plots and how to create them with Matplotlib."
436,Advanced Visualizations and Geospatial Data,3,https://www.coursera.org/learn/python-for-data-visualization,"In this module, you will learn about advanced visualization tools such as waffle charts and word clouds and how to create them. You will also learn about seaborn, which is another visualization library, and how to use it to generate attractive regression plots. In addition, you will learn about Folium, which is another visualization library, designed especially for visualizing geospatial data. Finally, you will learn how to use Folium to create maps of different regions of the world and how to superimpose markers on top of a map, and how to create choropleth maps."
437,Introduction,5,https://www.coursera.org/learn/applied-data-science-capstone,"In this module, you will learn about the scope of this capstone course and the context of the project that you will be working on. You will learn about different location data providers and what location data is normally composed of. Finally, you will be required to submit a link to a new repository on your Github account dedicated to this course."
438,Foursquare API,5,https://www.coursera.org/learn/applied-data-science-capstone,"In this module, you will learn in details about Foursquare, which is the location data provider we will be using in this course, and its API. Essentially, you will learn how to create a Foursquare developer account, and use your credentials to search for nearby venues of a specific type, explore a particular venue, and search for trending venues around a location."
439,Neighborhood Segmentation and Clustering,5,https://www.coursera.org/learn/applied-data-science-capstone,"In this module, you will learn about k-means clustering, which is a form of unsupervised learning. Then you will use clustering and the Foursquare API to segment and cluster the neighborhoods in the city of New York. Furthermore, you will learn how to scrape website and parse HTML code using the Python package Beautifulsoup, and convert data into a pandas dataframe."
440,The Battle of Neighborhoods,5,https://www.coursera.org/learn/applied-data-science-capstone,"In this module, you will start working on the capstone project. You will clearly define a problem and discuss the data that you will be using to solve the problem."
441,The Battle of Neighborhoods (Cont'd),5,https://www.coursera.org/learn/applied-data-science-capstone,"In this module, you will carry out all the remaining work to complete your capstone project. You will submit a report of your project for peer evaluation."
442,Meet Dr. Schweidel & Course Overview,5,https://www.coursera.org/learn/meaningful-marketing-insights?specialization=marketing-analytics,"In this module, students will be introduced to the instructor, Dr. David Schweidel and get and overview of the course. "
443,"Exploring your Data with Visualization and Descriptive Statistics, Part 1",5,https://www.coursera.org/learn/meaningful-marketing-insights?specialization=marketing-analytics,"Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions."
444,"Exploring your Data with Visualization and Descriptive Statistics, Part 2",5,https://www.coursera.org/learn/meaningful-marketing-insights?specialization=marketing-analytics,"Modules 2 and 3 focus on identifying appropriate descriptive statistics (measures of central tendency and dispersion) for different types of data, as well as recoding data using reference commands to prepare it for analysis. Additionally, you will manipulate and summarize data using pivot tables in Excel, produce visualizations that are appropriate based on the type of data being analyzed, and interpret statistics and visualizations to draw conclusions to address relevant marketing questions."
445,Regression Analysis for Marketing Data,5,https://www.coursera.org/learn/meaningful-marketing-insights?specialization=marketing-analytics,"In this module, you will be asked to determine the appropriate type of regression for different types of marketing data and will perform regression analysis to assess the impact of marketing actions on outcomes of interest, such as sales, traffic, and brand choices. You will also be asked to interpret regression output to understand overall model performance and importance of different predictors, as well as make predictions using the appropriate regression model."
446,From Analysis to Action,5,https://www.coursera.org/learn/meaningful-marketing-insights?specialization=marketing-analytics,"This final module will connect the results of regression analysis to marketing decisions. You will learn to build tools that allow users to evaluate outcomes based on different marketing decisions, as well as characterize the extent of uncertainty in outcomes based on the selected marketing decisions."
447,Randomness and Probability,4,https://www.coursera.org/learn/uncertainty-marketing-decisions?specialization=marketing-analytics,Module 1 focuses on developing an understanding where randomness appears in marketing problems. You will learn basic rules for calculating the probability of outcomes. We will also examine how these rules can be applied to determine the value of information
448,Conducting Monte Carlo Simulations in Excel,4,https://www.coursera.org/learn/uncertainty-marketing-decisions?specialization=marketing-analytics,"Building on the basics of randomness and probability discussed in Module 1, we examine the use of Monte Carlo simulations for incorporating randomness into business problems. Using Microsoft Excel, we will build a tool that conducts a Monte Carlo simulation. We will use this tool to evaluate the best course of action for a particular business problem."
449,Using Probability Distributions to Model Uncertainty,4,https://www.coursera.org/learn/uncertainty-marketing-decisions?specialization=marketing-analytics,"In Module 3, we look at the use of probability distributions as a means of characterizing uncertainty. We initially look at how uncertainty is incorporated into a general decision making framework. We then turn our attention to different probability distributions that can be used to model uncertainty, depending on the nature of the data. We examine the application of these probability distributions to assess the likelihood of events using features within Microsoft Excel."
450,Application: Designing Extended Service Warranty Plans,4,https://www.coursera.org/learn/uncertainty-marketing-decisions?specialization=marketing-analytics,"Building the the discussion of probability distributions in Module 3, we apply this knowledge to a specific application: the design of extended service warranty plans. We provide an overview of the business problem and discuss how to incorporate uncertainty in customers' use of the warranty plan using the Poisson distribution. Using Microsoft Excel, we design a spreadsheet tool that enables a user to adjust features of the service plans. By comparing firm profit under different scenarios, we investigate how different features of the service plan result in risk being shared by the consumer and the firm."
451,Basics of Forecasting Models,4,https://www.coursera.org/learn/forecasting-models-marketing-decisions?specialization=marketing-analytics,This module will discuss how to identify the necessary components of a forecasting model based on patterns in the history data.  You will also be able to evaluate the performance of a forecasting model using both in-sample and out-of-sample metrics.
452,Customer Analytics: Predicting Individual Customer Behavior,4,https://www.coursera.org/learn/forecasting-models-marketing-decisions?specialization=marketing-analytics,"""Meaningful Marketing Insights,"" This content will be familiar for learners who completed the first course; please think of this portion of the class as a review."
453,Managing Customer Equity: Linking Customer Analytics to Customer Value ,4,https://www.coursera.org/learn/forecasting-models-marketing-decisions?specialization=marketing-analytics,"This module will discuss managing customer equity, acquisition, retention, & market value, and customer valuation.  You will learn how to decompose customer value into its underlying components."
454,Marketing Mix Modeling,4,https://www.coursera.org/learn/forecasting-models-marketing-decisions?specialization=marketing-analytics,"A common task in developing forecasting models is to use them to make decisions regarding the marketing mix activity. With a marketing mix model, organizations can assess the efficacy of different marketing actions. Included is a sample of data for a popular frozen food category. In addition to weekly sales and pricing, for the focal brand we have information on whether the product was featured in the store’s advertising (e.g., newspaper circular) and if the product was on display in the store. We also have pricing information from competitors. In this module, we will build a series of regression models to evaluate the impact of the brand’s actions and competitors’ actions."
455,Introduction to Factor Analysis,4,https://www.coursera.org/learn/survey-analysis-marketing-insights?specialization=marketing-analytics,This module will provide readings and discussions to provide an introduction to the topic of Factor Analysis.  It is recommended that you complete the Meaningful Marketing Insights course offered by Coursera before taking this course.
456,Implementing Factor Analysis,4,https://www.coursera.org/learn/survey-analysis-marketing-insights?specialization=marketing-analytics,This module will provide lectures and exercises that will inform students on how to determine the number of factors to consider in your analysis and to evaluate the fit of the data.
457,Customer Segmentation,4,https://www.coursera.org/learn/survey-analysis-marketing-insights?specialization=marketing-analytics,This module will introduce components of customer segmentation to students.  Students will use this knowledge to be able to analyze data and make more informed business decisions.
458,Perceptual Maps,4,https://www.coursera.org/learn/survey-analysis-marketing-insights?specialization=marketing-analytics,This module will explain and describe perceptual maps.  Students will be able to create perceptual maps and also analyze data from perceptual maps.
459,Welcome,7,https://www.coursera.org/learn/ml-foundations,"Machine learning is everywhere, but is often operating behind the scenes. <p>This introduction to the specialization provides you with insights into the power of machine learning, and the multitude of intelligent applications you personally will be able to develop and deploy upon completion.</p>We also discuss who we are, how we got here, and our view of the future of intelligent applications."
460,Regression: Predicting House Prices,7,https://www.coursera.org/learn/ml-foundations,"This week you will build your first intelligent application that makes predictions from data.<p>We will explore this idea within the context of our first case study, predicting house prices, where you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,...).  <p>This is just one of the many places where regression can be applied.Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.</p>You will also examine how to analyze the performance of your predictive model and implement regression in practice using a Jupyter notebook."
461,Classification: Analyzing Sentiment,7,https://www.coursera.org/learn/ml-foundations,"How do you guess whether a person felt positively or negatively about an experience, just from a short review they wrote?<p>In our second case study, analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,...).This task is an example of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification.</p>You will analyze the accuracy of your classifier, implement an actual classifier in a Jupyter notebook, and take a first stab at a core piece of the intelligent application you will build and deploy in your capstone.  "
462,Clustering and Similarity: Retrieving Documents,7,https://www.coursera.org/learn/ml-foundations,"A reader is interested in a specific news article and you want to find a similar articles to recommend.  What is the right notion of similarity?  How do I automatically search over documents to find the one that is most similar?  How do I quantitatively represent the documents in the first place?<p>In this third case study, retrieving documents, you will examine various document representations and an algorithm to retrieve the most similar subset.  You will also consider structured representations of the documents that automatically group articles by similarity (e.g., document topic).</p>You will actually build an intelligent document retrieval system for Wikipedia entries in an Jupyter notebook."
463,Recommending Products,7,https://www.coursera.org/learn/ml-foundations,"Ever wonder how Amazon forms its personalized product recommendations?  How Netflix suggests movies to watch?  How Pandora selects the next song to stream?  How Facebook or LinkedIn finds people you might connect with?  Underlying all of these technologies for personalized content is something called collaborative filtering. <p>You will learn how to build such a recommender system using a variety of techniques, and explore their tradeoffs.</p> One method we examine is matrix factorization, which learns features of users and products to form recommendations.  In a Jupyter notebook, you will use these techniques to build a real song recommender system."
464,Deep Learning: Searching for Images,7,https://www.coursera.org/learn/ml-foundations,"You’ve probably heard that Deep Learning is making news across the world as one of the most promising techniques in machine learning. Every industry is dedicating resources to unlock the deep learning potential, including for tasks such as image tagging, object recognition, speech recognition, and text analysis.<p>In our final case study, searching for images, you will learn how layers of neural networks provide very descriptive (non-linear) features that provide impressive performance in image classification and retrieval tasks.  You will then construct deep features, a transfer learning technique that allows you to use deep learning very easily, even when you have little data to train the model.</p>Using iPhython notebooks, you will build an image classifier and an intelligent image retrieval system with deep learning.   "
465,Closing Remarks,7,https://www.coursera.org/learn/ml-foundations,"In the conclusion of the course, we will describe the final stage in turning our machine learning tools into a service: deployment.<p>We will also discuss some open challenges that the field of machine learning still faces, and where we think machine learning is heading.  We conclude with an overview of what's in store for you in the rest of the specialization, and the amazing intelligent applications that are ahead for us as we evolve machine learning.  "
466,Welcome,8,https://www.coursera.org/learn/ml-regression,"Regression is one of the most important and broadly used machine learning and statistics tools out there.  It allows you to make predictions from data by learning the relationship between features of your data and some observed, continuous-valued response.  Regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networks.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have."
467,Simple Linear Regression,8,https://www.coursera.org/learn/ml-regression,"Our course starts from the most basic regression model: Just fitting a line to data.  This simple model for forming predictions from a single, univariate feature of the data is appropriately called ""simple linear regression"".<p> In this module, we describe the high-level regression task and then specialize these concepts to the simple linear regression case. You will learn how to formulate a simple regression model and fit the model to data using both a closed-form solution as well as an iterative optimization algorithm called gradient descent.  Based on this fitted function, you will interpret the estimated model parameters and form predictions.  You will also analyze the sensitivity of your fit to outlying observations.<p> You will examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house."
468,Multiple Regression,8,https://www.coursera.org/learn/ml-regression,"The next step in moving beyond simple linear regression is to consider ""multiple regression"" where multiple features of the data are used to form predictions.  <p> More specifically, in this module, you will learn how to build models of more complex relationship between a single variable (e.g., 'square feet') and the observed response (like 'house sales price').  This includes things like fitting a polynomial to your data, or capturing seasonal changes in the response value.  You will also learn how to incorporate multiple input variables (e.g., 'square feet', '# bedrooms', '# bathrooms').  You will then be able to describe how all of these models can still be cast within the linear regression framework, but now using multiple ""features"".   Within this multiple regression framework, you will fit models to data, interpret estimated coefficients, and form predictions. <p>Here, you will also implement a gradient descent algorithm for fitting a multiple regression model."
469,Assessing Performance,8,https://www.coursera.org/learn/ml-regression,"Having learned about linear regression models and algorithms for estimating the parameters of such models, you are now ready to assess how well your considered method should perform in predicting new data.  You are also ready to select amongst possible models to choose the best performing.  <p> This module is all about these important topics of model selection and assessment.  You will examine both theoretical and practical aspects of such analyses. You will first explore the concept of measuring the ""loss"" of your predictions, and use this to define training, test, and generalization error.  For these measures of error, you will analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance.  This leads directly to an important conversation about the bias-variance tradeoff, which is fundamental to machine learning.  Finally, you will devise a method to first select amongst models and then assess the performance of the selected model. <p>The concepts described in this module are key to all machine learning problems, well-beyond the regression setting addressed in this course."
470,Ridge Regression,8,https://www.coursera.org/learn/ml-regression,"You have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data.   In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue.  This method is called ""ridge regression"".  You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions.  To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective.  You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression.  To select the strength of the bias away from overfitting, you will explore a general-purpose method called ""cross validation"". <p>You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant."
471,Feature Selection & Lasso,8,https://www.coursera.org/learn/ml-regression,"A fundamental machine learning task is to select amongst a set of features to include in a model.  In this module, you will explore this idea in the context of multiple regression, and describe how such feature selection is important for both interpretability and efficiency of forming predictions. <p> To start, you will examine methods that search over an enumeration of models including different subsets of features.  You will analyze both exhaustive search and greedy algorithms.  Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge.  This lasso method has had impact in numerous applied domains, and the ideas behind the method have fundamentally changed machine learning and statistics. You will also implement a coordinate descent algorithm for fitting a Lasso model. <p>Coordinate descent is another, general, optimization technique, which is useful in many areas of machine learning. "
472,Nearest Neighbors & Kernel Regression,8,https://www.coursera.org/learn/ml-regression,"Up to this point, we have focused on methods that fit parametric functions---like polynomials and hyperplanes---to the entire dataset.  In this module, we instead turn our attention to a class of ""nonparametric"" methods.  These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations.  <p> We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set.  This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors.  Building on this idea, we turn to kernel regression.  Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point.  You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle.  You will also implement these techniques and observe their practical behavior."
473,Closing Remarks,8,https://www.coursera.org/learn/ml-regression,"In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to regression, as well as foundational machine learning concepts that will appear throughout the specialization.  We also briefly discuss some important regression techniques we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  "
474,Welcome!,10,https://www.coursera.org/learn/ml-classification,"Classification is one of the most widely used techniques in machine learning, with a broad array of applications, including sentiment analysis, ad targeting, spam detection, risk assessment, medical diagnosis and image classification. The core goal of classification is to predict a category or class y from some inputs x. Through this course, you will become familiar with the fundamental models and algorithms used in classification, as well as a number of core machine learning concepts. Rather than covering all aspects of classification, you will focus on a few core techniques, which are widely used in the real-world to get state-of-the-art performance. By following our hands-on approach, you will implement your own algorithms on multiple real-world tasks, and deeply grasp the core techniques needed to be successful with these approaches in practice. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have."
475,Linear Classifiers & Logistic Regression,10,https://www.coursera.org/learn/ml-classification,"Linear classifiers are amongst the most practical classification methods. For example, in our sentiment analysis case-study, a linear classifier associates a coefficient with the counts of each word in the sentence. In this module, you will become proficient in this type of representation. You will focus on a particularly useful type of linear classifier called logistic regression, which, in addition to allowing you to predict a class, provides a probability associated with the prediction. These probabilities are extremely useful, since they provide a degree of confidence in the predictions. In this module, you will also be able to construct features from categorical inputs, and to tackle classification problems with more than two class (multiclass problems). You will examine the results of these techniques on a real-world product sentiment analysis task."
476,Learning Linear Classifiers,10,https://www.coursera.org/learn/ml-classification,"Once familiar with linear classifiers and logistic regression, you can now dive in and write your first learning algorithm for classification. In particular, you will use gradient ascent to learn the coefficients of your classifier from data. You first will need to define the quality metric for these tasks using an approach called maximum likelihood estimation (MLE). You will also become familiar with a simple technique for selecting the step size for gradient ascent. An optional, advanced part of this module will cover the derivation of the gradient for logistic regression.  You will implement your own learning algorithm for logistic regression from scratch, and use it to learn a sentiment analysis classifier."
477,Overfitting & Regularization in Logistic Regression,10,https://www.coursera.org/learn/ml-classification,"As we saw in the regression course, overfitting is perhaps the most significant challenge you will face as you apply machine learning approaches in practice. This challenge can be particularly significant for logistic regression, as you will discover in this module, since we not only risk getting an overly complex decision boundary, but your classifier can also become overly confident about the probabilities it predicts. In this module, you will investigate overfitting in classification in significant detail, and obtain broad practical insights from some interesting visualizations of the classifiers' outputs. You will then add a regularization term to your optimization to mitigate overfitting. You will investigate both L2 regularization to penalize large coefficient values, and L1 regularization to obtain additional sparsity in the coefficients. Finally, you will modify your gradient ascent algorithm to learn regularized logistic regression classifiers. You will implement your own regularized logistic regression classifier from scratch, and investigate the impact of the L2 penalty on real-world sentiment analysis data."
478,Decision Trees,10,https://www.coursera.org/learn/ml-classification,"Along with linear classifiers, decision trees are amongst the most widely used classification techniques in the real world. This method is extremely intuitive, simple to implement and provides interpretable predictions. In this module, you will become familiar with the core decision trees representation. You will then design a simple, recursive greedy algorithm to learn decision trees from data. Finally, you will extend this approach to deal with continuous inputs, a fundamental requirement for practical problems. In this module, you will investigate a brand new case-study in the financial sector: predicting the risk associated with a bank loan. You will implement your own decision tree learning algorithm on real loan data."
479,Preventing Overfitting in Decision Trees,10,https://www.coursera.org/learn/ml-classification,"Out of all machine learning techniques, decision trees are amongst the most prone to overfitting. No practical implementation is possible without including approaches that mitigate this challenge. In this module, through various visualizations and investigations, you will investigate why decision trees suffer from significant overfitting problems. Using the principle of Occam's razor, you will mitigate overfitting by learning simpler trees. At first, you will design algorithms that stop the learning process before the decision trees become overly complex. In an optional segment, you will design a very practical approach that learns an overly-complex tree, and then simplifies it with pruning. Your implementation will investigate the effect of these techniques on mitigating overfitting on our real-world loan data set. "
480,Handling Missing Data,10,https://www.coursera.org/learn/ml-classification,"Real-world machine learning problems are fraught with missing data. That is, very often, some of the inputs are not observed for all data points. This challenge is very significant, happens in most cases, and needs to be addressed carefully to obtain great performance. And, this issue is rarely discussed in machine learning courses. In this module, you will tackle the missing data challenge head on. You will start with the two most basic techniques to convert a dataset with missing data into a clean dataset, namely skipping missing values and inputing missing values. In an advanced section, you will also design a modification of the decision tree learning algorithm that builds decisions about missing data right into the model. You will also explore these techniques in your real-data implementation.  "
481,Boosting,10,https://www.coursera.org/learn/ml-classification,"One of the most exciting theoretical questions that have been asked about machine learning is whether simple classifiers can be combined into a highly accurate ensemble. This question lead to the developing of boosting, one of the most important and practical techniques in machine learning today. This simple approach can boost the accuracy of any classifier, and is widely used in practice, e.g., it's used by more than half of the teams who win the Kaggle machine learning competitions. In this module, you will first define the ensemble classifier, where multiple models vote on the best prediction. You will then explore a boosting algorithm called  AdaBoost, which provides a great approach for boosting classifiers. Through visualizations, you will become familiar with many of the practical aspects of this techniques. You will create your very own implementation of AdaBoost, from scratch, and use it to boost the performance of your loan risk predictor on real data. "
482,Precision-Recall,10,https://www.coursera.org/learn/ml-classification,"In many real-world settings, accuracy or error are not the best quality metrics for classification. You will explore a case-study that significantly highlights this issue: using sentiment analysis to display positive reviews on a restaurant website. Instead of accuracy, you will define two metrics: precision and recall, which are widely used in real-world applications to measure the quality of classifiers. You will explore how the probabilities output by your classifier can be used to trade-off precision with recall, and dive into this spectrum, using precision-recall curves. In your hands-on implementation, you will compute these metrics with your learned classifier on real-world sentiment analysis data."
483,Scaling to Huge Datasets & Online Learning,10,https://www.coursera.org/learn/ml-classification,"With the advent of the internet, the growth of social media, and the embedding of sensors in the world, the magnitudes of data that our machine learning algorithms must handle have grown tremendously over the last decade. This effect is sometimes called ""Big Data"". Thus, our learning algorithms must scale to bigger and bigger datasets. In this module, you will develop a small modification of gradient ascent called stochastic gradient, which provides significant speedups in the running time of our algorithms. This simple change can drastically improve scaling, but makes the algorithm less stable and harder to use in practice. In this module, you will investigate the practical techniques needed to make stochastic gradient viable, and to thus to obtain learning algorithms that scale to huge datasets. You will also address a new kind of machine learning problem, online learning, where the data streams in over time, and we must learn the coefficients as the data arrives. This task can also be solved with stochastic gradient. You will implement your very own stochastic gradient ascent algorithm for logistic regression from scratch, and evaluate it on sentiment analysis data. "
484,Welcome,6,https://www.coursera.org/learn/ml-clustering-and-retrieval,"Clustering and retrieval are some of the most high-impact machine learning tools out there.  Retrieval is used in almost every applications and device we interact with, like in providing a set of products related to one a shopper is currently considering, or a list of people you might want to connect with on a social media platform.  Clustering can be used to aid retrieval, but is a more broadly useful tool for automatically discovering structure in data, like uncovering groups of similar patients.<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have."
485,Nearest Neighbor Search,6,https://www.coursera.org/learn/ml-clustering-and-retrieval,"We start the course by considering a retrieval task of fetching a document similar to one someone is currently reading.  We cast this problem as one of nearest neighbor search, which is a concept we have seen in the Foundations and Regression courses.  However, here, you will take a deep dive into two critical components of the algorithms: the data representation and metric for measuring similarity between pairs of datapoints.  You will examine the computational burden of the naive nearest neighbor search algorithm, and instead implement scalable alternatives using KD-trees for handling large datasets and locality sensitive hashing (LSH) for providing approximate nearest neighbors, even in high-dimensional spaces.  You will explore all of these ideas on a Wikipedia dataset, comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced."
486,Clustering with k-means,6,https://www.coursera.org/learn/ml-clustering-and-retrieval,"In clustering, our goal is to group the datapoints in our dataset into disjoint sets.  Motivated by our document analysis case study, you will use clustering to discover thematic groups of articles by ""topic"".  These topics are not provided in this unsupervised learning task; rather, the idea is to output such cluster labels that can be post-facto associated with known topics like ""Science"", ""World News"", etc.  Even without such post-facto labels, you will examine how the clustering output can provide insights into the relationships between datapoints in the dataset.  The first clustering algorithm you will implement is k-means, which is the most widely used clustering algorithm out there.  To scale up k-means, you will learn about the general MapReduce framework for parallelizing and distributing computations, and then how the iterates of k-means can utilize this framework.  You will show that k-means can provide an interpretable grouping of Wikipedia articles when appropriately tuned."
487,Mixture Models,6,https://www.coursera.org/learn/ml-clustering-and-retrieval,"In k-means, observations are each hard-assigned to a single cluster, and these assignments are based just on the cluster centers, rather than also incorporating shape information.  In our second module on clustering, you will perform probabilistic model-based clustering that provides (1) a more descriptive notion of a ""cluster"" and (2) accounts for uncertainty in assignments of datapoints to clusters via ""soft assignments"".  You will explore and implement a broadly useful algorithm called expectation maximization (EM) for inferring these soft assignments, as well as the model parameters.  To gain intuition, you will first consider a visually appealing image clustering task.  You will then cluster Wikipedia articles, handling the high-dimensionality of the tf-idf document representation considered."
488,Mixed Membership Modeling via Latent Dirichlet Allocation,6,https://www.coursera.org/learn/ml-clustering-and-retrieval,"The clustering model inherently assumes that data divide into disjoint sets, e.g., documents by topic.  But, often our data objects are better described via memberships in a collection of sets, e.g., multiple topics.  In our fourth module, you will explore latent Dirichlet allocation (LDA) as an example of such a mixed membership model particularly useful in document analysis.  You will interpret the output of LDA, and various ways the output can be utilized, like as a set of learned document features.  The mixed membership modeling ideas you learn about through LDA for document analysis carry over to many other interesting models and applications, like social network models where people have multiple affiliations.<p>Throughout this module, we introduce aspects of Bayesian modeling and a Bayesian inference algorithm called Gibbs sampling.  You will be able to implement a Gibbs sampler for LDA by the end of the module."
489,Hierarchical Clustering & Closing Remarks,6,https://www.coursera.org/learn/ml-clustering-and-retrieval,"In the conclusion of the course, we will recap what we have covered.  This represents both techniques specific to clustering and retrieval, as well as foundational machine learning concepts that are more broadly useful.<p>We provide a quick tour into an alternative clustering approach called hierarchical clustering, which you will experiment with on the Wikipedia dataset.  Following this exploration, we discuss how clustering-type ideas can be applied in other areas like segmenting time series.  We then briefly outline some important clustering and retrieval ideas that we did not cover in this course.<p> We conclude with an overview of what's in store for you in the rest of the specialization.  "
490,Selecting a research question,5,https://www.coursera.org/learn/data-visualization?specialization=data-analysis,"We would like to welcome you to Wesleyan University's Data Analysis and Interpretation Specialization. In this session, we will discuss the basics of data analysis. Your task will be to select a data set that you would like to work with and to review available code books that help you develop your own research question. You will also set up a Tumblr blog that will allow you to reflect on these experiences, submit assignments and share your work with others throughout the course. First, you may want to check out the welcome video"
491,Writing your first program - SAS or Python,5,https://www.coursera.org/learn/data-visualization?specialization=data-analysis,"In this session, we will discuss how to write a basic program that allows you to load a data set and examine frequency distributions. Your task will be to write a program that helps you to explore the variables you have selected for your own research question. You may choose either Python or SAS. Both are made freely available, and we have created a helpful guide to support you in making the decision. Once you have selected your platform, just follow the instructions in the appropriate ""GETTING STARTED...."" file, and then check out this week's video lessons aimed at helping you write and run your first program. You need only view the lessons for one of the statistical platforms (SAS or Python)."
492,Managing Data,5,https://www.coursera.org/learn/data-visualization?specialization=data-analysis,"In this session, we will help you to make and implement even more decisions with data. Statisticians often call this task 'data management', while computer scientists like the term 'data munging'. Whatever you call it, it is a vital and ongoing process when working with data. Your task will be to write a program that manages the variables you have selected for your own research question. "
493,Visualizing Data,5,https://www.coursera.org/learn/data-visualization?specialization=data-analysis,In this session we will discuss descriptive statistics and get you visualizing your newly data managed variables individually and as graphs showing the relationships between them.  
494,Supplemental Materials (All Weeks),5,https://www.coursera.org/learn/data-visualization?specialization=data-analysis,
495,Hypothesis Testing and ANOVA,4,https://www.coursera.org/learn/data-analysis-tools?specialization=data-analysis,"This session starts where the Data Management and Visualization course left off. Now that you have selected a data set and research question, managed your variables of interest and visualized their relationship graphically, we are ready to test those relationships statistically. The first group of videos describe the process of hypothesis testing which you will use throughout this course to test relationships between different kinds of variables (quantitative and categorical). Next, we show you how to test hypotheses in the context of Analysis of Variance (when you have one quantitative variable and one categorical variable). Your task will be to write a program that manages any additional variables you may need and runs and interprets an Analysis of Variance test. Note that if your research question does not include one quantitative variable, you can use one from your data set just to get some practice with the tool. If your research question does not include a categorical variable, you can categorize one that is quantitative."
496,Chi Square Test of Independence,4,https://www.coursera.org/learn/data-analysis-tools?specialization=data-analysis,"This session shows you how to test hypotheses in the context of a Chi-Square Test of Independence (when you have two categorical variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a Chi-Square Test of Independence. Note that if your research question only includes quantitative variables, you can categorize those just to get some practice with the tool. "
497,Pearson Correlation,4,https://www.coursera.org/learn/data-analysis-tools?specialization=data-analysis,"This session shows you how to test hypotheses in the context of a Pearson Correlation (when you have two quantitative variables). Your task will be to write a program that manages any additional variables you may need and runs and interprets a correlation coefficient. Note that if your research question only includes categorical variables, you can choose other variables from your data set just to get some practice with the tool. "
498,Exploring Statistical Interactions,4,https://www.coursera.org/learn/data-analysis-tools?specialization=data-analysis,"In this session, we will discuss the basic concept of statistical interaction (also known as moderation). In statistics, moderation occurs when the relationship between two variables depends on a third variable. The effect of a moderating variable is often characterized statistically as an interaction; that is, a third variable that affects the direction and/or strength of the relation between your explanatory (X) and response (Y) variable. Your task will be to test your own research question in the context of one or more potential moderating variables. "
499,Introduction to Regression,4,https://www.coursera.org/learn/regression-modeling-practice?specialization=data-analysis,"This session starts where the Data Analysis Tools course left off. This first set of videos provides you with some conceptual background about the major types of data you may work with, which will increase your competence in choosing the statistical analysis that’s most appropriate given the structure of your data, and in understanding the limitations of your data set. We also introduce you to the concept of confounding variables, which are variables that may be the reason for the association between your explanatory and response variable. Finally, you will gain experience in describing your data by writing about your sample, the study data collection procedures, and your measures and data management steps. "
500,Basics of Linear Regression,4,https://www.coursera.org/learn/regression-modeling-practice?specialization=data-analysis,"In this session, we discuss more about the importance of testing for confounding, and provide examples of situations in which a confounding variable can explain the association between an explanatory and response variable. In addition, now that you have statistically tested the association between an explanatory variable and your response variable, you will test and interpret this association using basic linear regression analysis for a quantitative response variable. You will also learn about how the linear regression model can be used to predict your observed response variable. Finally, we will also discuss the statistical assumptions underlying the linear regression model, and show you some best practices for coding your explanatory variablesNote that if your research question does not include one quantitative response variable, you can use one from your data set just to get some practice with the tool."
501,Multiple Regression,4,https://www.coursera.org/learn/regression-modeling-practice?specialization=data-analysis,"Multiple regression analysis is tool that allows you to expand on your research question, and conduct a more rigorous test of the association between your explanatory and response variable by adding additional quantitative and/or categorical explanatory variables to your linear regression model. In this session, you will apply and interpret a multiple regression analysis for a quantitative response variable, and will learn how to use confidence intervals to take into account error in estimating a population parameter. You will also learn how to account for nonlinear associations in a linear regression model. Finally, you will develop experience using regression diagnostic techniques to evaluate how well your multiple regression model predicts your observed response variable. Note that if you have not yet identified additional explanatory variables, you should choose at least one additional explanatory variable from your data set. When you go back to your codebooks, ask yourself a few questions like “What other variables might explain the association between my explanatory and response variable?”; “What other variables might explain more of the variability in my response variable?”, or even “What other explanatory variables might be interesting to explore?” Additional explanatory variables can be either quantitative, categorical, or both. Although you need only two explanatory variables to test a multiple regression model, we encourage you to identify more than one additional explanatory variable. Doing so will really allow you to experience the power of multiple regression analysis, and will increase your confidence in your ability to test and interpret more complex regression models. If your research question does not include one quantitative response variable, you can use the same quantitative response variable that you used in Module 2, or you may choose another one from your data set."
502,Logistic Regression,4,https://www.coursera.org/learn/regression-modeling-practice?specialization=data-analysis,"In this session, we will discuss some things that you should keep in mind as you continue to use data analysis in the future. We will also teach also you how to test a categorical explanatory variable with more than two categories in a multiple regression analysis. Finally, we introduce you to logistic regression analysis for a binary response variable with multiple explanatory variables. Logistic regression is simply another form of the linear regression model, so the basic idea is the same as a multiple regression analysis. But, unlike the multiple regression model, the logistic regression model is designed to test binary response variables. You will gain experience testing and interpreting a logistic regression model, including using odds ratios and confidence intervals to determine the magnitude of the association between your explanatory variables and response variable.   You can use the same explanatory variables that you used to test your multiple regression model with a quantitative outcome, but your response variable needs to be binary (categorical with 2 categories). If you have a quantitative response variable, you will have to bin it into 2 categories. Alternatively, you can choose a different binary response variable from your data set that you can use to test a logistic regression model. If you have a categorical response variable with more than two categories, you will need to collapse it into two categories."
503,Decision Trees,4,https://www.coursera.org/learn/machine-learning-data-analysis?specialization=data-analysis,"In this session, you will learn about decision trees, a type of data mining algorithm that can select from among a large number of variables those and their interactions that are most important in predicting the target or response variable to be explained. Decision trees create segmentations or subgroups in the data, by applying a series of simple rules or criteria over and over again, which choose variable constellations that best predict the target variable."
504,Random Forests,4,https://www.coursera.org/learn/machine-learning-data-analysis?specialization=data-analysis,"In this session, you will learn about random forests, a type of data mining algorithm that can select from among a large number of variables those that are most important in determining the target or response variable to be explained. Unlike decision trees, the results of random forests generalize well to new data."
505,Lasso Regression,4,https://www.coursera.org/learn/machine-learning-data-analysis?specialization=data-analysis,"Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero. Variables with a regression coefficient equal to zero after the shrinkage process are excluded from the model. Variables with non-zero regression coefficients variables are most strongly associated with the response variable. Explanatory variables can be either quantitative, categorical or both. In this session, you will apply and interpret a lasso regression analysis. You will also develop experience using k-fold cross validation to select the best fitting model and obtain a more accurate estimate of your model’s test error rate. To test a lasso regression model, you will need to identify a quantitative response variable from your data set if you haven’t already done so, and choose a few additional quantitative and categorical predictor (i.e. explanatory) variables to develop a larger pool of predictors.  Having a larger pool of predictors to test will maximize your experience with lasso regression analysis. Remember that lasso regression is a machine learning method, so your choice of additional predictors does not necessarily need to depend on a research hypothesis or theory. Take some chances, and try some new variables. The lasso regression analysis will help you determine which of your predictors are most important. Note also that if you are working with a relatively small data set, you do not need to split your data into training and test data sets. The cross-validation method you apply is designed to eliminate the need to split your data when you have a limited number of observations."
506,K-Means Cluster Analysis,4,https://www.coursera.org/learn/machine-learning-data-analysis?specialization=data-analysis,"Cluster analysis is an unsupervised machine learning method that partitions the observations in a data set into a smaller set of clusters where each observation belongs to only one cluster. The goal of cluster analysis is to group, or cluster, observations into subsets based on their similarity of responses on multiple variables. Clustering variables should be primarily quantitative variables, but binary variables may also be included. In this session, we will show you how to use k-means cluster analysis to identify clusters of observations in your data set. You will gain experience in interpreting cluster analysis results by using graphing methods to help you determine the number of clusters to interpret, and examining clustering variable means to evaluate the cluster profiles. Finally, you will get the opportunity to validate your cluster solution by examining differences between clusters on a variable not included in your cluster analysis.  You can use the same variables that you have used in past weeks as clustering variables. If most or all of your previous explanatory variables are categorical, you should identify some additional quantitative clustering variables from your data set. Ideally, most of your clustering variables will be quantitative, although you may also include some binary variables. In addition, you will need to identify a quantitative or binary response variable from your data set that you will not include in your cluster analysis. You will use this variable to validate your clusters by evaluating whether your clusters differ significantly on this response variable using statistical methods, such as analysis of variance or chi-square analysis, which you learned about in Course 2 of the specialization (Data Analysis Tools). Note also that if you are working with a relatively small data set, you do not need to split your data into training and test data sets."
507,Course Overview and Data Setup,8,https://www.coursera.org/learn/sas-programming-basics,In this module you learn about the course and you set up the data you need to do the practices in the course. 
508,Essentials,8,https://www.coursera.org/learn/sas-programming-basics,In this module you learn how to use SAS programming tools and the fundamentals of SAS program structure and syntax. 
509,Accessing Data,8,https://www.coursera.org/learn/sas-programming-basics,"In this module, you learn to identify the features of a SAS table, access data through SAS libraries, and import data into SAS."
510,Exploring and Validating Data,8,https://www.coursera.org/learn/sas-programming-basics,"In this module, you learn to use SAS procedures that provide insights about your data. You also learn to subset data so you can focus on particular segments, format data so you can easily understand it, and sort data to identify and resolve duplicate values."
511,Preparing Data,8,https://www.coursera.org/learn/sas-programming-basics,"In this module, you learn how to do some common data manipulations, such as filtering rows and columns, computing new columns, and performing conditional processing."
512,Analyzing and Reporting on Data,8,https://www.coursera.org/learn/sas-programming-basics,"In this module, we concentrate on summarizing data by using the SAS procedures that we touched on for data exploration. You also learn how to use titles, column labels, footnotes, and macro variables to enhance your reports and make them more meaningful."
513,Exporting Results,8,https://www.coursera.org/learn/sas-programming-basics,"In this module, you learn to export SAS tables and results to Excel, Microsoft Word, and PDF files. "
514,Using SQL in SAS,8,https://www.coursera.org/learn/sas-programming-basics,"In this module, you learn to use the SQL procedure to read and filter data. You also learn to create and join tables by using SQL."
515,Course Overview and Data Setup,8,https://www.coursera.org/learn/sas-programming-advanced,In this module you get an overview of what you learn in this course and you set up the software and data you use for activities and practices in the course.
516,Controlling DATA Step Processing,8,https://www.coursera.org/learn/sas-programming-advanced,"In this module, we dig deeper into the DATA step. You learn how the DATA step processes data behind the scenes. Then you use this knowledge to control when and where the DATA step outputs rows to new tables."
517,Summarizing Data,8,https://www.coursera.org/learn/sas-programming-advanced,"In this module, you learn new syntax that enables you to alter the default behavior of the DATA step to solve a problem. First you learn to create an accumulating column, or in other words generate a running total.  Then you learn to process data in groups, so you can perform an action when each group begins or ends.   "
518,Manipulating Data with Functions,8,https://www.coursera.org/learn/sas-programming-advanced,"In this module, you learn to use some new functions that enable you to manipulate numeric, date, and character values. In addition, you learn to use functions that change a column from one data type to another."
519,Creating and Using Custom Formats,8,https://www.coursera.org/learn/sas-programming-advanced,"In this module, you learn to create and use custom formats to enhance the way your data is displayed in a table or report."
520,Combining Tables,8,https://www.coursera.org/learn/sas-programming-advanced,"In this module, we take a comprehensive look at combining tables by using the DATA step. You learn to concatenate tables, merge tables, and identify matching and nonmatching rows."
521,Processing Repetitive Code,8,https://www.coursera.org/learn/sas-programming-advanced,"In this module, you learn to save time by taking advantage of iterative processing with DO loops. First you learn to create an iterative DO Loop, then you learn to create conditional DO loops."
522,Restructuring Tables,8,https://www.coursera.org/learn/sas-programming-advanced,"In this module, you learn techniques that can be used to transpose or restructure a table. First you learn to restructure data with the DATA step. Then you learn to restructure data by using the TRANSPOSE procedure."
523,Course Overview and Data Setup,7,https://www.coursera.org/learn/sas-programming-certification-review,In this module you get an overview of this course and set up the data you need for practices and activities.
524,"Review of Getting Started with SAS Programming, Part 1",7,https://www.coursera.org/learn/sas-programming-certification-review,"This module is a review of the first three modules of the Getting Started with SAS Programming course. Lectures demonstrate the concepts you learned, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material."
525,"Review of Getting Started with SAS Programming, Part 2",7,https://www.coursera.org/learn/sas-programming-certification-review,"This module reviews the preparing, analyzing and exporting modules of the Getting Started with SAS Programming course. Lectures demonstrate the concepts you learned, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material."
526,Case Study/Programming Assignment: Analyze TSA Claims Data,7,https://www.coursera.org/learn/sas-programming-certification-review,This module enables you to apply what you learned in Getting Started with SAS Programming to a real programming problem.
527,"Review of Doing More with SAS Programming, Part 1",7,https://www.coursera.org/learn/sas-programming-certification-review,"This module is a review of the first four modules of the Doing More with SAS Programming course. Lectures demonstrate the concepts you learned about for preparing data, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material."
528,"Review of Doing More with SAS Programming, Part 2",7,https://www.coursera.org/learn/sas-programming-certification-review,"This module is a review of the last three modules of the Doing More with SAS Programming course. Lectures demonstrate the concepts you learned about for preparing data, and readings from the SAS Certification Prep Guide reinforce those concepts. The review and programming questions assess your understanding of the material."
529,Case Study: Preparing World Tourism Data,7,https://www.coursera.org/learn/sas-programming-certification-review,
530,Program Introduction,3,https://www.coursera.org/learn/macroscopic-microscopic-thermodynamics?specialization=statistical-thermodynamics-engineering,"In this module we explore the basics of macroscopic thermodynamics from a postulatory point of view. In this view, the meaning of temperature, thermodynamic pressure and chemical potential are especially clear and easy to understand. In addition, the development of the Fundamental Relation and its various transformations leads to a clear path to property relations."
531,Macroscopic Thermodynamics,3,https://www.coursera.org/learn/macroscopic-microscopic-thermodynamics?specialization=statistical-thermodynamics-engineering," We then explore the relationship between atomic and molecu-lar structure and macroscopic properties by taking a statistical point of view.  Using apostulatory approach, the method for doing this is made clear.  This leads to the devel-opment of the partition function which describes the distribution of molecular quantumstates as a function of the independent, macroscopic thermodynamic properties."
532,Microscopic Thermodynamics,3,https://www.coursera.org/learn/macroscopic-microscopic-thermodynamics?specialization=statistical-thermodynamics-engineering,"Here we explore microscopic thermodynamics from a postulatory point of view. We introduce the concept of ensembles as needed to understand the relationship between atomic and molecular structural properties and macroscopic properties. This leads to the partition function which relsates the distribution of molecular quantum states as a function of the independent, macroscopic thermodynamic properties."
533,Introduction to quantum mechanics,3,https://www.coursera.org/learn/quantum-mechanics?specialization=statistical-thermodynamics-engineering,"Module 1presents an introduction to quantum mechanics at a level appropriate for those with mechanical or aerospace engineering backgrounds. Using a postulatory approach that describes the steps to follow, the Schrodinger wave equation is derived and it is showen that the time dependence can be separated and a stationary wave equation results."
534,Simple Solutions of the Wave Equation,3,https://www.coursera.org/learn/quantum-mechanics?specialization=statistical-thermodynamics-engineering,"In module 2 we solve the stationary wave equation for several simple systems. These include the particle in a box, the rigid rotator, the harmonic oscillator, and the hydrogenic atom. These simple solutions form the basisi for discussing real atomic and molecular behavior in the next module."
535,Real Atomic and Molecular Behavior,3,https://www.coursera.org/learn/quantum-mechanics?specialization=statistical-thermodynamics-engineering,In Module 3 we explore the more realistic behavior of atoms and molecules. We also introduce and discuss numerical methods for solving the wave equation.
536,Simple Ideal Gas Property Relations,3,https://www.coursera.org/learn/ideal-gases?specialization=statistical-thermodynamics-engineering,"Module 1 starts an exploration of systems for which intermolecular forces are not important. This is done by evaluating the appropriate partition functions for translational, rotational, vibrational and/or electronic motion.  In this module we explore pure ideal gases including monatomic, diatomic and polyatomic species. We also explore literature sources of properties and empirical estimation methods."
537,Mixtures,3,https://www.coursera.org/learn/ideal-gases?specialization=statistical-thermodynamics-engineering,In Module 2 we discuss both non-reacting and  reacting ideal gas mixtures as both have many industrial applications.  Computational methods for calculating equilibrium properties are introduced.  
538,Photon and Electron Gases,3,https://www.coursera.org/learn/ideal-gases?specialization=statistical-thermodynamics-engineering,"Interestingly, in addition to normal low density gases, photons and electrons in metals can be described as though they are ideal gases and so we discuss them."
539,The Configuration Integral,4,https://www.coursera.org/learn/dense-gases-liquids-solids?specialization=statistical-thermodynamics-engineering,"As the density of a gas is increased, intermolecular forces begin to affect behavior. For small departures from ideal gas behavior, known as the dense gas limit, one can estimate the change in properties using the concept of a configuration integral, a modification to the partition function. This leads to the development of equations of state that are expansions in density from the ideal gas limit. Inter molecular potential energy functions are introduced and it is explored how they impact P-V-T behavior. "
540,Thermodynamic Stability,4,https://www.coursera.org/learn/dense-gases-liquids-solids?specialization=statistical-thermodynamics-engineering," As the density is increased, there is a transition to the liquid state. We explore whether this transition is smooth or abrupt by examining the stability of a thermodynamic system to small perturbations. We also explore Gibb's phase rule."
541,"The radial distribution function, thermodynamic properties, and MD simulations of liquid properties",4,https://www.coursera.org/learn/dense-gases-liquids-solids?specialization=statistical-thermodynamics-engineering,"In this Module we present a brief discussion regarding the determination of the thermodynamic properties of liquids using the concept of the radial distribution function (RDF), and how the function relates to thermodynamic properties. This includes introducing the use of molecular dynamics to obtain the radial distribution function."
542,Crystalline Solids,4,https://www.coursera.org/learn/dense-gases-liquids-solids?specialization=statistical-thermodynamics-engineering,It turns out that we can use the results of simple statistical thermodynamics to describe the behavior of crystalline solids.  
543,Algorithms and Starting to Code,4,https://www.coursera.org/learn/algorithms-data-collection-code,
544,"Data Types, Variables, and Constants",4,https://www.coursera.org/learn/algorithms-data-collection-code,
545,Data Collection and More Algorithms,4,https://www.coursera.org/learn/algorithms-data-collection-code,
546,STEM Computations,4,https://www.coursera.org/learn/algorithms-data-collection-code,
547,DATA ANALYSIS,4,https://www.coursera.org/learn/data-analysis-representation-selection-iteration,
548,Selection,4,https://www.coursera.org/learn/data-analysis-representation-selection-iteration,
549,Data Representation,4,https://www.coursera.org/learn/data-analysis-representation-selection-iteration,
550,Iteration,4,https://www.coursera.org/learn/data-analysis-representation-selection-iteration,
551,Arrays,4,https://www.coursera.org/learn/abstraction-problem-decomposition-functions,
552,Strings,4,https://www.coursera.org/learn/abstraction-problem-decomposition-functions,
553,Abstraction and Problem Decomposition,4,https://www.coursera.org/learn/abstraction-problem-decomposition-functions,
554,Functions,4,https://www.coursera.org/learn/abstraction-problem-decomposition-functions,
555,File IO and Automation,4,https://www.coursera.org/learn/simulation-algorithm-analysis-pointers,
556,Simulation and Parallelization,4,https://www.coursera.org/learn/simulation-algorithm-analysis-pointers,
557,Algorithm Analysis,4,https://www.coursera.org/learn/simulation-algorithm-analysis-pointers,
558,Pointers,4,https://www.coursera.org/learn/simulation-algorithm-analysis-pointers,
559,A Crash Course in Data Science,1,https://www.coursera.org/learn/data-science-course?specialization=executive-data-science,"This one-module course constitutes the first ""week"" of the Executive Data Science Specialization. This is an intensive introduction to what you need to know about data science itself. You'll learn important terminology and how successful organizations use data science."
560,Building a Data Science Team,1,https://www.coursera.org/learn/build-data-science-team?specialization=executive-data-science,"Welcome to Building a Data Science Team! This course is one module, intended to be taken in one week.  the course works best if you follow along with the material in the order it is presented. Each lecture consists of videos and reading materials and every lecture has a 5 question quiz. You need to get 4 out of 5 or better on the quiz to pass. Overall the quizzes are worth 17% of your grade each, with the exception of the last quiz, which is worth 15%. I'm excited to have you in the class and look forward to your contributions to the learning community. Click Discussions to see forums where you can discuss the course material with fellow students taking the class. Be sure to introduce yourself to everyone in the Meet and Greet forum.If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center.Good luck as you get started, and I hope you enjoy the course!  -Jeff"
561,Managing Data Analysis,1,https://www.coursera.org/learn/managing-data-analysis?specialization=executive-data-science,"Welcome to Managing Data Analysis! This course is one module, intended to be taken in one week. The course works best if you follow along with the material in the order it is presented. Each lecture consists of videos and reading materials that expand on the lecture. I'm excited to have you in the class and look forward to your contributions to the learning community. If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center. Good luck as you get started, and I hope you enjoy the course!"
562,"Introduction, the perfect data science experience",1,https://www.coursera.org/learn/real-life-data-science?specialization=executive-data-science,"This course is one module, intended to be taken in one week. Please do the course roughly in the order presented. Each lecture has reading and videos. Except for the introductory lecture, every lecture has a 5 question quiz; get 4 out of 5 or better on the quiz."
563,Executive Data Science Capstone,1,https://www.coursera.org/learn/executive-data-science-capstone,"It's time to put your skills to the test managing a data science project at Zillow, a data-driven online real estate and rental marketplace. Along the way, you'll make important decisions as you lead your team through the project. "
564,Introduction to Statistics in Public Health,4,https://www.coursera.org/learn/introduction-statistics-data-analysis-public-health,"Statistics has played a critical role of in public health research and practice, and you’ll start by looking at two examples: one from eighteenth century London and the other by the United Nations. The first task in carrying out a research study is to define the research question and express it as a testable hypothesis. With examples from the media, you’ll see what does and does not work in this regard, giving you a chance to define a research question from some real news stories."
565,"Types of Variables, Common Distributions and Sampling",4,https://www.coursera.org/learn/introduction-statistics-data-analysis-public-health,"This module will introduce you to some of the key building blocks of knowledge in statistical analysis: types of variables, common distributions and sampling. You’ll see the difference between “well-behaved” data distributions, such as the normal and the Poisson, and real-world ones that are common in public health data sets."
566,Introduction to R and RStudio,4,https://www.coursera.org/learn/introduction-statistics-data-analysis-public-health,"Now it’s time to get started with the powerful and completely free statistical software R and its popular interface RStudio. With the example of fruit and vegetable consumption, you’ll learn how to download R, import the data set and run essential descriptive analyses to get to know the variables."
567,Hypothesis Testing in R,4,https://www.coursera.org/learn/introduction-statistics-data-analysis-public-health,"Having learned how to define a research question and testable hypothesis earlier in the course, you’ll learn how to apply hypothesis testing in R and interpret the result. As all medical knowledge is derived from a sample of patients, random and other kinds of variation mean that what you measure on that sample, such as the average body mass index, is not necessarily the same as in the population as a whole. It’s essential that you incorporate this uncertainty in your estimate of average BMI when presenting it. This involves the calculation of a p value and confidence interval, fundamental concepts in statistical analysis. You’ll see how to do this for averages and proportions."
568,INTRODUCTION TO LINEAR REGRESSION,4,https://www.coursera.org/learn/linear-regression-r-public-health,"Before jumping ahead to run a regression model, you need to understand a related concept: correlation. This week you’ll learn what it means and how to generate Pearson’s and Spearman’s correlation coefficients in R to assess the strength of the association between a risk factor or predictor and the patient outcome. Then you’ll be introduced to linear regression and the concept of model assumptions, a key idea underpinning so much of statistical analysis."
569,Linear Regression in R,4,https://www.coursera.org/learn/linear-regression-r-public-health,"You’ll be introduced to the COPD data set that you’ll use throughout the course and will run basic descriptive analyses. You’ll also practise running correlations in R. Next, you’ll see how to run a linear regression model, firstly with one and then with several predictors, and examine whether model assumptions hold."
570,Multiple Regression and Interaction,4,https://www.coursera.org/learn/linear-regression-r-public-health,"Now you’ll see how to extend the linear regression model to include binary and categorical variables as predictors and learn how to check the correlation between predictors. Then you’ll see how predictors can interact with each other and how to incorporate the necessary interaction terms into the model and interpret them. Different kinds of interactions exist and can be challenging to interpret, so we will take it slowly with worked examples and opportunities to practise."
571,MODEL BUILDING,4,https://www.coursera.org/learn/linear-regression-r-public-health,"The last part of the course looks at how to build a regression model when you have a choice of what predictors to include in it. It describes commonly used automated procedures for model building and shows you why they are so problematic. Lastly, you’ll have the chance to fit some models using a more defensible and robust approach."
572,Introduction to Logistic Regression,4,https://www.coursera.org/learn/logistic-regression-r-public-health,"Welcome to Statistics for Public Health: Logistic Regression for Public Health! In this week, you will be introduced to logistic regression and its uses in public health. We will focus on why linear regression does not work with binary outcomes and on odds and odds ratios, and you will finish the week by practising your new skills. By the end of this week, you will be able to explain when it is valid to use logistic regression, and define odds and odds ratios. Good luck!"
573,Logistic Regression in R,4,https://www.coursera.org/learn/logistic-regression-r-public-health,"In this week, you will learn how to prepare data for logistic regression, how to describe data in R, how to run a simple logistic regression model in R, and how to interpret the output. You will also have the opportunity to practise your new skills. By the end of this week, you will be able to run simple logistic regression analysis in R and interpret the output. Good luck! "
574,Running Multiple Logistic Regression in R,4,https://www.coursera.org/learn/logistic-regression-r-public-health,"Now that you're happy with including one predictor in the model, this week you'll learn how to run multiple logistic regression, including describing and preparing your data and running new logistic regression models. You will have the opportunity to practise your new skills. By the end of the week, you will be able to run multiple logistic regression analysis in R and interpret the output. Good luck!"
575,Assessing Model Fit,4,https://www.coursera.org/learn/logistic-regression-r-public-health,"Welcome to the final week of the course! In this week, you will learn how to assess model fit and model performance, how to avoid the problem of overfitting, and how to choose what variables from your data set should go into your multiple regression model. You will put all the skills you have learned throughout the course into practice. By the end of this week, you will be able to evaluate the model assumptions for multiple logistic regression in R, and describe and compare some common ways to choose a multiple regression model. Good luck! "
576,The Kaplan-Meier Plot ,4,https://www.coursera.org/learn/survival-analysis-r-public-health,"What is survival analysis? You’ll see what it is, when to use it and how to run and interpret the most common descriptive survival analysis method, the Kaplan-Meier plot and its associated log-rank test for comparing the survival of two or more patient groups, e.g. those on different treatments. You’ll learn about the key concept of censoring."
577,The Cox Model,4,https://www.coursera.org/learn/survival-analysis-r-public-health,"This week you’ll get to know the most commonly used survival analysis method for incorporating not just one but multiple predictors of survival: Cox proportional hazards regression modelling. You’ll learn about the key concepts of hazards and the risk set. From now and until the end of this course, there’ll be plenty of chance to run Cox models on data simulated from real patient-level records for people admitted to hospital with heart failure. You’ll see why missing data and categorical variables can cause problems in regression models such as Cox."
578,The Multiple Cox Model,4,https://www.coursera.org/learn/survival-analysis-r-public-health,"You’ll extend the simple Cox model to the multiple Cox model. As preparation, you’ll run the essential descriptive statistics on your main variables. Then you’ll see what can happen with real-life public health data and learn some simple tricks to fix the problem."
579,The Proportionality Assumption,4,https://www.coursera.org/learn/survival-analysis-r-public-health,"In this final part of the course, you’ll learn how to assess the fit of the model and test the validity of the main assumptions involved in Cox regression such as proportional hazards. This will cover three types of residuals. Lastly, you’ll get to practise fitting a multiple Cox regression model and will have to decide which predictors to include and which to drop, a ubiquitous challenge for people fitting any type of regression model."
580,Course Orientation,5,https://www.coursera.org/learn/firm-level-economics?specialization=managerial-economics-business-analysis,"You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course."
581,"Module 1: Scarcity, Allocation, and Markets",5,https://www.coursera.org/learn/firm-level-economics?specialization=managerial-economics-business-analysis,"The fundamental problem of scarcity challenges us to think about an allocation mechanism to determine what is produced and who consumes it. We will discuss scarcity and allocation mechanisms. In this course, we will focus on markets and prices as the solution to this resource allocation problem."
582,Module 2: Government Intervention in Markets,5,https://www.coursera.org/learn/firm-level-economics?specialization=managerial-economics-business-analysis,Markets are frequent targets of governments. This module will introduce government policy intervention into the market. This intervention can be direct control of prices or it could be indirect price pressure through the imposition of taxes or subsidies. Both forms of intervention are impacted by elasticity.
583,"Module 3: Firms, Production, and Costs",5,https://www.coursera.org/learn/firm-level-economics?specialization=managerial-economics-business-analysis,"This module will introduce cost theory. Firms are interested in producing profits, which are the residuals when costs are subtracted from revenue. Earlier modules constructed demand curves. They give us an idea of how many units of product we can sell at different prices; this would be firm revenue. We will work to understand inputs, production, and costs."
584,Module 4: Firm Behavior,5,https://www.coursera.org/learn/firm-level-economics?specialization=managerial-economics-business-analysis,"The firm goal of profit maximization requires an understanding of costs and revenues.  In this module, we will see how a firm optimally responds to a given market price by finding the profit maximizing output.  The level of profits at this maximum profit point will help determine short run equilibrium."
585,Course Orientation,5,https://www.coursera.org/learn/firm-level-economics-markets?specialization=managerial-economics-business-analysis,"You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course."
586,Module 1: Perfect Competition,5,https://www.coursera.org/learn/firm-level-economics-markets?specialization=managerial-economics-business-analysis,"This module introduces the concept of a perfectly competitive market. It is a benchmark construction, but it accurately models many markets in our economy. We will understand equilibrium outcomes in both the short run and the long run. We will understand how to analyze shocks to these equilibria."
587,Module 2: Monopoly Markets and Efficiency,5,https://www.coursera.org/learn/firm-level-economics-markets?specialization=managerial-economics-business-analysis,"Analysts can predict equilibrium outcomes with some degree of certainty. We want to construct a measure of efficiency that will allow us to evaluate the attractiveness of these equilibrium market outcomes. After using this metric to consider the efficiency of the competitive market, we will introduce a different market structure, monopoly, and use our efficiency metric to evaluate the equilibrium resource allocation under monopoly."
588,Module 3: Oligopoly and Game Theory,5,https://www.coursera.org/learn/firm-level-economics-markets?specialization=managerial-economics-business-analysis,"Perfectly competitive markets have many sellers. Monopoly has one seller. But much economic activity takes place in markets with just a handful of very large producers. These are called oligopoly markets. We will look at collusive arrangements among a small number of rivals, and then will use simple game theoretic techniques to model equilibrium."
589,Module 4: Market Failures,5,https://www.coursera.org/learn/firm-level-economics-markets?specialization=managerial-economics-business-analysis,"Sometimes even markets that appear to be capable of great efficiency in resource allocation, such as the perfectly competitive market, can fall short of efficiency. Economists call this market failure. In this module, we will consider information issues and the impact on efficiency. We will also introduce externalities (spillovers) such as pollution and model these impacts."
590,Course Orientation,5,https://www.coursera.org/learn/country-level-economics?specialization=managerial-economics-business-analysis,"You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course."
591,Module 1: Key Macroeconomic Indicators and Their Measurement,5,https://www.coursera.org/learn/country-level-economics?specialization=managerial-economics-business-analysis,"What do macroeconomic indicators like GDP, the unemployment rate, and inflation really mean? How are they measured? How should the figures for such variables be interpreted?"
592,"Module 2: GDP Components, Twin Deficits, and Balance of Payments",5,https://www.coursera.org/learn/country-level-economics?specialization=managerial-economics-business-analysis,"Expenditure is often different from income for individuals, but for the economy as a whole, aggregate income is always identical to aggregate expenditure. This has important implications for the functioning of the macroeconomy and the way policies affect it. The income-expenditure identity is also fundamental to the ways various part of the economy with different processes interact with each other. For example, it sheds a lot of light on the formation of the trade deficit and its connection with budget deficit and private savings."
593,Module 3: The Foreign Exchange Market,5,https://www.coursera.org/learn/country-level-economics?specialization=managerial-economics-business-analysis,"How does the exchange rate affect the trade balance and foreign payments of an economy? How does the exchange rate interact with domestic and foreign prices to determine the competitiveness of an economy’s producers? Where does the exchange rate come from? Since currencies are assets that can be bought and resold at different times, their exchange rates must depend on expectations and futures markets. How do the spot and forward exchange rates interact with the expected rates of future dates?"
594,"Module 4: Money, Interest Rate, and the Exchange Rate",5,https://www.coursera.org/learn/country-level-economics?specialization=managerial-economics-business-analysis,"The interest rate determines the exchange rate, the cost of capital, and the opportunity cost of using money. How is the interest rate determined? What factors drive the supply and demand for money? What constitutes money? What role do banks play in the monetary system? How do central banks influence the money market and the interest rate?"
595,Course Orientation,5,https://www.coursera.org/learn/macroeconomic-factors?specialization=managerial-economics-business-analysis,"You will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course."
596,"Module 1:  Aggregate Expenditure and GDP in the Short Run When Prices Are ""Sticky""",5,https://www.coursera.org/learn/macroeconomic-factors?specialization=managerial-economics-business-analysis,"What determines the GDP? In the previous course on Macroeconomic Variables and Markets, we saw how the exchange rate and the interest rate are determined given the real income, aggregate price level, and expectations about the future. This module focuses on GDP determination in the short run, which is a critical step in understanding macroeconomic fluctuations and the role of stabilization policies. Long-run trends, expectations, and price level movements will be examined in subsequent modules."
597,Module 2: Expectations and the Long-Run Exchange Rate,5,https://www.coursera.org/learn/macroeconomic-factors?specialization=managerial-economics-business-analysis,"Where do expectations about the future of economic variables come from? This question is important because expectations matter a great deal in the choices made by economic agents at every point in time. This is quite easy to see in the connection between the exchange rate in the spot market at each moment and the expected exchange rate in the future spot market. In this module, we examine the formation of exchange rate expectations based on a model of long-run equilibrium in the foreign exchange market. The model turns out to be very insightful regarding the factors that drive the real exchange rates and competitiveness of economies over the years."
598,Module 3: Long-Run Economic Performance and Short-Run Adjustments,5,https://www.coursera.org/learn/macroeconomic-factors?specialization=managerial-economics-business-analysis,Why are some nations so much poorer than others? Why do some countries manage to grow fast over decades while others stagnate? What determines the real per capita income in a country in the long run? One goal of this module is to examine these fundamental questions in economics. The module also discusses the relationship between short-run and long-run equilibria and shows how the process of adjustment may lead to macroeconomic instability. It ends by examining the role of monetary and fiscal policies in stabilizing or destabilizing the economy as it goes through the adjustment process.
599,Module 4: Institutions and Macroeconomic Policies,5,https://www.coursera.org/learn/macroeconomic-factors?specialization=managerial-economics-business-analysis,Some countries seem to be much more prone to macroeconomic crises and stagnation than others. Why do policymakers in some countries fail to follow more productive and stabilizing policies? What roles do a country’s politics and institutions play in the policy choices by the government and the central bank? What factors and variables do we need to know about in order to be able to assess a country’s long-term macroeconomic prospects?
600,Week 1,4,https://www.coursera.org/learn/python-data-analysis?specialization=data-science-python,"In this week you'll get an introduction to the field of data science, review common Python functionality and features which data scientists use, and be introduced to the Coursera Jupyter Notebook for the lectures. All of the course information on grading, prerequisites, and expectations are on the course syllabus, and you can find more information about the Jupyter Notebooks on our Course Resources page."
601,Week 2,4,https://www.coursera.org/learn/python-data-analysis?specialization=data-science-python,"In this week of the course you'll learn the fundamentals of one of the most important toolkits Python has for data cleaning and processing -- pandas. You'll learn how to read in data into DataFrame structures, how to query these structures, and the details about such structures are indexed. The module ends with a programming assignment and a discussion question."
602,Week 3,4,https://www.coursera.org/learn/python-data-analysis?specialization=data-science-python,"In this week you'll deepen your understanding of the python pandas library by learning how to merge DataFrames, generate summary tables, group data into logical pieces, and manipulate dates. We'll also refresh your understanding of scales of data, and discuss issues with creating metrics for analysis. The week ends with a more significant programming assignment."
603,Week 4,4,https://www.coursera.org/learn/python-data-analysis?specialization=data-science-python,"In this week of the course you'll be introduced to a variety of statistical techniques such a distributions, sampling and t-tests. The majority of the week will be dedicated to your course project, where you'll engage in a real-world data cleaning activity and provide evidence for (or against!) a given hypothesis. This project is suitable for a data science portfolio, and will test your knowledge of cleaning, merging, manipulating, and test for significance in data. The week ends with two discussions of science and the rise of the fourth paradigm -- data driven discovery."
604,Module 1: Principles of Information Visualization,4,https://www.coursera.org/learn/python-plotting?specialization=data-science-python,"In this module, you will get an introduction to principles of information visualization. We will be introduced to tools for thinking about design and graphical heuristics for thinking about creating effective visualizations. All of the course information on grading, prerequisites, and expectations are on the course syllabus, which is included in this module. "
605,Module 2: Basic Charting,4,https://www.coursera.org/learn/python-plotting?specialization=data-science-python,"In this module, you will delve into basic charting. For this week’s assignment, you will work with real world CSV weather data. You will manipulate the data to display the minimum and maximum temperature for a range of dates and demonstrate that you know how to create a line graph using matplotlib. Additionally, you will demonstrate the procedure of composite charts, by overlaying a scatter plot of record breaking data for a given year."
606,Module 3: Charting Fundamentals,4,https://www.coursera.org/learn/python-plotting?specialization=data-science-python,In this module you will explore charting fundamentals. For this week’s assignment you will work to implement a new visualization technique based on academic research. This assignment is flexible and you can address it using a variety of difficulties - from an easy static image to an interactive chart where users can set ranges of values to be used.
607,Module 4: Applied Visualizations,4,https://www.coursera.org/learn/python-plotting?specialization=data-science-python,"In this module, then everything starts to come together. Your final assignment is entitled “Becoming a Data Scientist.” This assignment requires that you identify at least two publicly accessible datasets from the same region that are consistent across a meaningful dimension. You will state a research question that can be answered using these data sets and then create a visual using matplotlib that addresses your stated research question. You will then be asked to justify how your visual addresses your research question."
608,Module 1: Fundamentals of Machine Learning - Intro to SciKit Learn,4,https://www.coursera.org/learn/python-machine-learning?specialization=data-science-python,"This module introduces basic machine learning concepts, tasks, and workflow using an example classification problem based on the K-nearest neighbors method, and implemented using the scikit-learn library."
609,Module 2: Supervised Machine Learning - Part 1,4,https://www.coursera.org/learn/python-machine-learning?specialization=data-science-python,"This module delves into a wider variety of supervised learning methods for both classification and regression, learning about the connection between model complexity and generalization performance, the importance of proper feature scaling, and how to control model complexity by applying techniques like regularization to avoid overfitting.  In addition to k-nearest neighbors, this week covers linear regression (least-squares, ridge, lasso, and polynomial regression), logistic regression, support vector machines, the use of cross-validation for model evaluation, and decision trees. "
610,Module 3: Evaluation,4,https://www.coursera.org/learn/python-machine-learning?specialization=data-science-python,This module covers evaluation and model selection methods that you can use to help understand and optimize the performance of your machine learning models. 
611,Module 4: Supervised Machine Learning - Part 2,4,https://www.coursera.org/learn/python-machine-learning?specialization=data-science-python,"This module covers more advanced supervised learning methods that include ensembles of trees (random forests, gradient boosted trees), and neural networks (with an optional summary on deep learning).  You will also learn about the critical problem of data leakage in machine learning and how to detect and avoid it."
612,Module 1: Working with Text in Python,4,https://www.coursera.org/learn/python-text-mining?specialization=data-science-python,
613,Module 2: Basic Natural Language Processing,4,https://www.coursera.org/learn/python-text-mining?specialization=data-science-python,
614,Module 3: Classification of Text,4,https://www.coursera.org/learn/python-text-mining?specialization=data-science-python,
615,Module 4: Topic Modeling,4,https://www.coursera.org/learn/python-text-mining?specialization=data-science-python,
616,Analysing returns,4,https://www.coursera.org/learn/introduction-portfolio-construction-python,
617,An Introduction to Portfolio Optimization,4,https://www.coursera.org/learn/introduction-portfolio-construction-python,
618,Beyond Diversification,4,https://www.coursera.org/learn/introduction-portfolio-construction-python,
619,Introduction to Asset-Liability Management,4,https://www.coursera.org/learn/introduction-portfolio-construction-python,
620,Style & Factors,4,https://www.coursera.org/learn/advanced-portfolio-construction-python,
621,Robust estimates for the covariance matrix,4,https://www.coursera.org/learn/advanced-portfolio-construction-python,
622,Robust estimates for expected returns,4,https://www.coursera.org/learn/advanced-portfolio-construction-python,
623,Portfolio Optimization in Practice,4,https://www.coursera.org/learn/advanced-portfolio-construction-python,
624,Introducing the fundamentals of machine learning,5,https://www.coursera.org/learn/python-machine-learning-for-investment-management,
625,Machine learning techniques for robust estimation of factor models,5,https://www.coursera.org/learn/python-machine-learning-for-investment-management,
626,Machine learning techniques for efficient portfolio diversification,5,https://www.coursera.org/learn/python-machine-learning-for-investment-management,
627,Machine learning techniques for regime analysis ,5,https://www.coursera.org/learn/python-machine-learning-for-investment-management,
628,"Identifying recessions, crash regimes and feature selection",5,https://www.coursera.org/learn/python-machine-learning-for-investment-management,
629,Consumption,4,https://www.coursera.org/learn/machine-learning-asset-management-alternative-data,"The consumption module introduces students to the basics of consumption-based alternative data. By aggregating online and offline consumer purchase activity and behavioral datasets including geolocation data (e.g., cell locations, satellite imagery etc.), transaction data (e.g.,  credit card transaction logs and point of sale data), as well as consumer interaction with brands and products on social media, researchers can learn about company performance ahead of official company earning announcements. 
Such information may be extremely useful and can provide investment and risk management advantages. This module reviews the theoretical aspects of various consumption datasets, and provides practical demonstrations of relevant data analytics."
630,Textual Analysis for Financial Applications,4,https://www.coursera.org/learn/machine-learning-asset-management-alternative-data,"Module 2 is an introduction to text mining as well as a demonstration of how to get from data retrieval (web scraping) to financial market insights. Some of the classic text mining methodologies are covered such as vectorization of text (the bag of words approach), stop words for filtering, and term frequency-inverse document frequency (TF-IDF). Students will learn how text can be mathematically represented, and regularized/filtered to reduce noise. Measures of text-similarity will be covered in theoretical and practice sessions. Lab sessions go through examples of web scraping data, regularizing with the described techniques and finally, insights will be derived from the textual data."
631,Processing Corporate Filings,4,https://www.coursera.org/learn/machine-learning-asset-management-alternative-data,"Module 3 is a practical extension of the text mining lessons to 10-K and 13-F, two of the most commonly researched corporate filings. This type of data can be extremely daunting when used by individual analysts due to the sheer size of the documents, but module 3 describes the methodologies for quantitatively analyzing these documents with Python code. Both the 10-K and 13-F documents are worked through, and within the lab sessions it is demonstrated how one can automatically pull this kind of data as well as define metrics around them. We investigate implementations of research in this field around similarity of given companies 10-K statements over time as well as similarity between fund holdings from the 13-F in the lab."
632,Using Media-Derived Data,4,https://www.coursera.org/learn/machine-learning-asset-management-alternative-data,"The final module introduces both sentiment analysis in the context of textual data as well as network analysis in the context of connectivity of firms. Sentiment analysis is an avenue of potentially fruitful information that when done correctly can display what a general population might believe about a company (through for example social media) or even whether the company itself is positive or negative on future outlook (through analysis of tone in corporate filings). Network analysis, as shown in the research of course instructors and his colleagues, can be used to accurately capture how a financial network is oriented and what companies might perform well because of other firm’s mentioning them as a threat. The lab session of this module extends the corporate filings analysis to examine sentiment while also introducing a set of tweets which are then transformed into a network representation."
633,"Introduction to Trading, Machine Learning and GCP",4,https://www.coursera.org/learn/introduction-trading-machine-learning-gcp,"In this module you will be introduced to the fundamentals of trading. You will also be introduced to machine learning. Machine Learning is both an art that involves knowledge of the right mix of parameters that yields accurate, generalized models and a science that involves knowledge of the theory to solve specific types of problems."
634,Supervised Learning and Forecasting,4,https://www.coursera.org/learn/introduction-trading-machine-learning-gcp,In this module you will be introduced to supervised machine learning and some relevant algorithms commonly applied to trading problems. You will get some hands-on experience building a regression model using BigQuery Machine Learning
635,Time Series and ARIMA Modeling,4,https://www.coursera.org/learn/introduction-trading-machine-learning-gcp,In this module you will learn about ARIMA modeling and how it is applied to time series data. You will get hands-on experience building an ARIMA model for a financial dataset.
636,Introduction to Neural Networks and Deep Learning,4,https://www.coursera.org/learn/introduction-trading-machine-learning-gcp,"In this module you'll learn about neural networks and how they relate to deep learning. You'll also learn how to gauge model generalization using regularization, and cross-validation. Also, you'll be introduced to Google Cloud Platform (GCP). Specifically, you'll be shown how to leverage GCP for implementing trading techniques. "
637,Introduction to Quantitative Trading and TensorFlow,3,https://www.coursera.org/learn/machine-learning-trading-finance,"In this module we discuss the key components that are common to every trading strategy, no matter how complex. This foundation will help guide you as you develop more advanced strategies using machine learning techniques."
638,Build a Pair Trading Strategy Prediction Model,3,https://www.coursera.org/learn/machine-learning-trading-finance,"In this module, we introduce pairs trading.  We will discuss what pairs trading is, and how you can make money doing it.  We will discuss what you need to know about the members to form a suitable pair. "
639,Build a Momentum-based Trading System,3,https://www.coursera.org/learn/machine-learning-trading-finance,"Momentum trading is a strategy in which traders buy or sell assets according to the strength of recent price trends. Price momentum is similar to momentum in physics, where mass multiplied by velocity determines the persistence with which an object will follow its current path (like a heavy train on a track). In financial markets, however, momentum is determined by other factors like trading volume and rate of price changes. Momentum traders bet that an asset price that is moving strongly in a given direction will continue to move in that direction until the trend loses strength or reverses. This module teaches you all about momentum trading. "
640,Introduction to Course and Reinforcement Learning,3,https://www.coursera.org/learn/trading-strategies-reinforcement-learning,"In this module, reinforcement learning is introduced at a high level. The history and evolution of reinforcement learning is presented, including key concepts like value and policy iteration. Also, the benefits and examples of using reinforcement learning in trading strategies is described.  We also introduce LSTM and AutoML as additional tools in your toolkit to use in implementing trading strategies."
641,Neural Network Based Reinforcement Learning,3,https://www.coursera.org/learn/trading-strategies-reinforcement-learning,"In the previous module, reinforcement learning was discussed before neural networks were introduced. In this module, we look at how reinforcement learning has been integrated with neural networks. We also look at LSTMs and how they can be applied to time series data. "
642,Portfolio Optimization,3,https://www.coursera.org/learn/trading-strategies-reinforcement-learning,"In this module we discuss the practical steps required to create a reinforcement learning trading system. Also, we introduce AutoML, a powerful service on Google Cloud Platform for training machine learning models with minimal coding. "
643,Module 1: Introduction to Models ,4,https://www.coursera.org/learn/wharton-quantitative-modeling,"In this module, you will learn how to define a model, and how models are commonly used. You’ll examine the central steps in the modeling process, the four key mathematical functions used in models, and the essential vocabulary used to describe models. By the end of this module, you’ll be able to identify the four most common types of models, and how and when they should be used. You’ll also be able to define and correctly use the key terms of modeling, giving you not only a foundation for further study, but also the ability to ask questions and participate in conversations about quantitative models."
644,Module 2: Linear Models and Optimization,4,https://www.coursera.org/learn/wharton-quantitative-modeling,"This module introduces linear models, the building block for almost all modeling. Through close examination of the common uses together with examples of linear models, you’ll learn how to apply linear models, including cost functions and production functions to your business. The module also includes a presentation of growth and decay processes in discrete time, growth and decay in continuous time, together with their associated present and future value calculations. Classical optimization techniques are discussed. By the end of this module, you’ll be able to identify and understand the key structure of linear models, and suggest when and how to use them to improve outcomes for your business. You’ll also be able to perform present value calculations that are foundational to valuation metrics. In addition, you will understand how you can leverage models for your business, through the use of optimization to really fine tune and optimize your business functions."
645,Module 3: Probabilistic Models,4,https://www.coursera.org/learn/wharton-quantitative-modeling,"This module explains probabilistic models, which are ways of capturing risk in process. You’ll need to use probabilistic models when you don’t know all of your inputs. You’ll examine how probabilistic models incorporate uncertainty, and how that uncertainty continues through to the outputs of the model. You’ll also discover how propagating uncertainty allows you to determine a range of values for forecasting. You’ll learn the most-widely used models for risk, including regression models, tree-based models, Monte Carlo simulations, and Markov chains, as well as the building blocks of these probabilistic models, such as random variables, probability distributions, Bernoulli random variables, binomial random variables, the empirical rule, and perhaps the most important of all of the statistical distributions, the normal distribution, characterized by mean and standard deviation. By the end of this module, you’ll be able to define a probabilistic model, identify and understand the most commonly used probabilistic models, know the components of those models, and determine the most useful probabilistic models for capturing and exploring risk in your own business."
646,Module 4: Regression Models,4,https://www.coursera.org/learn/wharton-quantitative-modeling,"This module explores regression models, which allow you to start with data and discover an underlying process. Regression models are the key tools in predictive analytics, and are also used when you have to incorporate uncertainty explicitly in the underlying data.  You’ll learn more about what regression models are, what they can and cannot do, and the questions regression models can answer. You’ll examine correlation and linear association, methodology to fit the best line to the data, interpretation of regression coefficients, multiple regression, and logistic regression. You’ll also see how logistic regression will allow you to estimate probabilities of success. By the end of this module, you’ll be able to identify regression models and their key components, understand when they are used, and be able to interpret them so that you can discuss your model and convince others that your model makes sense, with the ultimate goal of implementation."
647,Spreadsheets: A Tool for Thinking with Numbers,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models,"This module was designed to introduce you to the history of spreadsheets, their basic capabilities, and how they can be used to create models. You'll learn the different types of data used in spreadsheets, spreadsheet notations for mathematical operations, common built-in formulas and functions, conditional expressions, relative and absolute references, and how to identify and correct circular references. By the end of this module, you'll understand the context of spreadsheets, be able to navigate a spreadsheet, use built-in formulas and functions in spreadsheets, create your own simple formulas, and identify and correct common errors so you can put spreadsheets to work for you."
648,From Spreadsheet to Model,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models,"In this module, you'll move from spreadsheet to model, so you can begin to create your own models that reflect real-world events. You'll learn how to organize and lay out model elements, as well as the types of objective functions and their use. You'll also learn what-if analysis and scenarios, sensitivity analysis, and other classic models. By the end of this module, you'll be able to design a spreadsheet reflecting assumptions, decision variables, and outcomes, create a basic cashflow model, evaluate a small business opportunity, conduct what-if analysis, identify key variables using sensitivity analysis, and linear programming models and deterministic models."
649,Addressing Uncertainty and Probability in Models,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models,"This module was designed to introduce you to how you can use spreadsheets to address uncertainty and probability. You'll learn about random variables, probability distributions, power, exponential, and log functions in model formulas, models for calculating probability trees and decision trees, how to use regression tools to make predictions, as well as multiple regression. By the end of this module, you'll be able to measure correlations between variables using spreadsheet statistical functions, understand the results of functions that calculate correlations, use regression tools to make predictions, and improve forecasts with multiple regression."
650, Simulation and Optimization,4,https://www.coursera.org/learn/wharton-introduction-spreadsheets-models,"In this module, you'll learn to use spreadsheets to implement Monte Carlo simulations as well as linear programs for optimization. You'll examine the purpose of Monte Carlo simulations, how to implement Monte Carlo simulations in spreadsheets, the types of problems you can address with linear programs and how to implement those linear programs in spreadsheets. By the end of this module, you'll be able to model uncertainty and risk in spreadsheets, and use Excel's solver to optimize resources to reach a desired outcome.  You'll also be able to identify the similarities and differences between Excel and Sheets, and be prepared for the next course in the Business and Financial Modeling Specialization."
651,"Module 1: Introduction, Balance Sheet and Income Statement",4,https://www.coursera.org/learn/finance-healthcare-managers,"This module was designed to give you a foundational overview of financial reporting and income statements. You’ll identify and analyze balance sheet equations and its key components such as assets, liabilities, and shareholders’ equity. Through examining a sample real-world financial statement, you’ll learn how to calculate income, revenue, and expenses transactions, and see how the income statement is linked to changes in the balance sheet. By the end of this module, you’ll have a better understanding of the key components in financial reporting and learn how to craft an accurate income statement for your organization."
652,Module 2: Cash Flow Statement,4,https://www.coursera.org/learn/finance-healthcare-managers,"In this module, you’ll examine cash flow statements further, and take a closer look at methods of creating cash flow statements. Using your understanding of financial reporting, you’ll be able to classify different business activities into separate categories such as operating, investing, and financing activities. Through analyzing the real-world financial statement from the previous module, you’ll learn about both direct and indirect methods of creating cash flow statements. By the end of this module, you’ll be able to differentiate between various business activities and effectively utilize both direct and indirect methods of creating cash flow statements for your organization."
653,Module 3: Financial Statement Analysis,4,https://www.coursera.org/learn/finance-healthcare-managers,"In this module, you’ll examine a systematic approach to ratio analysis and other common tools of financial statement analysis. You’ll develop an understanding of ratios and liquidity measures so you can accurately assess risk within your organization’s financial activities. You’ll discover different approaches to profitability measures such as Earnings Per Share (EPS), Return on Equity (ROE), and the Dupont Analysis. You will be able to match Return on Assets (ROA) to various types of companies and gain a better understanding of the drivers of ROA. Then, you’ll explore the concepts of sales revenue and different qualities of earnings. By the end of this module, you’ll understand the theoretical basis behind ratio analysis, and be able to employ different ratio analyses and accurately calculate profitability measures for your organization."
654,Module 4: Linking Non-Financial Metrics to Financial Performance,4,https://www.coursera.org/learn/finance-healthcare-managers,"In this module, you’ll discover how to determine which non-financial performance measures predict financial results through asking these fundamental questions: Of the hundreds of non-financial measures, which are the key drivers of financial success? How do you rank or weight non-financial measures which don’t share a common denominator? And what performance targets are desirable? You’ll examine comprehensive examples of how companies have used accounting data to show how investments in non-financial dimensions pay off in the future and important organizational issues that commonly arise using these models. By the end of this module, you’ll know how predictive analytics can be used to determine what you should be measuring, how to weight different performance measures when trying to analyze potential financial results, how to make trade-offs between short-term and long-term objectives, and how to set performance targets for optimal financial performance."
655,Week 1: Time Value of Money,4,https://www.coursera.org/learn/wharton-finance,"Welcome to Introduction to Corporate Finance! This first module will introduce you to one of the most important foundational concepts in Finance, the time value of money. Before diving into the Video lectures, I encourage you to take a look at the brief pre-reading for the course. Specifically, have a look at “Big Picture Course Motivation,” for additional motivation and context for the course, “Time Value of Money Overview,” for a motivation and context for our first topic, and “Quiz Problem Answer Input.” This last note is particularly important to avoid confusion with the problem sets. Then, go to the Video Lectures and start learning Finance!  "
656,Week 2: Interest Rates,4,https://www.coursera.org/learn/wharton-finance,"In this module, we wrap up the Time Value of Money topic with a discussion of inflation before moving on to our second topic, Interest Rates, and introducing our third topic, Discounted Cash Flow Analysis. By the end of this module, you should feel comfortable with discounting and compounding arbitrary cash flow streams in order to value different claims and make better financial decisions. "
657,Week 3: Discounted Cash Flow Analysis,4,https://www.coursera.org/learn/wharton-finance,"This module continues our discussion of discounted cash flow analysis by way of a capital budgeting case. (You might want to download or view the Excel file, “Tablet Case Spreadsheet.xlsx,” that I use in the lectures, but it is not necessary for understanding the material.) By the end of this module, you should feel comfortable valuing claims and making financing decisions in which the timing of the cash flows and compounding of interest is arbitrary (e.g., annual, semi-quarterly, monthly, etc.). Having worked through the problems, you should also be more comfortable with real world financial decision making related to retirement savings, home financing and refinancing, auto leases, and other scenarios. "
658,Week 4: Return on Investment,4,https://www.coursera.org/learn/wharton-finance,"This module closes out our discussion of discounted cash flow analysis and caps off the course with a discussion of return on investment. By the end of this module, you should feel comfortable with the notion of free cash flow and the ability to apply a set of forecast drivers to project free cash flows into the future. These are some of the elements of a basic financial model, which we will use to come to a decision about the tablet project and to analyze the assumptions behind our valuation. "
659,About This Specialization and Course ,5,https://www.coursera.org/learn/analytics-business-metrics?specialization=excel-mysql,"This Coursera Specialization: Excel to MySQL: Analytic Techniques for Business, is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more, watch the video and review the specialization overview document we provided. In the first course of the specialization: Business Metrics for Data-Driven Companies, you will be able to learn best practices for using data analytics to make any company more competitive and more profitable; learn to recognize the most critical business metrics and distinguish those from mere data; understand the vital but different roles business analysts, business data analysts, and data scientists each play in various types of companies; and know exactly the skills required to be hired for, and succeed at, these high-demand jobs. Finally, using a 20-item checklist for evaluating a business, you'll be able to score any company on how effectively it is embracing big data culture. Digital companies like Amazon, Uber and Airbnb are transforming entire industries through their creative use of big data. You’ll understand why these companies are so disruptive, and how they use data-analytics techniques to out-compete traditional companies.To get started, please begin with the video 'About This Specialization.'I hope you enjoy this week's materials!"
660,Introducing Business Metrics,5,https://www.coursera.org/learn/analytics-business-metrics?specialization=excel-mysql,"Welcome! This week we will explore business metrics - the critical numbers that help companies figure out how to survive and thrive. Inside every pile of data is a vital metric trying to get out! By the end of this week, you will be able to: distinguish business metrics from mere business data; identify critical business metrics such as cash flow, profitability, and online retail marketing metrics;  distinguish revenue, profitability and risk metrics; and distinguish traditional from dynamic metrics. Included in this week’s course materials is a Cash Flow and P&L statement for Egger’s Roast Coffee, as a supplemental document, so be sure to review it carefully and refer to the glossary for key information. "
661,Working in the Business Data Analytics Marketplace,5,https://www.coursera.org/learn/analytics-business-metrics?specialization=excel-mysql,"Welcome!  This week, we will meet some great people - all former students of mine - now working at super-interesting and exciting jobs as business analysts, business data analysts, or data scientists. We’ll explore what they do, how their role relates to big data, and the skills they needed to get hired! Our hope is this information will give you a better understanding of the type of data-related job you might apply for once you've completed this specialization, and a sense of the type of company you would find most appealing to work for. By the end of this week, you will be able to:  differentiate among different job roles within a company that work with data; identify how each role works with data; and describe the skills required to perform each job role. You will differentiate how different types of companies relate to big data culture, and rank any company according to a 20-item checklist. You will also learn to differentiate how different types of companies relate to big data culture. Included in this week’s materials is a 20-item checklist to rank companies. This week also includes in-video polls so you can see how others are ranking their businesses."
662,Going Deeper into Business Metrics ,5,https://www.coursera.org/learn/analytics-business-metrics?specialization=excel-mysql,"Welcome! This week we’re going to go deeper into the critically-important metrics for web marketing - metrics every type of business needs to understand in order to survive. We’ll dive into the 'vertical' market of financial services - where digital companies are threatening to take away the market from traditional 'brick-and- mortar' companies.By the end of this week, you will be able to: Identify critical business metrics for all companies engaged in web-based marketing; and identify critical business metrics for financial services companies. You’ll find additional website links that expand some of the course materials covered in this week’s video lectures. "
663,Applying Business Metrics to a Business Case Study,5,https://www.coursera.org/learn/analytics-business-metrics?specialization=excel-mysql,"This week contains the final course assignment, a peer assessment in which you will identify business metrics of interest in a case example, describe those metrics, and propose a business process change that could be supported by the metric chosen."
664,About This Course,7,https://www.coursera.org/learn/analytics-excel?specialization=excel-mysql,"This course will prepare you to design and implement realistic predictive models based on data. In the Final Project (module 6) you will assume the role of a business data analyst for a bank, and develop two different predictive models to determine which applicants for credit cards should be accepted and which rejected.  Your first model will focus on minimizing default risk, and your second on maximizing bank profits. The two models should demonstrate to you in a practical, hands-on way the idea that your choice of business metric drives your choice of an optimal model.The second big idea this course seeks to demonstrate is that your data-analysis results cannot and should not aim to eliminate all uncertainty.  Your role as a data-analyst is to reduce uncertainty for decision-makers by a financially valuable increment, while quantifying how much uncertainty remains. You will learn to calculate and apply to real-world examples the most important uncertainty measures used in business, including classification error rates, entropy of information, and confidence intervals for linear regression. All the data you need is provided within the course, and all assignments are designed to be done in MS Excel. The course will give you enough practice with Excel to become fluent in its most commonly used business functions, and you’ll be ready to learn any other Excel functionality you might need in future (module 1). The course does not cover Visual Basic or Pivot Tables and you will not need them to complete the assignments. All advanced concepts are demonstrated in individual Excel spreadsheet templates that you can use to answer relevant questions. You will emerge with substantial vocabulary and practical knowledge of how to apply business data analysis methods based on binary classification (module 2), information theory and entropy measures (module 3), and linear regression (module 4 and 5), all using no software tools more  complex than Excel. "
665,Excel Essentials for Beginners,7,https://www.coursera.org/learn/analytics-excel?specialization=excel-mysql,"In this module, will explore the essential Excel skills to address typical business situations you may encounter in the future. The Excel vocabulary and functions taught throughout this module make it possible for you to understand the additional explanatory Excel spreadsheets that accompany later videos in this course. "
666,Binary Classification,7,https://www.coursera.org/learn/analytics-excel?specialization=excel-mysql,"Separating collections into two categories, such as “buy this stock, don’t but that stock” or “target this customer with a special offer, but not that one” is the ultimate goal of most business data-analysis projects. There is a specialized vocabulary of measures for comparing and optimizing the performance of the algorithms used to classify collections into two groups. You will learn how and why to apply these different metrics, including how to calculate the all-important AUC: the area under the Receiver Operating Characteristic (ROC) Curve. "
667,Information Measures,7,https://www.coursera.org/learn/analytics-excel?specialization=excel-mysql,"In this module, you will learn how to calculate and apply the vitally useful uncertainty metric known as “entropy.” In contrast to the more familiar “probability” that represents the uncertainty that a single outcome will occur, “entropy” quantifies the aggregate uncertainty of all possible outcomes.The entropy measure provides the framework for accountability in data-analytic work. Entropy gives you the power to quantify the uncertainty of future outcomes relevant to your business twice: using the best-available estimates before you begin a project, and then again after you have built a predictive model.
The difference between the two measures is the Information Gain contributed by your work."
668,Linear Regression,7,https://www.coursera.org/learn/analytics-excel?specialization=excel-mysql,"The Linear Correlation measure is a much richer metric for evaluating associations than is commonly realized. You can use it to quantify how much a linear model reduces uncertainty.  When used to forecast future outcomes, it can be converted into a “point estimate” plus a “confidence interval,” or converted into an information gain measure. You will develop a fluent knowledge of these concepts and the many valuable uses to which linear regression is put in business data analysis. This module also teaches how to use the Central Limit Theorem (CLT) to solve practical problems. The two topics are closely related because regression and the CLT both make use of a special family of probability distributions called “Gaussians.” You will learn everything you need to know to work with Gaussians in these and other contexts. "
669,Additional Skills for Model Building,7,https://www.coursera.org/learn/analytics-excel?specialization=excel-mysql,"This module gives you additional valuable concepts and skills related to building high-quality models. As you know, a “model” is a description of a process applied to available data (inputs) that produces an estimate of a future and as yet unknown outcome as output. 
Very often, models for outputs take the form of a probability distribution. This module covers how to estimate probability distributions from data (a “probability histogram”), and how to describe and generate the most useful probability distributions used by data scientists. It also covers in detail how to develop a binary classification model with parameters optimized to maximize the AUC, and how to apply linear regression models when your input consists of multiple types of data for each event. 
The module concludes with an explanation of “over-fitting” which is the main reason that apparently good predictive models often fail in real life business settings. We conclude with some tips for how you can avoid over-fitting in you own predictive model for the final project – and in real life."
670,Final Course Project,7,https://www.coursera.org/learn/analytics-excel?specialization=excel-mysql,"The final course project is a comprehensive assessment covering all of the course material, and consists of four quizzes and a peer review assignment.  For quiz one and quiz two, there are learning points that explain components of the quiz.  These learning points will unlock only after you complete the quiz with a passing grade. Before you start, please read through the final project instructions.  From past student experience, the final project which includes all the quizzes and peer assessment, takes anywhere from 10-12 hours."
671,About this Specialization and Course,6,https://www.coursera.org/learn/analytics-tableau?specialization=excel-mysql,"<p>The Coursera Specialization: <a href='https://www.coursera.org/specializations/excel-mysql' target='_blank'>Excel to MySQL: Analytic Techniques for Business</a>, is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to a real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more, watch the video and review the specialization overview document we provided.<p>In the third course of the specialization: <b>Data Visualization and Communication with Tableau</b>, you will learn how to communicate business-relevant implications of data analyses.  <p>Specifically, you will:<ul><li>craft the right questions to ensure your analysis projects succeed;</li> <li>leverage questions to design logical and structured analysis plans;</li> <li>create the most important graphs used in business analysis and transform data in Tableau;</li><li>design business dashboards with Tableau;</li> <li>tell stories with data;</li><li>design effective slide presentations to showcase your data story; and </li><li>deliver compelling business presentations.</li></ul> <p>By the end of this course, you will know how to structure your data analysis projects to ensure the fruits of your hard labor yield results for your stakeholders.  You will also know how to streamline your analyses and highlight their implications efficiently using visualizations in Tableau, the most popular visualization program in the business world.  Using other Tableau features, you will be able to make effective visualizations that harness the human brain’s innate perceptual and cognitive tendencies to convey conclusions directly and clearly.  Finally, you will be practiced in designing and persuasively presenting business “data stories” that use these visualizations, capitalizing on business-tested methods and design principles by completing a final peer assessed project recommending a business process change. <P>To get started, please begin with the video 'About This Specialization.'<P>I hope you enjoy this week's materials!</P>"
672,"Asking The ""Right Questions""",6,https://www.coursera.org/learn/analytics-tableau?specialization=excel-mysql,"Welcome! This week, you will learn how data analysts ask the right questions to ensure project success. By the end of this week, you will be able to: <p><ul><li>Craft the right questions to ensure your analysis projects succeed</li><li> Leverage questions to design logical and structured analysis plans</ul></li> <p> Remember to refer back to the Additional Resources reading: Identifying and Eliciting Information from Stakeholders). In addition, you will complete a  graded quiz. <p> As always, if you have any questions, post them to the <b>Discussions.</b> <p>To get started, please begin with the video “Tips for Becoming a Data Analyst.” <p>I hope you enjoy this week's materials!</p>"
673,Data Visualization with Tableau,6,https://www.coursera.org/learn/analytics-tableau?specialization=excel-mysql,"Welcome to week 2! This week you'll install Tableau Desktop to learn how visualizing data helps you figure out what your data mean efficiently, and in the process of doing so, helps you narrow in on what factors you should take into consideration in your statistical models or predictive algorithms.  Over the next two weeks, we’re going to learn how to use Tableau to implement this type of visualization and to help you find, and communicate, answers to business questions, as well as work with the Tableau functions that all data analysts should be familiar with.  You will learn to install Tableau Desktop and learn to use the program by working with two data sets. In addition, through a series of practice exercises, you will use a data set to do example analyses and to answer specific sample questions about salaries for certain data-related jobs across the United State. Then for graded exercises, you will use a different data set to work out analyses and questions that will require you to directly apply the Tableau skills you have acquired through practice. <p>By the end of this week, you will be able to: <ul><li>Create the most important graphs used in business analysis and transform data in Tableau </li></ul><p>Once you have watched the ""Why Tableau"" video, review the ""Written Instructions to install Tableau Desktop"" and install the software. Remember to refer back to the Salary Data Set and to the Dognition Data Set resources posted on the course site this week. You will also complete a graded quiz at the end of the week. <p>As always, if you have any questions, post them to the <b>Discussions</b>.<p>To get started, please begin with the video “Use Data Visualization to Drive Your Analysis"" and then review the ""Written Instructions to install Tableau Desktop.<p>I hope you enjoy this week's materials! "
674,Dynamic Data Manipulation and Presentation in Tableau,6,https://www.coursera.org/learn/analytics-tableau?specialization=excel-mysql,"Welcome to week 3! This week you'll continue learning how to use Tableau to answer data analysis questions. You will learn how to use Tableau to both find, and eventually communicate answers to business questions. You'll learn about the process of elicitation, and learn how to ensure your data story is not undermined by overgeneralization or bias and how to format your data charts to begin creating a compelling data story.  By the end of this week, you will be able to: <ul><li>Write calculations and equations in Tableau</li> <li>Publish online business dashboards with Tableau.</li></ul> <p>Remember to refer  to the additional resources for this week: “Examples of Tableau Dashboards and Stories” and ""Using Tableau Dashboards When You Don't Have To."" <p>You will also complete a graded quiz. <p>As always, if you have any questions, post them to the Discussions. <p>To get started, please begin with the video “Customizing and Sharing New Data in Tableau.” <p>I hope you enjoy this week's materials!"
675,"Your Communication Toolbox: Visualizations, Logic, and Stories",6,https://www.coursera.org/learn/analytics-tableau?specialization=excel-mysql,"Welcome to week 4! This week you will become a master at getting people to agree with your data-driven business recommendations as you learn to deliver a compelling business presentation.  You’ll learn about the insight from the intersection of visualization science and decision science, and what this means for you as a  data analyst, who seeks to design a compelling and effective business presentations. If you intend to affect people’s decisions, you need to influence where they look. This week we will review  a set of tools and concepts you can use to optimize your visualizations and your presentation style. You will soon  be a master at getting people to agree with your data-driven business recommendations! <p>By the end of this week, you will be able to:<p><ul><li>Tell stories with data</li> <li>Design effective slide presentations to showcase your data story, and</li> <li> Deliver compelling business presentations</ul></li><p> Remember to refer back to the Study Guide: Designing and Delivering Effective Presentations. You will also complete a graded quiz. <p>As always, if you have any questions, post them to the <b>Discussions</b>.<p> To get started, please begin with the video “Using Visualization to Influence Business Decisions.”<p> I hope you enjoy this week's materials!"
676,Final Project,6,https://www.coursera.org/learn/analytics-tableau?specialization=excel-mysql,"Welcome to week 5! This week you will complete your final project. This assignment requires you to submit a recording of yourself giving a 4-5 minute presentation in which you present a data-driven business process change proposal to Dognition company management about how to increase the numbers of tests users complete. Students will give a short, peer-reviewed business presentation that uses a specified chart in Tableau. The final project will assess your mastery of the following: <p> <ul><li>Demonstrated understanding the Tableau functions discussed in this course</li><li>Adapting visualizations to make them maximally communicative</li><li>Storyboarding skills</li> <li>Translating your story into a presentation ready for the boardroom</li><li>Effective presentation delivery</li><li>Evaluating business presentations</ul></li><p> Remember to refer to the Background Information for Peer Review Assignment  on the course web site before you begin. This final course project is a comprehensive assessment covering all of the course material and will take approximately 6-8 hours to complete.<p>As always, if you have any questions, post them to the Discussions. Thank you for your contributions to this  final project!<p> "
677,About this Specialization and Course ,6,https://www.coursera.org/learn/analytics-mysql?specialization=excel-mysql,"The Coursera Specialization, ""Managing Big Data with MySQL"" is about how 'Big Data' interacts with business, and how to use data analytics to create value for businesses. This specialization consists of four courses and a final Capstone Project, where you will apply your skills to a real-world business process. You will learn to perform sophisticated data-analysis functions using powerful software tools such as Microsoft Excel, Tableau, and MySQL. To learn more about the specialization, please review the first lesson below, ""Specialization Introduction: Excel to MySQL: Analytic Techniques for Business.""  In this fourth course of this specialization, ""Managing Big Data with MySQL” you will learn how relational databases  work and how they are used in business analysis. Specifically, you will: (1) Describe the structure of relational databases; (2) Interpret and create entity-relationship diagrams and relational schemas that describe the contents of specific databases; (3) Write queries that retrieve and sort data that meet specific criteria, and retrieve such data from real MySQL and Teradata business databases that contain over 1 million rows of data; (4) Execute practices that limit the impact of your queries on other coworkers; (5) Summarize rows of data using aggregate functions, and segment aggregations according to specified variables; (6) Combine and manipulate data from multiple tables across a database; (7) Retrieve records and compute calculations that are dependent on dynamic data features; (8) Translate data analysis questions into SQL queries that accommodate the types of anomalies found in real data sets. By the end of this course, you will have a clear understanding of how relational databases work and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of information with the hope that data will yield novel insights into how to improve businesses.  Analysts that understand how to access this data – this means you! – will have a strong competitive advantage in this data-smitten business world.  To get started with this course, you can begin with, ""Introduction to Managing Big Data with MySQL.""  Please take some time to not only watch the videos, but also read through the course overview as there is extremely important course information in the overview.  "
678,Understanding Relational Databases,6,https://www.coursera.org/learn/analytics-mysql?specialization=excel-mysql,"Welcome to week 1! This week  you will learn how relational databases are organized, and practice making and interpreting Entity Relationship (ER) diagrams and relational schemas that describe the structure of data stored in a database. <p>By the end of the week, you will be able to:<ul><li>Describe the fundamental principles of relational database design <li>Interpret Entity Relationship (ER) diagrams and Entity Relationship (ER) schemas, and</li><li>Create your own ER diagrams and relational schemas using a software tool called ERDPlus that you will use to aid your query-writing later in the course.</li></ul><p>This week’s exercises are donated from a well-known Database Systems textbook, and will help you deepen and strengthen your understanding of how relational databases are organized.  This deeper understanding will help you navigate complicated business databases, and allow you to write more efficient queries.  At the conclusion of the week, you will test your understanding of database design principles by completing the Week 1 graded quiz.</p> <p>To get started, please begin with the video “Problems with Having a Lot of Data Used by a Lot of People.” <p>As always, if you have any questions, post them to the Discussions. <p>I hope you enjoy this week's materials!"
679, Queries to Extract Data from Single Tables ,6,https://www.coursera.org/learn/analytics-mysql?specialization=excel-mysql,"Welcome to week 2! This week, you will start interacting with business databases. You will write SQL queries that query data from two real companies. One data set, donated from a local start-up in Durham, North Carolina called Dognition, is a MySQL database containing tables of over 1 million rows. The other data set, donated from a national US department store chain called Dillard’s, is a Teradata database containing tables with over a hundred million rows. By the end of the week, you will be able to:1.  Use two different database user interfaces2.  Write queries to verify and describe all the contents of the Dognition MySQL database and the Dillard’s Teradata database3.  Retrieve data that meet specific criteria in a socially-responsible using SELECT, FROM, WHERE, LIMIT, and TOP clauses, and4.  Format the data you retrieve using aliases, DISTINCT clauses, and ORDER BY clauses.Make sure to watch the instructional videos about how to use the database interfaces we have established for this course, and complete both the MySQL and the Teradata exercises. At the end of the week, you will test your understanding of the SQL syntax introduced this week by completing the Week 2 graded quiz.To get started, please begin with the video “Introduction to Week 2.”  As always, if you have any questions, post them to the Discussions. Enjoy this week's materials!"
680,Queries to Summarize Groups of Data from Multiple Tables ,6,https://www.coursera.org/learn/analytics-mysql?specialization=excel-mysql,"<p>Welcome to week 3! This week, we are going to learn the SQL syntax that allows you to segment your data into separate categories and segment.  We are also going to learn how to combine data stored in separate tables.</p><p>By the end of the week, you will be able to:</p><ul><li>Summarize values across entire columns, and break those summaries up according to specific variables or values in others columns using GROUP BY and HAVING clauses</li><li>Combine information from multiple tables using inner and outer joins</li><li>Use strategies to manage joins between tables with duplicate rows, many-to-many relationships, and atypical configurations</li><li>Practice one of the slightly more challenging use cases of aggregation functions, and</li><li>Work with the Dognition database to learn more about how MySQL handles mismatched aggregation levels.</li></ul><p>Make sure to watch the videos about joins, and complete both the MySQL and the Teradata exercises. At the end of the week, you will test your understanding of the SQL syntax introduced this week by completing the Week 3 graded quiz.</p><p>We strongly encourage you to use the course Discussions to help each other with questions. </p> <p>To get started, please begin with the video 'Welcome to Week 3.’</p><p>I hope you enjoy this week’s materials!</p>"
681, Queries to Address More Detailed Business Questions,6,https://www.coursera.org/learn/analytics-mysql?specialization=excel-mysql,"<p>Welcome to week 4, the final week of Managing Big Data with MySQL!  This week you will practice integrating the SQL syntax you’ve learn so far into queries that address analysis questions typical of those you will complete as a business data analyst.</p>  <p>By the end of the week, you will be able to:</p><ul><li>Design and execute subqueries</li><li>Introduce logical conditions into your queries using IF and CASE statements</li><li>Implement analyses that accommodate missing data or data mistakes, and</li><li>Write complex queries that incorporate many tables and clauses.</li></ul><p>By the end of this week you will feel confident claiming that you know how to write SQL queries to create business value. Due to the extensive nature of the queries we will practice this week, we have put the graded quiz that tests your understanding of the SQL strategies you will practice in its own week rather than including it in this week’s materials. </p> <p>Make sure to complete both the MySQL exercises and the Teradata exercises, and we strongly encourage you to use the course Discussions to help each other with questions.  </p><p>To get started, please begin with the video 'Welcome to Week 4.’</p><p>I hope you enjoy this week’s materials!</p>"
682,Strengthen and Test Your Understanding ,6,https://www.coursera.org/learn/analytics-mysql?specialization=excel-mysql,"This week contains the final ungraded Teradata exercises, and the final graded quiz for the course. The exercises are intended to hone and build your understanding of the last important concepts in the course, and lead directly to the quiz so be sure to do both!"
683,Overview,4,https://www.coursera.org/learn/introduction-genomics?specialization=genomic-data-science,"In this Module, you can expect to study topics of ""Just enough molecular biology"", ""The genome"", ""Writing a DNA sequence"", ""Central dogma"", ""Transcription"", ""Translation"", and ""DNA structure and modifications""."
684,Measurement Technology,4,https://www.coursera.org/learn/introduction-genomics?specialization=genomic-data-science,"In this module, you'll learn about polymerase chain reaction, next generation sequencing, and applications of sequencing."
685,Computing Technology,4,https://www.coursera.org/learn/introduction-genomics?specialization=genomic-data-science,"The lectures for this module cover a few basic topics in computing technology. We'll go over the foundations of computer science, algorithms, memory and data structures, efficiency, software engineering, and computational biology software."
686,Data Science Technology,4,https://www.coursera.org/learn/introduction-genomics?specialization=genomic-data-science,"In this module on Data Science Technology, we'll be covering quite a lot of information about how to handle the data produced during the sequencing process. We'll cover reproducibility, analysis, statistics, question types, the central dogma of inference, analysis code, testing, prediction, variation, experimental design, confounding, power, sample size, correlation, causation, and degrees of freedom."
687,Introduction,4,https://www.coursera.org/learn/galaxy-project?specialization=genomic-data-science,"This week, we will present some of the research challenges that motivated the development of the Galaxy framework. We will then introduce Galaxy, describe what the Galaxy framework is, and look at different ways you can use it."
688,Galaxy 101,4,https://www.coursera.org/learn/galaxy-project?specialization=genomic-data-science,In this module and the following modules we will start to use Galaxy to perform different types of analysis. 
689,Working with sequence data,4,https://www.coursera.org/learn/galaxy-project?specialization=genomic-data-science,In this module we will be studying sequence data quality control as well as ChIP-Sequence Analysis with MACS.
690,RNA-seq & Running your own Galaxy,4,https://www.coursera.org/learn/galaxy-project?specialization=genomic-data-science,"In these final modules, we'll take a look at working with sequence data and RNA-seq and at installing and running your own Galaxy."
691,Week One,4,https://www.coursera.org/learn/python-genomics?specialization=genomic-data-science,This week we will have an overview of Python and take the first steps towards programming.
692,Week Two,4,https://www.coursera.org/learn/python-genomics?specialization=genomic-data-science,"In this module, we'll be taking a look at Data Structures and Ifs and Loops."
693,Week Three,4,https://www.coursera.org/learn/python-genomics?specialization=genomic-data-science,"In this module, we have a long three-part lecture on Functions as well as a 10-minute look at Modules and Packages."
694,Week Four,4,https://www.coursera.org/learn/python-genomics?specialization=genomic-data-science,"In this module, we have another long three-part lecture, this time about Communicating with the Outside, as well as a final lecture about Biopython."
695,"DNA sequencing, strings and matching",4,https://www.coursera.org/learn/dna-sequencing?specialization=genomic-data-science,"This module we begin our exploration of algorithms for analyzing DNA sequencing data. We'll discuss DNA sequencing technology, its past and present, and how it works."
696,"Preprocessing, indexing and approximate matching",4,https://www.coursera.org/learn/dna-sequencing?specialization=genomic-data-science,"In this module, we learn useful and flexible new algorithms for solving the exact and approximate matching problems.  We'll start by learning Boyer-Moore, a fast and very widely used algorithm for exact matching"
697,"Edit distance, assembly, overlaps",4,https://www.coursera.org/learn/dna-sequencing?specialization=genomic-data-science,"This week we finish our discussion of read alignment by learning about algorithms that solve both the edit distance problem and related biosequence analysis problems, like global and local alignment."
698,Algorithms for assembly,4,https://www.coursera.org/learn/dna-sequencing?specialization=genomic-data-science,"In the last module we began our discussion of the assembly problem and we saw a couple basic principles behind it.  In this module, we'll learn a few ways to solve the alignment problem."
699,Введение,4,https://www.coursera.org/learn/mathematics-and-python?specialization=machine-learning-data-analysis,"Добро пожаловать! На этой неделе мы начнём осваивать язык Python — один из главных инструментов специалиста в науке о данных, и вспомним кое-что о производных, которые активно используются при настройке моделей машинного обучения."
700,Библиотеки Python и линейная алгебра,4,https://www.coursera.org/learn/mathematics-and-python?specialization=machine-learning-data-analysis,"На этой неделе мы познакомимся с Python-библиотеками, содержащими большое количество полезных инструментов: от быстрых операций с многомерными массивами до визуализации и реализации различных математических методов. Кроме того, мы освоим линейную алгебру — основной математический аппарат для работы с данными: в большинстве задач данные можно представить в виде векторов или матриц."
701,Оптимизация и матричные разложения,4,https://www.coursera.org/learn/mathematics-and-python?specialization=machine-learning-data-analysis,"На этой неделе мы научимся с помощью методов оптимизации находить наилучшие значения параметров системы, чтобы минимизировать затраты или максимизировать точность предсказаний, а также познакомимся с матричными разложениями, которые используются при построении регрессионных моделей, для уменьшения размерности данных, в рекомендательных системах и в анализе текстов. "
702,Случайность,4,https://www.coursera.org/learn/mathematics-and-python?specialization=machine-learning-data-analysis,"На этой неделе мы освоим базовые концепции теории вероятностей и статистики, которые необходимы для понимания механизма работы практически всех методов анализа данных. Мы разберёмся с самыми популярными распределениями, узнаем, какие явления ими описываются и какими статистиками оцениваются их параметры, а также научимся строить доверительные интервалы."
703,Машинное обучение и линейные модели,5,https://www.coursera.org/learn/supervised-learning?specialization=machine-learning-data-analysis,"Добро пожаловать на курс ""Обучение на размеченных данных""! В этом модуле вы узнаете, что такое машинное обучение, какие в нём бывают постановки задачи, и что особенного в обучении на размеченных данных. Затем вы изучите один из основных способов решения задач обучения на размеченных данных — предсказание с помощью линейных моделей. Мы обсудим, как их настраивать и применять в задачах регрессии и классификации. В практических заданиях вы поработаете с настоящими данными и узнаете, какие проблемы в них можно обнаружить, а также попробуете делать прогнозы при помощи линейных моделей."
704,Борьба с переобучением и оценивание качества,5,https://www.coursera.org/learn/supervised-learning?specialization=machine-learning-data-analysis,"Вторая неделя нашего курса будет посвящена общим вопросам, с которыми приходится столкнуться в любой задаче анализа данных. Вы узнаете, что такое проблема переобучения, из-за чего она возникает, как её можно обнаружить и как с ней бороться — в частности, вы познакомитесь с кросс-валидацией, с помощью которой можно оценить способность алгоритма давать хорошие предсказания на новых данных. Далее речь пойдёт о метриках качества — без них невозможно понять, подходит ли алгоритм для решения той или иной задачи. Наконец, вы познакомитесь с библиотекой scikit-learn, которая является одним из основных инструментов современных специалистов по анализу данных."
705,Линейные модели: классификация и практические аспекты,5,https://www.coursera.org/learn/supervised-learning?specialization=machine-learning-data-analysis,"Добро пожаловать на третью неделю курса! Вы уже поработали с линейными моделями, научились измерять их качество и устранять переобучение с помощью регуляризации. Пришло время разобраться, почему регуляризация действительно помогает уменьшить сложность модели или произвести отбор признаков — об этом пойдёт речь в первом уроке. Там же вы познакомитесь с логистической регрессией, которая является одним из наиболее популярных методов для решения задач классификации. Далее вы узнаете о некоторых важных нюансах работы с линейными моделями: масштабировании признаков, переходе в новые признаковые пространства и т.д. Мы не только расскажем обо всём этом, но и покажем, как оно работает в Python и библиотеке scikit-learn."
706,Решающие деревья и композиции алгоритмов,5,https://www.coursera.org/learn/supervised-learning?specialization=machine-learning-data-analysis,"Линейные модели — очень важный и полезный, но слишком простой класс алгоритмов в машинном обучении; не во всех задачах они позволяют добиться желаемого качества. В этом модуле вы познакомитесь с новым семейством алгоритмов — решающими деревьями. Они во многом являются полной противоположностью линейных моделей. В частности, сами по себе они очень сложны и подвержены переобучению. При этом оказывается, что если объединить много деревьев в одну сложную модель, то можно получить очень качественное решение. Об этом крайне важном подходе — построении композиций решающих деревьев — мы в основном и будем говорить на этой неделе."
707,Нейронные сети и обзор методов,5,https://www.coursera.org/learn/supervised-learning?specialization=machine-learning-data-analysis,"Рады приветствовать вас на последней, пятой неделе курса! Мы расскажем ещё о нескольких подходах к решению задач машинного обучения, которые не были затронуты в предыдущих модулях, но при этом играют важную роль в практических задачах. Это нейронные сети, байесовские модели, метрические методы. Вы узнаете, для чего они нужны, и попробуете самостоятельно применить их в практических заданиях."
708,Кластеризация,4,https://www.coursera.org/learn/unsupervised-learning?specialization=machine-learning-data-analysis,"Добро пожаловать на курс ""Поиск структуры в данных""! В этом курсе вы узнаете про задачи машинного обучения, в которых требуется не предсказать целевую переменную, а найти некоторые внутренние закономерности в данных — например, сгруппировать объекты по схожести, или определить наиболее важные признаки. В первом модуле мы изучим задачу кластеризации, направленную на поиск групп близких объектов. Вы узнаете про основные подходы к её решению, а также узнаете, как можно выбрать хороший алгоритм кластеризации, не имея правильных ответов."
709,Понижение размерности и матричные разложения,4,https://www.coursera.org/learn/unsupervised-learning?specialization=machine-learning-data-analysis,"В предыдущем модуле мы обсуждали, как кластеризовать объекты, а в этом модуле займёмся признаками. Нередко возникают ситуации, в которых далеко не все признаки нужны для решения задачи — или же нужны все, но при этом их слишком много. В этом случае нужно перейти в новое признаковое пространство меньшей размерности. Для этого можно либо отбирать наиболее важные признаки, либо порождать новые на основе исходных — мы обсудим оба подхода. В частности, мы разберёмся с методом главных компонент, который используется в самых разных задачах машинного обучения. Затем мы перейдём к матричным разложениям — мы изучим несколько методов, позволяющих получить приближение исходной матрицы в виде произведения нескольких матриц меньшей размерности. Такая аппроксимация часто используется в задачах машинного обучения, например, для понижения размерности данных, восстановления пропущенных значений в матрицах и построения рекомендательных систем."
710,Визуализация и поиск аномалий,4,https://www.coursera.org/learn/unsupervised-learning?specialization=machine-learning-data-analysis,"Добро пожаловать на третью неделю курса! В ней мы обсудим две задачи: обнаружение аномалий и визуализация данных. Обнаружение аномалий направлено на поиск объектов, которые являются особенными в некотором смысле. Например, это могут объекты с такими значениями признаков, которые далеки от имеющихся в обучающей выборке — вполне ожидаемо, что на таких объектах модель выдаст очень плохие прогнозы. Вы узнаете, как можно формально дать определение аномалий и с помощью каких методов можно решать задачу их поиска. Вторая задача, о которой мы поговорим — это визуализация, то есть отображение многомерной выборки в пространство размерности два или три. В теории визуализация близка к понижению размерности — но за счёт того, что нам нужно найти всего два или три признака, можно использовать очень сложные нелинейные методы."
711,Тематическое моделирование,4,https://www.coursera.org/learn/unsupervised-learning?specialization=machine-learning-data-analysis,"Люди уже много веков сохраняют свои знания в виде книг, а крупнейшая на сегодняшний день коллекция информации — Интернет — состоит из огромного количества текстов. Тексты, по сути, являются наиболее популярным видом данных, и поэтому очень важно уметь искать в них закономерности. Тематическое моделирование — это способ семантического анализа коллекции текстовых документов. Тематическая модель позволяет для каждого документа найти темы, которые его описывают, и кроме того показывает, какие слова характеризуют ту или иную тему. Другими словами, мы находим более компактное представление большого набора текстов в виде нескольких тем. С математической точки зрения тематическая модель — это еще один вид матричного разложения, где в качестве исходной матрицы выступает матрица частот слов в документах. На четвертой неделе мы поговорим о том, где применяют тематические модели, какие они бывают, как их строить и как оценивать. "
712,Интервалы и гипотезы,5,https://www.coursera.org/learn/stats-for-data-analysis?specialization=machine-learning-data-analysis,"Добро пожаловать на курс ""Построение выводов по данным""! В этом модуле вы узнаете, как работают базовые статистические техники — интервальное оценивание и проверка гипотез. В тестах вас ждёт большое количество задач с реальными данными на применение этих техник."
713,АБ-тестирование,5,https://www.coursera.org/learn/stats-for-data-analysis?specialization=machine-learning-data-analysis,"Вторая неделя посвящена задачам АБ-тестирования — статистической технике, позволяющей оценить действие изменений в вашем продукте на конечного пользователя. Вы узнаете, как правильно такой эксперимент строить и какими методами анализировать. "
714,Закономерности и зависимости,5,https://www.coursera.org/learn/stats-for-data-analysis?specialization=machine-learning-data-analysis,"На этой неделе мы будем искать закономерности и выявлять зависимости. Для этого можно использовать разные методы; мы поговорим о корреляционных и регрессионных. Поскольку в основе этих методов лежит проверка большого количества гипотез, необходимо делать поправку на множественность — почему и как, вы тоже узнаете."
715,Неделя задач,5,https://www.coursera.org/learn/stats-for-data-analysis?specialization=machine-learning-data-analysis,"На этой неделе мы поговорим с экспертами в прикладных областях анализа данных и узнаем, чем особенны их задачи, какие методы построения выводов они используют, и на что они советуют обращать внимание.Для прохождения курса вам нужно решить как минимум два задания, но, если вам интересно, вы можете сделать все."
716,Неделя задач: Lesson Choices,5,https://www.coursera.org/learn/stats-for-data-analysis?specialization=machine-learning-data-analysis,
717,Introduction,4,https://www.coursera.org/learn/human-computer-interaction?specialization=interaction-design,A brief introduction to the topics and goals of this Interaction Design Specialization
718,Needfinding,4,https://www.coursera.org/learn/human-computer-interaction?specialization=interaction-design,"This module’s videos and assignment cover a really important topic: where can you get good design ideas from? Ideas that help you create meaningful designs that have a real impact on real people’s lives. Of course, good ideas come from lots of places. And wherever they come from, great. There are a few strategies I’ve found that are especially valuable, and that’s what we cover in this module’s videos. Given our focus on *real* people, we focus on going out, watching what people do, and talking to them. Check out the first video, describing participant observation. Combining observation with interviewing (the second video) provides a powerful foundation for needfinding and brainstorming.So what happens after you’ve observed a lot of stuff -- how do you connect the high-level needs to concrete design ideas? To help you bridge this gap, this module closes with a video on Creating Design Goals."
719,Rapid Prototyping: Wizard of Oz Prototyping,4,https://www.coursera.org/learn/human-computer-interaction?specialization=interaction-design,"This module's lectures introduce storyboarding and several strategies and media for rapid prototyping, including paper, Wizard of Oz Prototyping, and video.  An important part of the creativity of a designer is to think about how you can rapidly prototype and get feedback on your ideas. Because it's almost never the case that the first idea you have will be the best. As a designer, you can learn the most when you're creating and getting feedback on multiple alternatives. Your work will nearly always benefit from thinking broadly to find the right design, and then from lots of polish to get the design right. Prototyping is also a great way to achieve common ground across the design team and other stakeholders. We begin with storyboards, paper prototyping and mockups. Students often ask about the relationship between needfinding and prototyping: how closely does one flow into the other? Ultimately the quality of your final design is the real measure and there are lots of ways to get there. While most design work benefits from prototypes directly informed by the needfinding process, it's not required. This module seeks to introduce you to doing human-centered design, and walk you through one path that such an approach could take. In both this Specialization and in real life, you are welcome to revise your ideas as much as you like. "
720,Heuristic Evaluation,4,https://www.coursera.org/learn/human-computer-interaction?specialization=interaction-design,"With this module's videos, we turn our focus from brainstorming and prototyping to the concrete elements of interaction design. We introduce ten key principles of good design  -- like the importance of feedback and helping people recover from errors. We call these heuristics -- watch the first video here. I hope you'll find these heuristics practical and applicable to your work both inside and outside class. The lectures discuss these heuristics with a bunch of examples drawn from real-world designs. The goal is to illustrate the many ways that designs can be successful or run into trouble.  While heuristic evaluations (HEs) focus on issues that lead to improvement they can also identify areas we think show successful compliance with a heuristic"
721,Welcome and Course Overview,4,https://www.coursera.org/learn/design-principles?specialization=interaction-design,Welcome to the course! Here are some helpful resources to guide you through this course.
722,Direct Manipulation and Representations,4,https://www.coursera.org/learn/design-principles?specialization=interaction-design,"Our lecture videos in this module begin with the major innovation of the graphical interface: enabling people to perform input directly on top of output. This directness makes interfaces easier to learn because it enables people to recognize familiar elements. And continuous feedback makes interfaces easier to use, encourages exploration, and prevents errors. To illustrate the benefits of direct manipulation in real interfaces, the videos provide several examples of both particular designs and interface styles. I find that's a lot more useful than just stating abstract principles. Now is a good time to remind everyone that I am not endorsing (or rejecting) any particular product, organization, or person. What I am doing: real people in the real world make real design decisions -- you can learn from this -- and in this course I'll discuss these concrete examples so you can gain real knowledge. The rest of the videos will cover topics related to the importance of representations, such as understanding a user's mental model and helping people to distribute cognition. I will show some examples of how representational differences can impact performance. As you watch these videos, think about how you have arranged or lamented representations in your everyday life. Maybe you put your keys by the door, sunglasses on your hat, or a post-it on your laptop? You'll get a chance to delve into these examples in the assignment."
723,Visual Design and Information Design,4,https://www.coursera.org/learn/design-principles?specialization=interaction-design,"So far, many examples in our videos have been physical. I like physical examples because they’re often easier to understand, and they durably express fundamental principles. Equipped with those fundamentals, we'll now focus more on concrete issues in interaction design to help you flesh out your interactive prototypes. This module’s videos introduce visual and information design. These are the nuts and bolts of user interfaces: scale, contrast, pattern, shape, color, typography, and layout. What I hope you'll take away from these lectures is a newfound appreciation for how subtle changes in this visual variables can powerfully impact people's experience of documents and interfaces. Dive into the first visual design lecture here. Visual design organizes the world of information. As this module’s lectures show, that visual organization provides important cues, yet the structure itself is often invisible."
724,Designing Experiments,4,https://www.coursera.org/learn/design-principles?specialization=interaction-design,"After you’ve made a design, how do you know whether it is good? Or if your team has a couple ideas it is considering, how do you know which one is better? Rather than arguing, throwing chairs, or playing rochambeau, we suggest getting your designs in front of real users to see how well they actually work. To enable you to do this, our final module of lectures will introduce you to designing, running, and analyzing experiments. Testing your ideas with people and using what you learn to make them better can often mean the difference between a flop and a hit. Usability testing also gives you a chance to flex your rapid prototyping muscles. Build several interfaces quickly, try them out with people, and use what you learn to revise them. Through repeated iteration and testing, you can end up with a wonderfully polished interface. For me, the most exhilarating aspect of running experiments is the element of surprise. Nearly every time my students, colleagues, and I run a study, we learn something that we never even thought to think of. Sometimes, it's a roadblock or bug. Other times, it's an unexpected new use of a system -- many great startups have emerged out of finding unexpected new uses for technology. Either way, it'll give you new fodder for design. As in the prototyping lectures, the evaluation lectures emphasize comparison -- testing multiple ideas. In many ways, design is choice, and comparing multiple interfaces helps you make good choices. Learn more about designing studies here."
725,Connecting People through Technology,3,https://www.coursera.org/learn/social-computing?specialization=interaction-design,"When you think of social media, you might think of a popular social network, like Facebook, Twitter, or LinkedIn. These are indeed powerful, but the universe of social media is much larger. Both online and off, most everything we do has a social component that is mediated by some technology.  After all, technologies have been helping people communicate since long before your favorite curly-haired CEO dropped out of college. In this module, you’ll learn about different types of social software, how to think about physical collaboration and digital collaboration using the same framework, and what the challenges to success for collaborative software are. "
726,Challenges & Opportunities of Collaboration Online,3,https://www.coursera.org/learn/social-computing?specialization=interaction-design,"In our always-on, pervasively connected world, it can seem as if space and distance don’t matter anymore. Indeed, more and more organizations are outsourcing work around the globe and/or opening satellite locations. But being instantly reachable from anywhere in the world, isn’t the same as being there. In this module, you’ll learn the ways that distance does and doesn’t matter, how to design for this reality and how to create technologies that go beyond being there. "
727,Crowdsourcing,3,https://www.coursera.org/learn/social-computing?specialization=interaction-design,"It can’t possibly work, can it? Slice up challenging problems into tiny pieces, distribute them to people all over the planet, and then gather their responses to piece together a solution. In this module, you’ll learn both the possibilities and limits of crowdsourcing. You’ll learn strategies for being able to use crowdsourcing successfully, and even hear from the voice of a professional crowdworker about what makes it successful from a worker’s perspective. To gain deeper intuitions, you’ll get to try it out yourself in the assignment. "
728,Input,3,https://www.coursera.org/learn/interaction-techniques?specialization=interaction-design,"There’s more to interfaces than what’s on the screen. While often overlooked, input deserves to be on equal footing with its more popular sibling, output. Also, input is a rare case where we can model user behavior mathematically. In this module, you’ll learn how good input is more than just preference, trace input from the fingertip to the screen, and think about the diversity of possible input devices and their relative merits. "
729,Search & Navigation,3,https://www.coursera.org/learn/interaction-techniques?specialization=interaction-design,"We live in an information-rich world. Consequently, the hardest part of interaction is often finding what we want or finding something that we didn’t know what to ask for. From e-commerce to digital libraries, good search design is central to human-computer interaction in the 21st century."
730,Gestural Interfaces,3,https://www.coursera.org/learn/interaction-techniques?specialization=interaction-design,"The graphical user interface with windows, icons, menus, and pointers (WIMP) was a massive advance beyond the command line. It also took 20 years to go from research labs into people’s homes. We’re in the midst of a new sea change now. Gestural interfaces can be even more direct and more natural. They can also be even more frustrating. In this module, you’ll learn how to tell the difference and design gestural interfaces that work. "
731,Introduction to the Data Science Workflow,5,https://www.coursera.org/learn/exploratory-data-analysis-matlab,"In this module you’ll learn about the key steps in a data science workflow and begin exploring a data set using a script provided for you. As you work with the file, take note of the different elements in the script. As you progress through the course, you’ll create a similar script yourself."
732,Importing Data,5,https://www.coursera.org/learn/exploratory-data-analysis-matlab,"In this module you’ll import data into MATLAB, customize the import options, and generate code to automate the process. You’ll also work with different types of data, such as numeric, dates, and text."
733,Visualizing and Filtering Data,5,https://www.coursera.org/learn/exploratory-data-analysis-matlab,In this module you’ll create visualizations and learn how to customize figures. You’ll also filter your data to select only what is needed for your analysis. You’ll create new tables and save them to use in the future or share with others outside of MATLAB.
734,Performing Calculations,5,https://www.coursera.org/learn/exploratory-data-analysis-matlab,"In this module you’ll write small pieces of code to extend your analysis. You’ll calculate summary statistics on groups of data and determine if variables are correlated. You’ll extend your ability to filter data to defining conditions across multiple variables. You’ll also modify categorical data to remove, combine, or create new categories to use for defining groups."
735,Documenting Your Work,5,https://www.coursera.org/learn/exploratory-data-analysis-matlab,In this module you’ll create live scripts with interactive controls. Then you’ll create your own analysis of a weather event to submit as a peer-reviewed assignment.
736,Surveying Your Data,5,https://www.coursera.org/learn/feature-engineering-matlab,In this module you'll apply the skills gained in Exploratory Data Analysis with MATLAB on a new dataset. You'll explore different types of distributions and calculate quantities like the skewness and interquartile range. You'll also learn about more types of plots for visualizing multi-dimensional data.
737,Organizing Your Data,5,https://www.coursera.org/learn/feature-engineering-matlab,In this module you'll learn to prepare data for analysis. Often data is not recorded as required. You'll learn to manipulate string variables to extract key information. You'll create a single datetime variable from date and time information spread across multiple columns in a table. You'll efficiently load and combine data from multiple files to create a final table for analysis.
738,Cleaning Your Data,5,https://www.coursera.org/learn/feature-engineering-matlab,"In this module you'll clean messy data. Missing data, outliers, and variables with very different scales can obscure trends in the data. You'll find and address missing data and outliers in a data set. You'll compare variables with different scales by normalizing variables."
739,Finding Features that Matter,5,https://www.coursera.org/learn/feature-engineering-matlab,In this module you'll create new features to better understand your data. You'll evaluate features to determine if a feature is potentially useful for making predictions.
740,Domain-Specific Feature Engineering,5,https://www.coursera.org/learn/feature-engineering-matlab,In this module you'll apply the concepts from Modules 1 through 4 to different domains. You'll create and evaluate features using time-based signals such as accelerometer data from a cell phone. You'll use Apps in MATLAB to perform image processing and create features based on segmented images. You'll also use text processing techniques to find features in unstructured text.
741,Creating Regression Models,1,https://www.coursera.org/learn/predictive-modeling-machine-learning,
742,IBM AI Enterprise Workflow Introduction,3,https://www.coursera.org/learn/ibm-ai-workflow-business-priorities-data-ingestion?specialization=ibm-ai-workflow,"Like most subjects, practice makes perfect in Data Science.   In the capstone project, you will apply the skills learned across courses in the Practical Data Science with MATLAB specialization to explore, process, analyze, and model data.   You will choose your own pathway to answer key questions with the provided data.To complete the project, you must have mastery of the skills covered in other courses in the specialization.  The project will test your ability to import and explore your data, prepare the data for analysis, train a predictive model, evaluate and improve your model, and communicate your results."
743,Data Collection,3,https://www.coursera.org/learn/ibm-ai-workflow-business-priorities-data-ingestion?specialization=ibm-ai-workflow,"Accelerating the pace of discovery, innovation, development, and learning in engineering and science."
744,Data Ingestion,3,https://www.coursera.org/learn/ibm-ai-workflow-business-priorities-data-ingestion?specialization=ibm-ai-workflow,"The goal of this first module is to introduce you to the overall specialization requirements, evaluate your understanding of some key prerequisite knowledge, and become familiar with several process models being used today.  In this course we will use the process of design thinking, but it is the consistent application of a process in practice that is important, not the exact process itself. There are a number of reasons for choosing the design thinking process, but the most important is that it is being applied in a cross-disciplinary way—that is outside of data science."
745,Data Analysis,2,https://www.coursera.org/learn/ibm-ai-workflow-data-analysis-hypothesis-testing?specialization=ibm-ai-workflow,"Throughout this module you will learn or reinforce what you already know about identifying and articulating business opportunities. In this module you will learn the importance of applying a scientific thought process to the task of understanding the business use case. This process has many similarities to that of being an investigator. You will also generate a healthy respect for the need to pause, step back and think scientifically about the main processes in this stage."
746,Data Investigation,2,https://www.coursera.org/learn/ibm-ai-workflow-data-analysis-hypothesis-testing?specialization=ibm-ai-workflow,"Cleaning, parsing, assembling and gut-checking data is among the most time-consuming tasks that a data scientist has to perform. The time spent on data cleaning can start at 60% and increase depending on data quality and the project requirements. This module looks at the process of ingesting data and presents a case study working a real world scenario."
747,Data transforms and feature engineering,2,https://www.coursera.org/learn/ibm-ai-workflow-feature-engineering-bias-detection?specialization=ibm-ai-workflow,"Exploratory data analysis is mostly about gaining insight through visualization and hypothesis testing.  This unit looks at EDA, data visualization, and missing values. One missing value strategy may be better for some models, but for others another strategy may show better predictive performance."
748,Pattern recognition and data mining best practices,2,https://www.coursera.org/learn/ibm-ai-workflow-feature-engineering-bias-detection?specialization=ibm-ai-workflow,Data scientists employ a broad range of statistical tools to analyze data and reach conclusions from data. This unit focuses on the foundational techniques of estimation with probability distributions and extending these estimates to apply null hypothesis significance tests.
749,Model Evaluation and Performance Metrics,2,https://www.coursera.org/learn/ibm-ai-workflow-machine-learning-vr-nlp?specialization=ibm-ai-workflow,This module will introduce you to skills required for effective feature engineering in today's business enterprises.  The skills are presented as a series of best practices representing years of practical experience.
750,Building Machine Learning and Deep Learning Models,2,https://www.coursera.org/learn/ibm-ai-workflow-machine-learning-vr-nlp?specialization=ibm-ai-workflow,"This module will continue the discussion of skill related to feature engineering for practicing data scientists, with a focus on outliers and the use of unsupervised learning techniques for finding patterns."
751,Artificial Intelligence & Machine Learning,4,https://www.coursera.org/learn/guided-tour-machine-learning-finance,"This week covers model selection, evaluation and performance metrics.  The focus is on evaluating models iteratively for improvements. You will survey the landscape of evaluation metrics and linear models in order to ensure you are comfortable using implementing baseline models. The materials build up to the case study where you will use natural language processing in a classification setting. When you are done iterating on your model you will connect its model performance to business metrics as an approach to better understand model utility."
752,Mathematical Foundations of Machine Learning,4,https://www.coursera.org/learn/guided-tour-machine-learning-finance,"This week is primarily focused on building supervised learning models. We will survey available methods in two popular and effective areas of machine learning: tree-based algorithms and deep-learning algorithms. We will cover the use of tree-based methods like random forests and boosting along with other ensemble approaches. Many of these approaches serve as an important middle layer between interpretable linear models and difficult to interpret deep-learning models. For deep-learning we will use a pre-built visual recognition model and use TensorFlow to demonstrate how to build, turn and iterate on neural networks. We will also make sure that you understand popular neural network architectures. In the case study you will implement a convolutional neural network and ready it for deployment."
753,Introduction to Supervised Learning,4,https://www.coursera.org/learn/guided-tour-machine-learning-finance,
754,Supervised Learning in Finance,4,https://www.coursera.org/learn/guided-tour-machine-learning-finance,
755,Fundamentals of Supervised Learning in Finance,4,https://www.coursera.org/learn/fundamentals-machine-learning-in-finance,
756,"Core Concepts of Unsupervised Learning, PCA & Dimensionality Reduction",4,https://www.coursera.org/learn/fundamentals-machine-learning-in-finance,
757,Data Visualization & Clustering,4,https://www.coursera.org/learn/fundamentals-machine-learning-in-finance,
758,Sequence Modeling and Reinforcement Learning,4,https://www.coursera.org/learn/fundamentals-machine-learning-in-finance,
759,MDP and Reinforcement Learning,4,https://www.coursera.org/learn/reinforcement-learning-in-finance,
760,MDP model for option pricing: Dynamic Programming Approach,4,https://www.coursera.org/learn/reinforcement-learning-in-finance,
761,MDP model for option pricing - Reinforcement Learning approach,4,https://www.coursera.org/learn/reinforcement-learning-in-finance,
762,RL and INVERSE RL for Portfolio Stock Trading,4,https://www.coursera.org/learn/reinforcement-learning-in-finance,
763,"Black-Scholes-Merton model, Physics and Reinforcement Learning",4,https://www.coursera.org/learn/advanced-methods-reinforcement-learning-finance,
764,Reinforcement Learning for Optimal Trading and Market Modeling,4,https://www.coursera.org/learn/advanced-methods-reinforcement-learning-finance,
765,Perception - Beyond Reinforcement Learning,4,https://www.coursera.org/learn/advanced-methods-reinforcement-learning-finance,
766,"Other Applications of Reinforcement Learning: P-2-P Lending, Cryptocurrency, etc.",4,https://www.coursera.org/learn/advanced-methods-reinforcement-learning-finance,
767,Preface,6,https://www.coursera.org/learn/recommender-systems-introduction?specialization=recommender-systems,
768,Introducing Recommender Systems,6,https://www.coursera.org/learn/recommender-systems-introduction?specialization=recommender-systems,
769,Non-Personalized and Stereotype-Based Recommenders,6,https://www.coursera.org/learn/recommender-systems-introduction?specialization=recommender-systems,This brief module introduces the topic of recommender systems (including placing the technology in historical context) and provides an overview of the structure and coverage of the course and specialization.
770,Content-Based Filtering -- Part I,6,https://www.coursera.org/learn/recommender-systems-introduction?specialization=recommender-systems,"This module introduces recommender systems  in more depth.  It includes a detailed taxonomy of the types of recommender systems, and also includes tours of two systems heavily dependent on recommender technology:  MovieLens and Amazon.com. There is an introductory assessment in the final lesson to ensure that you understand the core concepts behind recommendations before we start learning how to compute them."
771,Content-Based Filtering -- Part II,6,https://www.coursera.org/learn/recommender-systems-introduction?specialization=recommender-systems,"In this module, you will learn several techniques for non- and lightly-personalized recommendations, including how to use meaningful summary statistics, how to compute product association recommendations, and how to explore using demographics as a means for light personalization.  There is both an assignment (trying out these techniques in a spreadsheet) and a quiz to test your comprehension.  "
772,Course Wrap-up,6,https://www.coursera.org/learn/recommender-systems-introduction?specialization=recommender-systems,"The next topic in this course is content-based filtering, a technique for personalization based on building a profile of personal interests.  Divided over two weeks, you will learn and practice the basic techniques for content-based filtering and then explore a variety of advanced interfaces and content-based computational techniques being used in recommender systems.  "
773,Preface,6,https://www.coursera.org/learn/collaborative-filtering?specialization=recommender-systems,"The assessments for content-based filtering include an assignment where you compute three types of profile and prediction using a spreadsheet and a quiz on the topics covered.  The assignment is in three parts -- a written assignment, a video intro, and a ""quiz"" where you provide answers from your work to be automatically graded."
774,User-User Collaborative Filtering Recommenders Part 1,6,https://www.coursera.org/learn/collaborative-filtering?specialization=recommender-systems,We close this course with a set of mathematical notation that will be helpful as we move forward into a wider range of recommender systems (in later courses in this specialization).  
775,User-User Collaborative Filtering Recommenders Part 2,6,https://www.coursera.org/learn/collaborative-filtering?specialization=recommender-systems,"Note that this course is structured into two-week chunks.  The first chunk focuses on User-User Collaborative Filtering; the second chunk on Item-Item Collaborative Filtering.  Each chunk has most of the lectures in the first week, and assignments/quizzes and advanced topics in the second week.  We encourage learners to treat each two-week chunk as one unit, starting the assignments as soon as they feel they have learned enough to get going."
776,Item-Item Collaborative Filtering Recommenders Part 1,6,https://www.coursera.org/learn/collaborative-filtering?specialization=recommender-systems,
777,Item-Item Collaborative Filtering Recommenders Part 2,6,https://www.coursera.org/learn/collaborative-filtering?specialization=recommender-systems,
778,Advanced Collaborative Filtering Topics,6,https://www.coursera.org/learn/collaborative-filtering?specialization=recommender-systems,
779,Preface,5,https://www.coursera.org/learn/recommender-metrics?specialization=recommender-systems,
780,Basic Prediction and Recommendation Metrics,5,https://www.coursera.org/learn/recommender-metrics?specialization=recommender-systems,
781,Advanced Metrics and Offline Evaluation,5,https://www.coursera.org/learn/recommender-metrics?specialization=recommender-systems,
782,Online Evaluation,5,https://www.coursera.org/learn/recommender-metrics?specialization=recommender-systems,
783,Evaluation Design,5,https://www.coursera.org/learn/recommender-metrics?specialization=recommender-systems,
784,Preface,6,https://www.coursera.org/learn/matrix-factorization?specialization=recommender-systems,
785,Matrix Factorization (Part 1),6,https://www.coursera.org/learn/matrix-factorization?specialization=recommender-systems,
786,Matrix Factorization (Part 2),6,https://www.coursera.org/learn/matrix-factorization?specialization=recommender-systems,
787,Hybrid Recommenders,6,https://www.coursera.org/learn/matrix-factorization?specialization=recommender-systems,"This is a two-part, two-week module on matrix factorization recommender techniques.  It includes an assignment and quiz (both due in the second week), and an honors assignment (also due in the second week).  Please pace yourself carefully -- it will be difficult to finish in two weeks unless you start the assignments during the first week.  "
788,Advanced Machine Learning,6,https://www.coursera.org/learn/matrix-factorization?specialization=recommender-systems,
789,Advanced Topics,6,https://www.coursera.org/learn/matrix-factorization?specialization=recommender-systems,"This is a three-part, two-week module on hybrid and machine learning recommendaton algorithms and advanced recommender techniques.  It includes a quiz (due in the second week), and an honors assignment (also due in the second week).  Please pace yourself carefully -- it will be difficult to finish the honors track in two weeks unless you start the assignments during the first week.  "
790,Research Designs and Data Sources,4,https://www.coursera.org/learn/data-collection-framework?specialization=data-collection,
791,Measurements and Analysis Plan,4,https://www.coursera.org/learn/data-collection-framework?specialization=data-collection,
792,Quality Framework,4,https://www.coursera.org/learn/data-collection-framework?specialization=data-collection,"The first course in the specialization provides an overview of the topics to come. This module walks you through the process of data collection and analysis. Starting with a research question and a review of existing data sources, we cover survey data collection techniques, highlight the importance of data curation, and discuss some basic features that can affect your data analysis when dealing with sample data. Issues of data access and resources for access are introduced in this module. "
793,Application of TSE Framework to Existing Surveys,4,https://www.coursera.org/learn/data-collection-framework?specialization=data-collection,"In this module we will emphasize the importance of having a well-specified research question and analysis plan. We will provide an overview over the various data collection strategies, a variety of available modes for data collection and some thinking on how to choose the right mode. "
794,"Module 1: Introduction, Classic Modes of Survey Data Collection ",4,https://www.coursera.org/learn/data-collection-methods?specialization=data-collection,"In this module you will be introduced to a general framework that allows you to not only understand each step required for a successful data collection and analysis, but also helps you to identify errors associated with different data sources. You will learn some metrics to quantify each potential error, and thus you will have tools at hand to describe the quality of a data source."
795,"Module  2: Self-administration, Online Data Collection",4,https://www.coursera.org/learn/data-collection-methods?specialization=data-collection,In this module we introduce a few surveys across a variety of topics. For each we highlight data collection features. The surveys span a variety of topics. We challenge you to think about alternative data sources that can be used to gather the same information or insights.
796,Module 3: Interviewers and Interviewing,4,https://www.coursera.org/learn/data-collection-methods?specialization=data-collection,"In this lesson, you will be introduced to some key concepts about survey data collection methods that we will rely on throughout the course.  By the end of this lesson, you should be well acquainted with the major sources of survey error and how these are affected -- usually in the form of tradeoffs -- by the particular mode used to administer questions and capture responses."
797,"Module 4: Emerging modes, new data sources",4,https://www.coursera.org/learn/data-collection-methods?specialization=data-collection,"This second lesson focuses on modes in which survey respondents self-administer questions and provide their responses directly to researchers.  By the end of Lesson 2, you will understand the pros and cons of self-administered modes from the TSE perspective."
798,Introduction and Unit 1: Overview of Standardized Interviewing,6,https://www.coursera.org/learn/questionnaire-design?specialization=data-collection,"In this lesson, we explore the various roles interviewers take on beside asking questions and collecting answers, as well as some of the different approaches to interviewing that have been proposed and how they affect the accuracy of responses.  By the end of Lesson 3, you will appreciate the benefits and costs of collecting data in interviews and will be able to contrast them with the costs and benefits of self-administration. "
799,Unit 2: Response Process,6,https://www.coursera.org/learn/questionnaire-design?specialization=data-collection,"In this lesson, we focus on some new data collection modes such as mobile web surveys and SMS text interviews, as well as alternative data sources such as sensor data, administrative data, and social media.  By the end of this lesson, you will have a sense of the issues to which survey methodologists and survey researchers are devoting much of their attention these days. You will be able to weigh the pros and cons of these new methods and data sources. "
800,Unit 3: Asking Factual Questions,6,https://www.coursera.org/learn/questionnaire-design?specialization=data-collection,"Introduction; Different types of questions, Measurement error in questions: Bias and variance; Standardized and conversational interviewing; From specifying a concept to asking questions"
801,Unit 4: Measuring Attitudes,6,https://www.coursera.org/learn/questionnaire-design?specialization=data-collection,Comprehension; Retrieval; Judgment; Response
802,Unit 5: Testing Questionnaires,6,https://www.coursera.org/learn/questionnaire-design?specialization=data-collection,"Facts and quasi facts; Memory and recall; Asking sensitive questions; Mode, privacy and confidentiality"
803,Unit 6: Putting It All Together,6,https://www.coursera.org/learn/questionnaire-design?specialization=data-collection,Context effects in attitude questions; Use of different scales; Offering don’t know options; Response order effects
804,Module 1: Sampling as a research tool,6,https://www.coursera.org/learn/sampling-methods?specialization=data-collection,Expert reviews and focus groups; Cognitive interviews; Behavior coding; Quantitative techniques
805,Mere randomization,6,https://www.coursera.org/learn/sampling-methods?specialization=data-collection,The questionnaire from start to finish; Things to put at the end; Mode Choice: Implementations for layout; Self-administered questionnaires
806,Saving money using cluster sampling,6,https://www.coursera.org/learn/sampling-methods?specialization=data-collection,
807,Using auxiliary data to be more efficient,6,https://www.coursera.org/learn/sampling-methods?specialization=data-collection,
808,Simplified sampling,6,https://www.coursera.org/learn/sampling-methods?specialization=data-collection,
809,Pulling it all together,6,https://www.coursera.org/learn/sampling-methods?specialization=data-collection,
810,Introduction to Strategic Business Analytics,5,https://www.coursera.org/learn/strategic-business-analytics,
811,Finding groups within Data,5,https://www.coursera.org/learn/strategic-business-analytics,
812,Factors leading to events,5,https://www.coursera.org/learn/strategic-business-analytics," In this module, we will introduce you to the course and instructional approach. You will learn that Strategic Business Analytics relies on four distinct skills: IT, Analytics, Business and Communication."
813,Predictions and Forecasting,5,https://www.coursera.org/learn/strategic-business-analytics,"In this module, you will learn how identifying groups of observations enables you to improve business efficiency. You will then learn to create those groups in a business-oriented and actionable way. We will use examples to illustrate various concepts. The assessments will also provide you with opportunities to replicate these examples."
814,Recommendation production and prioritization,5,https://www.coursera.org/learn/strategic-business-analytics,"In this module, you will learn why using rigorous statistical methods to understand the relationship between different events is crucial. We’ll cover two examples: first, using a credit scoring example, you will learn how to derive information about what makes an individual more or less likely to have a strong credit score? Then, in a second example drawn from HR Analytics, you will learn to estimate what makes an employee more or less likely to leave the company. 
As usual, we invite you to replicate those examples thanks to the recital 
and to use the assessments provided at the end of the module to strengthen your understanding of these concepts."
815,Module 0 : Introduction to Foundation of Marketing Analytics,5,https://www.coursera.org/learn/foundations-marketing-analytics," In this module you will learn more about the importance of forecasting the future.You will learn through examples from various sectors: first, using the previous examples of credit scoring and HR Analytics, you will learn to predict what will happen. Then, you will be introduced to predictive maintenance using survival analysis via a case discussion. Finally, we’ll discuss seasonality in the context of the first example discussed in this MOOC: using analytics for managing your supply chain and logistics better."
816,Module 1 : Statistical segmentation,5,https://www.coursera.org/learn/foundations-marketing-analytics,"So far, you’ve learnt to use Business Analytics to glean important information relevant to the success of your business. In this module, you’ll learn more about how to present your Business Analytics work to a business audience. This module is also important for your final capstone project presentation.You’ll learn that it is important to find an angle, and tell a story.Instead of presenting a list of results that are not connected to each other, you will learn to take your audience by the hand and steer it to the recommendations you want to conclude on.You’ll learn to structure your story and your slides, and master the most used visualization tips and tricks. The assessment at the end of this module will provide an opportunity for you to practice these methods and to prepare the first step of the capstone project."
817,Module 2 : Managerial segmentation,5,https://www.coursera.org/learn/foundations-marketing-analytics,"In this short module, we will introduce the field of marketing analytics, and layout the structure of this course.

We will also take that opportunity to explore a retailing data set that we’ll be using throughout this course. We will setup the environment, load the data in R (we’ll be using the RStudio environment), and explore it using simple SQL statements."
818,Module 3 : Targeting and scoring models,5,https://www.coursera.org/learn/foundations-marketing-analytics,"In this module, you will learn the inner workings of statistical segmentation, how to compute statistical indicators about customers such as recency or frequency, and how to identify homogeneous groups of customers within a database.

We will alternate lectures and R tutorials, making sure that, by the end of this module, you will be able to apply every concept we will cover."
819,Module 4 : Customer lifetime value,5,https://www.coursera.org/learn/foundations-marketing-analytics,"Statistical segmentation is an invaluable tool, especially to explore, summarize, or make a snapshot of an existing database of customers. But what most academics will fail to tell you is that this kind of segmentation is not the method of choice for many companies, and for good reasons.

In this module, you will learn to perform managerial segmentations, which are not built upon statistical techniques, but are an essential addition to your toolbox of marketing analyst.

You will also learn how to segment a database now, but also at any point in time in the past, and why it is useful to managers to do so."
820,Introduction to case studies in business analytics with Accenture,3,https://www.coursera.org/learn/case-studies-business-analytics-accenture,"How can Target predict which of its customers are pregnant? How can a bank predict the likelihood you will default on their loan, or crash your car within the next five years, and price accordingly? And if your firm only has the budget to reach a few customers during a marketing campaign, who should it target to maximize profit?

The answer to all these questions is… by building a scoring model, and targeting your customers accordingly.

In this module, you will learn how to build a customer score, which in marketing usually combines two predictions in one : what is the likelihood that a customer will buy something, and if he does, how much will he buy for?"
821,"Digital Transformation in the Media, the Financial Services and the Retail Sector",3,https://www.coursera.org/learn/case-studies-business-analytics-accenture,"In this module, you will learn how to use R to execute lifetime value analyses. You will learn to estimate what is called a transition matrix -which measures how customers transition from one segment to another- and use that information to make invaluable predictions about how a customer database is likely to evolve over the next few years, and how much money it should be worth."
822,Advanced Analytics in Healthcare and the Pharmaceutical industry / Wrap up and Introduction to capstone,3,https://www.coursera.org/learn/case-studies-business-analytics-accenture,"In this introductory module, Fabrice Marque, Managing Director Customer Strategy Practice Lead for France, Belgium and the Netherlands, also in charge of the ESSEC-Accenture Strategic Business Analytics Chair, will first introduce the MOOC in general. Then Michael Svilar, Global Accenture Data Science Group Lead, will identify the general trends in this sector. In this module, we will cover three different real-life examples. First, Rohit Banerji, Accenture business lead responsible for big data analytics for the resource sector, will present an example from a water utilities company. Second, Cian O’Hare, Managing Director at Accenture Digital, will present a case study from a global communication provider. Finally, Christopher Gray, public service expert at Accenture, will discuss challenges arising in the public sector where Analytics and Big Data can provide effective solutions.   At the end of each example there will be quiz questions. Note that those questions may require you to collect additional information from that which was delivered during the videos. Do not hesitate to consult additional books, websites and examples about this topic: some of the answers can actually be found directly thanks to open access research engines or online encyclopedias! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field."
823,Introduction and step 1 : Define the analysis framework,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,"During this module, different real-life examples will be discussed. Christine Removille, Digital Marketing Lead at the European Level, will present a data-centric digital transformation at a French TV company: Canal +. Edwin Van der Ouderaa, Financial Services Lead, will then explain how digital developments and data are disrupting the financial service sector.At the end of each video there will be  quiz questions. Do not hesitate to consult additional books, websites and examples about this topic! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field."
824,Required assignement 1: Define the analysis framework,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,"During this module, two different real-life examples will be discussed. First, Paul Pierotti, Managing Director at Accenture Digital, will explain how Analytics can transform how health services are delivered. Second Xavier Cimino, Managing Director in charge of the Analytics Practice in the Life Science industry for Europe, will present an award-winning project in this sector. At the end of each video, there will be quiz questions. Do not hesitate to consult additional books, websites and examples about this topic! The objective with this final MOOC in the Strategic Business Analytics specialization is to assess whether you now master the different concepts that are implemented within this field.Finally, Michael Svilar, Global Accenture Data Science Group Lead, will conclude the MOOC."
825,Required feed back on Delivery 1:Define the analysis framework and preparation of deliverable 2,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,"Module 1 in the Business Analytics capstone project provides you with a clear idea of how to successfully complete the ESSEC Business Analytics MOOC. It is dedicated to ensuring that you understand the objectives of the capstone project and lets you consult the datasets to be used for the project as well as examples of what the expected deliverable should look like and contain. Before beginning the project, you are advised to review two previous modules dealing with how to effectively structure and present your findings, and how to approach and explore datasets (""Foundation of Business Analytics"", the wrap up of ""Case Studies in Business Analytics with Accenture""). This module  also gives you the opportunity to try out the preparation of deliverable 1 and receive non-graded feedback from your peers, thereby giving you an essential insight into how the other deliverables and peer review steps will work. "
826,Practice for Deliverable 2,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,The module 2 sets the assignment of preparing deliverable 1 – your analysis framework – for assessment by your peers. 
827,Required assignement 2: Present the intermediary outputs and adjustments to the analysis framework,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,"In Module 3 you will be involved in reviewing the deliverables of a minimum of 3 other students( if you can manage more than 3, then all the better!) as well as preparing deliverable 2"
828,Required feedback for delivery 2 and preparation of delivery 3,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,Module 4 enables you to submit your draft proposal for deliverable 2 to your peers for review and feedback. 
829,Required Delivery 3: Present the final outputs and value case,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,The module 5 sets the assignment of preparing deliverable 2 – Present the intermediary outputs and adjustments to the analysis framework – for assessment by your peers. 
830,Required feedback on Assignment 3: Present the final outputs and value case,8,https://www.coursera.org/learn/strategic-business-analytics-capstone,"In Module 6 you will be involved in reviewing the deliverables of a minimum of 3 other students( if you can manage more than 3, then all the better!) as well as preparing deliverable 3"
831,Los datos y la toma de decisiones en tu vida y en las organizaciones,4,https://www.coursera.org/learn/analisis-de-datos?specialization=analisis-datos,The module 7 sets the assignment of preparing deliverable 3 – Present the final outputs and value case – for assessment by your peers. 
832,Exploración y predicción de datos,4,https://www.coursera.org/learn/analisis-de-datos?specialization=analisis-datos,"In Module 8 you will be involved in reviewing the deliverables of a minimum of 3 other students ( if you can manage more than 3, then all the better!) as well as preparing deliverable 3"
833,Integración y análisis de datos para el diagnóstico organizacional ,4,https://www.coursera.org/learn/analisis-de-datos?specialization=analisis-datos,"¡Bienvenido a Fundamentos del análisis de datos para la toma de decisiones! Te invito a explorar el rol que desempeña el análisis de datos en el mundo actual y su importancia para apoyar la toma de decisiones, tanto en tu vida diaria como en tu contexto laboral. Al concluir este módulo serás capaz de identificar la importancia y el rol que juega el análisis de datos en las organizaciones, así como su enorme utilidad para apoyar la toma de decisiones en las mismas. Conocerás el crecimiento que ha tenido esta área de conocimiento en los últimos años, pero sobre todo el gran potencial que te presenta en tu futuro inmediato. 
De manera particular, este módulo te permitirá no solo identificar casos y ejemplos de aplicación en distintos sectores e industrias, sino también datos e información clave en tu vida y organización, para utilizarlos en forma efectiva en las decisiones más importantes que te interese considerar."
834,Herramientas y métodos para la toma de decisiones,4,https://www.coursera.org/learn/analisis-de-datos?specialization=analisis-datos,"A través de este módulo lograrás desarrollar capacidades iniciales para el Análisis de Datos, que te serán de gran utilidad para una buena toma de decisiones en tu organización. Para lograrlo, nos enfocaremos en dos tareas que son fundamentales en esta rama de conocimiento: la exploración de patrones y comportamientos clave en los datos, así como el realizar predicciones y pronósticos a partir de los mismos.Para lograrlo, desarrollarás la capacidad de utilizar un conjunto de herramientas estadísticas, así como apoyarte en software computacional especializado para este tipo de actividades y que te permitirá generar análisis y resultados de gran utilidad para ti y tu organización, revisando diversos casos reales en donde se han aplicado este tipo de herramientas y estrategias."
835,El reto de tomar mejores decisiones,4,https://www.coursera.org/learn/ordenamiento-datos?specialization=analisis-datos,"A través de este módulo lograrás realizar un análisis integral de datos e información, para generar un diagnóstico que apoye la toma de decisiones en tu organización. Para lograrlo, desarrollarás la capacidad de generar diversos análisis y reportes, que te servirán como base para la identificación de áreas de oportunidad y cursos de acción a tomar.Para lograrlo, desarrollarás la capacidad de utilizar un conjunto de herramientas estadísticas, así como apoyarte en software computacional especializado para este tipo de actividades y que te permitirá generar análisis y resultados de gran utilidad para ti y tu organización, revisando además diversos casos reales en donde se han aplicado este tipo de herramientas y estrategias."
836,Recopilación de datos,4,https://www.coursera.org/learn/ordenamiento-datos?specialization=analisis-datos,"Al terminar este módulo serás capaz de identificar los distintos criterios bajo los cuales podrás tomar decisiones en tu vida y tu organización, a partir de las capacidades de análisis de datos e información que adquiriste en los módulos previos. Conocerás la estructura y beneficios de seguir distintos enfoques para la toma de decisiones, así como cuándo y bajo qué contextos es deseable seguir cada uno de ellos. Además, identificarás también la mejor estrategia a seguir bajo criterios específicos como los costos de oportunidad o escenarios de alta incertidumbre."
837,Un mundo de datos no ordenados,4,https://www.coursera.org/learn/ordenamiento-datos?specialization=analisis-datos,"En este módulo, comprende la parte teórica del curso, examinarás cómo en este nuevo entorno de negocios las organizaciones utilizan el análisis de datos para competir y obtener una ventaja competitiva. Identificarás los tipos de datos y sus características a efecto que reconozcas el nivel de madurez de la organización (con respecto al uso de datos)."
838,Exploración de datos,4,https://www.coursera.org/learn/ordenamiento-datos?specialization=analisis-datos,En este módulo conocerás más sobre los datos y el valor de los mismos. Explorarás el uso de la  herramienta computacional de Watson Analytics para la recolección de datos que te permitirá importar datos estructurados y realizar un análisis descriptivo de los mismos.
839,¿Cómo confiar en los datos?,4,https://www.coursera.org/learn/datos-tecnicas?specialization=analisis-datos,"En este módulo aprenderás el manejo de datos no estructurados utilizando la herramienta computacional Import-io para la lectura de datos de algún sitio web, así como la importación a Watson Analytics de datos de Twitter. Este módulo comprende la parte práctica del curso, por lo que al final del mismo deberás resolver un caso práctico utilizando las dos herramientas antes descritas."
840,¿Son válidas las suposiciones sobre mis datos?,4,https://www.coursera.org/learn/datos-tecnicas?specialization=analisis-datos,"Al finalizar este módulo serás capaz de explorar los datos  utilizando las capacidades de visualización de la herramienta Watson Analytics. De la misma manera, aprenderás a encontrar patrones y relaciones en los datos que representan respuestas a problemas de negocio.  "
841,Modelos de regresión,4,https://www.coursera.org/learn/datos-tecnicas?specialization=analisis-datos,"Con frecuencia, en tu actividad profesional te enfrentas con la necesidad de analizar una gran cantidad de datos con el propósito de identificar si existe alguna relación entre ellos y de esta forma contar con información valiosa que te permita poder tomar una decisión. Los datos con los cuales se genera la información además de requerir de un apropiado tratamiento también demandan de una adecuada técnica para su análisis. En este curso serás capaz de conocer y utilizar distintas técnicas basadas en el análisis estadístico con un enfoque hacia la inteligencia de negocios (BI),  los cuales te permitirán crear modelos para mejorar la comprensión de cómo los datos se relacionan con la población subyacente, validar el modelo  y emplear el análisis predictivo para evaluar escenarios factibles encaminados a  orientar tus decisiones futuras. Al finalizar este curso, habrás desarrollado la capacidad de utilizar distintas técnicas para la construcción y evaluación de modelos  que con base en criterios de desempeño preestablecidos te permitirán aprovechar el valor de los datos.     "
842,Teoría de filas  ,4,https://www.coursera.org/learn/datos-tecnicas?specialization=analisis-datos,"En diversas problemáticas profesionales relacionadas con el análisis de un gran número de datos,  seguramente  te has encontrado en la disyuntiva de aceptar o rechazar una proposición y seguramente te gustaría tener más información para tomar tu decisión acertada; la forma de lograrlo  es formular el problema a través de una prueba de hipótesis.¿Cómo saber si una hipótesis estadística está correctamente planteada? ¿La verdad o falsedad de una hipótesis en particular se conoce con certeza sin tener que analizar a toda la población? 

Al finalizar este módulo serás capaz de desarrollar un procedimiento de prueba de hipótesis teniendo en cuenta el tipo de información contenida en la muestra aleatoria de la población de interés y evitar la posibilidad de llegar a una conclusión equivocada."
843,Análisis de datos en el sector salud: Identificando como mejorar la atención ,4,https://www.coursera.org/learn/analisis-de-datos-aplicaciones?specialization=analisis-datos,"En este módulo conocerás la aplicación de uno de los modelos más populares para la toma de decisiones ya que te permite estimar el valor promedio de una variable dependiente tomando en cuenta una o más variables explicativas o bien hacer inferencias acerca de algún fenómeno del cual no conozcas aún su resultado. Los modelos de regresión pueden ser adaptados a un sinfín de aplicaciones ejecutivas. Hoy en día es mucho más fácil realizar el análisis de un gran volumen de datos gracias a que muchos paquetes estadísticos han desarrollado interfaces amigables con el usuario que le evitan realizar cálculos matemáticos y así el usuario centre su interés en el análisis.  Finalmente, este módulo te ayudará a identificar elementos básicos que integran un modelo de regresión pero sobre todo te permitirán el valor de los datos para una adecuada tomar de decisiones dentro de tu organización."
844,Generando valor para el consumidor: Aplicaciones de análisis de datos en ventas al detalle (Retail).,4,https://www.coursera.org/learn/analisis-de-datos-aplicaciones?specialization=analisis-datos,"Las ""filas"" son un aspecto típico de la vida moderna que nos encontramos continuamente en nuestras actividades cotidianas, en un banco, en un centro comercial, al abordar un avión, en un call center, etc. Este fenómeno se origina cuando tenemos la necesidad de compartir uno o más recursos, los cuales son utilizados para dar atención a un gran número de trabajadores. Las organizaciones frecuentemente deben tomar decisiones respecto a la capacidad de servicios que debe ofrecer. Sin embargo, muchas veces es imposible predecir con exactitud cuándo llegarán los clientes que demandan el servicio y/o cuánto tiempo será necesario para dar ese servicio; es por eso que esas decisiones implican dilemas que se deben de resolver con información escasa. Los modelos de filas no resuelven  directamente el problema, pero generan información que se necesita para tomar las decisiones adecuadas prediciendo algunas características sobre la línea de espera: Al finalizar este módulo serás capaz de entender cómo se estructura un sistema de filas de espera y analizar el costo que implica para las organizaciones operar con recursos ociosos cuando no se analiza adecuadamente la información del número de clientes que demandan un servicio y la duración de éste."
845,Generando mejores rendimientos: Utilizando análisis de datos para mejores decisiones financieras,4,https://www.coursera.org/learn/analisis-de-datos-aplicaciones?specialization=analisis-datos,"La atención a la salud se ha vuelto más compleja en función a factores socio-demográficos, económicos y biológicos. Hay una creciente necesidad por mejores servicios de salud, que además de ser eficaces operen de manera eficientes. 

En este módulo exploraremos algunas aplicaciones del análisis de datos al análisis, planeación y diseño de servicios de salud."
846,Mejores decisiones en la cadena de suministro mediante análisis de datos,4,https://www.coursera.org/learn/analisis-de-datos-aplicaciones?specialization=analisis-datos,"Dentro de los diversos sectores de la economía, el comercio al detalle (también conocido con el término anglosajón retail) es uno de los más dinámicos debido a la extensa competencia que hay en él, así como la rápida evolución del comportamiento del consumidor. En este módulo identificaremos las aplicaciones de análisis de datos que permiten encontrar formas más eficientes para planificar y operar esta industria con el fin de lograr resultados óptimos."
847,Week 1:  Introduction to Data Products,5,https://www.coursera.org/learn/basic-data-processing-visualization-python,"Las finanzas son fundamentales para el funcionamiento de nuestro mundo y por tanto demandan una eficiente gestión de los recursos, la complejidad de las decisiones relacionadas con ésta, vuelve indispensable contar con herramientas que nos permitan contar con una alta eficacia y eficiencia al tomarlas. En este módulo exploraremos algunas de las ventajas de emplear el análisis de datos orientado a fenómenos económicos y financieros y cómo podemos sacar ventaja de la información disponible."
848,Week 2: Reading Data in Python,5,https://www.coursera.org/learn/basic-data-processing-visualization-python,"La gestión de la cadena de suministro es una función vital en las organizaciones. Su operación eficaz es indispensable para cumplir expectativas de servicio de los clientes y consumidores, y conducirla de manera eficiente permite que los beneficios puedan maximizarse. En este módulo conoceremos algunas de las aplicaciones del análisis de datos en los procesos fundamentales de la cadena de suministro, tales como planeación de la demanda, gestión de proveedores y distribución de productos."
849,Week 3: Data Processing in Python,5,https://www.coursera.org/learn/basic-data-processing-visualization-python,"This week, we will go over the syllabus and set you up with the course materials and software. We will introduce you to data products and refresh your memory on Python and Jupyter notebooks."
850,Week 4: Python Libraries and Toolkits,5,https://www.coursera.org/learn/basic-data-processing-visualization-python,"This week, we will learn how to load in datasets from CSV and JSON files. We will also practice manipulating data from these datasets with basic Python commands."
851,Final Project,5,https://www.coursera.org/learn/basic-data-processing-visualization-python,"This week, our goal is to understand how to clean up a dataset before analyzing it. We will go over how to work with different types of  data, such as strings and dates."
852,Week 1: Supervised Learning & Regression,5,https://www.coursera.org/learn/design-thinking-predictive-analytics-data-products,"In this last week, we will get a sense of common libraries in Python and how they can be useful. We will cover data visualization with numpy and MatPlotLib, and also introduce you to the basics of webscraping with urllib and BeautifulSoup."
853,Week 2: Features ,5,https://www.coursera.org/learn/design-thinking-predictive-analytics-data-products,"Create your own Jupyter notebook with a dataset of your own choosing and practice data manipulation. Show off the skills you've learned and the libraries you know about in this project. We hope you enjoyed the course, and best of luck in your future learning!"
854,Week 3: Classification,5,https://www.coursera.org/learn/design-thinking-predictive-analytics-data-products,"Welcome to the second course in this specialization! This week, we will go over the syllabus, download all course materials, and get your system up and running for the course. We will also introduce the basics of supervised learning and regression."
855,Week 4: Gradient Descent,5,https://www.coursera.org/learn/design-thinking-predictive-analytics-data-products,"This week, we will learn what features are in a dataset and how we can work with them through cleaning, manipulation, and analysis in Jupyter notebooks."
856,Final Project,5,https://www.coursera.org/learn/design-thinking-predictive-analytics-data-products,"This week, we will learn about classification and several ways you can implement it, such as K-nearest neighbors, logistic regression, and support vector machines."
857,Week 1: Diagnostics for Data,4,https://www.coursera.org/learn/meaningful-predictive-modeling,"This week, we will learn the importance of properly training and testing a model. We will also implement gradient descent in both Python and TensorFlow."
858,"Week 2: Codebases, Regularization, and Evaluating a Model",4,https://www.coursera.org/learn/meaningful-predictive-modeling,"In the final week of this course, you will continue building on the project from the first course of Python Data Products for Predictive Analytics with simple predictive machine learning algorithms. Find a dataset, clean it, and perform basic analyses on the data."
859,Week 3: Validation and Pipelines,4,https://www.coursera.org/learn/meaningful-predictive-modeling,"For this first week, we will go over the syllabus, download all course materials, and get your system up and running for the course. We will also introduce the basics of diagnostics for the results of supervised learning."
860,Final Project,4,https://www.coursera.org/learn/meaningful-predictive-modeling,"This week, we will learn how to create a simple bag of words for analysis. We will also cover regularization and why it matters when building a model. Lastly, we will evaluate a model with regularization, focusing on classifiers."
861,Introduction,5,https://www.coursera.org/learn/deploying-machine-learning-models,"This week, we will learn about validation and how to implement it in tandem with training and testing. We will also cover how to implement a regularization pipeline in Python and introduce a few guidelines for best practices."
862,Implementing Recommender Systems,5,https://www.coursera.org/learn/deploying-machine-learning-models,"In the final week of this course, you will continue building on the project from the first and second courses of Python Data Products for Predictive Analytics with simple predictive machine learning algorithms. Find a dataset, clean it, and perform basic analyses on the data. Evaluate your model, validate your analyses, and make sure you aren't overfitting the data."
863,Deploying Recommender Systems,5,https://www.coursera.org/learn/deploying-machine-learning-models,"Welcome to the first week of Deploying Machine Learning Models! We will go over the syllabus, download all course materials, and get your system up and running for the course. We will also introduce the basics of recommender systems and differentiate it from other types of machine learning"
864,Project 4: Recommender System,5,https://www.coursera.org/learn/deploying-machine-learning-models,"This week, we will learn how to implement a similarity-based recommender, returning predictions similar to an user's given item. We will cover how to optimize these models based on gradient descent and Jaccard similarity."
865,Capstone,5,https://www.coursera.org/learn/deploying-machine-learning-models,"This week, we will learn about Python web server frameworks and the overall structure of interactive Python data applications. We will also cover some tips for best practices on deploying and monitoring your applications."
866,Data Science Context and Concepts,5,https://www.coursera.org/learn/data-manipulation,"For this final project, you will build a recommender system of your own. Find a dataset, clean it, and create a predictive system from the dataset. This will help prepare you for the upcoming capstone, where you will harness your skills from all courses of this specialization into one single project!"
867,Relational Databases and the Relational Algebra,5,https://www.coursera.org/learn/data-manipulation,"Time to put all your hard work to the test! This capstone project consists of four components, each drawing from a separate course in this specialization. It's time to show off everything you've learned from this specialization."
868,MapReduce and Parallel Dataflow Programming,5,https://www.coursera.org/learn/data-manipulation,"Understand the terminology and recurring principles associated with data science, and understand the structure of data science projects and emerging methodologies to approach them.    Why does this emerging field exist?  How does it relate to other fields?  How does this course distinguish itself?  What do data science projects look like, and how should they be approached?  What are some examples of data science projects?  "
869,NoSQL: Systems and Concepts,5,https://www.coursera.org/learn/data-manipulation,"Relational Databases are the workhouse of large-scale data management.  Although originally motivated by problems in enterprise operations, they have proven remarkably capable for analytics as well.  But most importantly, the principles underlying relational databases are universal in managing, manipulating, and analyzing data at scale.  Even as the landscape of large-scale data systems has expanded dramatically in the last decade, relational models and languages have remained a unifying concept.  For working with large-scale data, there is no more important programming model to learn."
870,Graph Analytics,5,https://www.coursera.org/learn/data-manipulation,"The MapReduce programming model (as distinct from its implementations) was proposed as a simplifying abstraction for parallel manipulation of massive datasets, and remains an important concept to know when using and evaluating modern big data platforms.  "
871,Practical Statistical Inference,4,https://www.coursera.org/learn/predictive-analytics,"NoSQL systems are purely about scale rather than analytics, and are arguably less relevant for the practicing data scientist.  However, they occupy an important place in many practical big data platform architectures, and data scientists need to understand their limitations and strengths to use them effectively."
872,Supervised Learning,4,https://www.coursera.org/learn/predictive-analytics,"Graph-structured data are increasingly common in data science contexts due to their ubiquity in modeling the communication between entities: people (social networks), computers (Internet communication), cities and countries (transportation networks), or corporations (financial transactions).  Learn the common algorithms for extracting information from graph data and how to scale them up. "
873,Optimization,4,https://www.coursera.org/learn/predictive-analytics,"Learn the basics of statistical inference, comparing classical methods with resampling methods that allow you to use a simple program to make a rigorous statistical argument.  Motivate your study with current topics at the foundations of science: publication bias and reproducibility."
874,Unsupervised Learning,4,https://www.coursera.org/learn/predictive-analytics,"Follow a tour through the important methods, algorithms, and techniques in machine learning.  You will learn how these methods build upon each other and can be combined into practical algorithms that perform well on a variety of tasks.  Learn how to evaluate machine learning methods and the pitfalls to avoid."
875,Visualization,3,https://www.coursera.org/learn/data-results,"You will learn how to optimize a cost function using gradient descent, including popular variants that use randomization and parallelization to improve performance.  You will gain an intuition for popular methods used in practice and see how similar they are fundamentally. "
876,Privacy and Ethics,3,https://www.coursera.org/learn/data-results,A brief tour of selected unsupervised learning methods and an opportunity to apply techniques in practice on a real world problem.
877,Reproducibility and Cloud Computing,3,https://www.coursera.org/learn/data-results,"Statistical inferences from large, heterogeneous, and noisy datasets are useless if you can't communicate them to your colleagues, your customers, your management and other stakeholders.  Learn the fundamental concepts behind information visualization, an increasingly critical field of research and increasingly important skillset for data scientists.  This module is taught by Cecilia Aragon, faculty in the Human Centered Design and Engineering Department."
878,Project A: Blight Fight,6,https://www.coursera.org/learn/datasci-capstone,"Big Data has become closely linked to issues of privacy and ethics: As the limits on what we *can* do with data continue to evaporate, the question of what we *should* do with data becomes paramount.  Motivated in the context of case studies, you will learn the core principles of codes of conduct for data science and statistical analysis.  You will learn the limits of current theory on protecting privacy while still permitting useful statistical analysis. "
879,Week 2: Derive a list of buildings,6,https://www.coursera.org/learn/datasci-capstone,"Science is facing a credibility crisis due to unreliable reproducibility, and as research becomes increasingly computational, the problem seems to be paradoxically getting worse.  But reproducibility is not just for academics: Data scientists who cannot share, explain, and defend their methods for others to build on are dangerous.  In this module, you will explore the importance of reproducible research and how cloud computing is offering new mechanisms for sharing code, data, environments, and even costs that are critical for practical reproducibility."
880,Week 3: Construct a training dataset,6,https://www.coursera.org/learn/datasci-capstone,"In this project, you will build a model to predict when a building is likely to be condemned.  The data is real, the problem is real, and the impact is real.  "
881,Week 4: Train and evaluate a simple model,6,https://www.coursera.org/learn/datasci-capstone,You are given sets of incidents with location information; you need to use some assumptions to group these incidents by location to identify specific buildings.
882,Week 5: Feature Engineering,6,https://www.coursera.org/learn/datasci-capstone,Construct a training set by associating each of your buildings with a ground truth label derived from the permit data.
883,Week 6: Final Report,6,https://www.coursera.org/learn/datasci-capstone,Use a trivial feature set to train and evaluate a simple model
